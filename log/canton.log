2025-08-24 00:02:40,323 [canton-env-ec-153] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::1220864c3a9b... authenticated new token with expiry 2025-08-24T01:02:40.321081Z
2025-08-24 00:02:41,065 [canton-env-ec-153] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::1220864c3a9b... authenticated new token with expiry 2025-08-24T01:02:41.063506Z
2025-08-24 00:03:00,298 [canton-env-ec-91] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 00:03:00,299 [canton-env-ec-152] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 00:03:00,352 [canton-env-ec-165] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::1220864c3a9b... subscribes from counter=21
2025-08-24 00:03:01,043 [canton-env-ec-165] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 00:03:01,043 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 00:03:01,098 [canton-env-ec-91] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::1220864c3a9b... subscribes from counter=44
2025-08-24 00:03:25,267 [canton-env-ec-166] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220c57b912a... authenticated new token with expiry 2025-08-24T01:03:25.265888Z
2025-08-24 00:03:45,246 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220864c3a9b - The sequencer subscription has been terminated by the server.
2025-08-24 00:03:45,247 [canton-env-ec-153] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::1220864c3a9b tid:7439dbe66eac906d7099dbf606d32d7f - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 00:03:45,302 [canton-env-ec-156] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:7439dbe66eac906d7099dbf606d32d7f - PAR::participant1::1220c57b912a... subscribes from counter=55
2025-08-24 00:03:47,841 [daml.index.db.threadpool.connection.indexer-15] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(000000000000000010)),2025-08-24T00:00:00Z)
2025-08-24 00:03:47,843 [daml.index.db.threadpool.connection.indexer-15] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(000000000000000010)),2025-08-24T00:00:00Z)
2025-08-24 01:02:20,345 [canton-env-ec-165] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::1220864c3a9b... authenticated new token with expiry 2025-08-24T02:02:20.342842Z
2025-08-24 01:02:21,089 [canton-env-ec-156] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::1220864c3a9b... authenticated new token with expiry 2025-08-24T02:02:21.087426Z
2025-08-24 01:02:40,324 [canton-env-ec-166] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 01:02:40,324 [canton-env-ec-156] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 01:02:40,379 [canton-env-ec-167] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::1220864c3a9b... subscribes from counter=21
2025-08-24 01:02:41,066 [canton-env-ec-91] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 01:02:41,067 [canton-env-ec-165] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 01:02:41,122 [canton-env-ec-156] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::1220864c3a9b... subscribes from counter=44
2025-08-24 01:03:05,319 [canton-env-ec-152] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220c57b912a... authenticated new token with expiry 2025-08-24T02:03:05.316491Z
2025-08-24 01:03:25,271 [canton-env-ec-152] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220864c3a9b - The sequencer subscription has been terminated by the server.
2025-08-24 01:03:25,272 [canton-env-ec-156] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::1220864c3a9b tid:7439dbe66eac906d7099dbf606d32d7f - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 01:03:25,331 [canton-env-ec-166] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:7439dbe66eac906d7099dbf606d32d7f - PAR::participant1::1220c57b912a... subscribes from counter=55
2025-08-24 01:03:47,842 [daml.index.db.threadpool.connection.indexer-8] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(000000000000000010)),2025-08-24T01:00:00Z)
2025-08-24 01:03:47,844 [daml.index.db.threadpool.connection.indexer-8] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(000000000000000010)),2025-08-24T01:00:00Z)
2025-08-24 01:12:06,422 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 01:12:06,425 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 01:12:06,428 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,429 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,430 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,430 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,441 [canton-env-ec-153] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 01:12:06,446 [canton-env-ec-165] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:67fe431013ebd2561e887284c269d215 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:39048: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 01:12:06,459 [canton-env-ec-65] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 01:12:06,470 [canton-env-ec-156] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 01:12:06,476 [canton-env-ec-156] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 01:12:06,476 [canton-env-ec-167] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 01:12:06,477 [canton-env-ec-167] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 01:12:06,479 [canton-env-ec-167] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 01:12:06,482 [canton-env-ec-156] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 01:12:06,501 [canton-env-ec-156] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 01:12:06,501 [canton-env-ec-156] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 01:12:06,502 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 01:12:06,503 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 01:12:06,503 [canton-env-ec-156] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 01:12:06,503 [canton-env-ec-156] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 01:12:06,510 [canton-env-ec-165] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 01:12:06,513 [canton-env-ec-165] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 01:12:06,516 [canton-env-ec-153] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::1220864c3a9b - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,516 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,517 [canton-env-ec-153] INFO  c.d.c.s.c.ResilientSequencerSubscription:participant=participant1/domainId=mydomain::1220864c3a9b - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,517 [canton-env-ec-153] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::1220864c3a9b - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,517 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,518 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220864c3a9b - GRPC subscription successfully closed due to client shutdown.
2025-08-24 01:12:06,519 [canton-env-ec-167] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:7439dbe66eac906d7099dbf606d32d7f - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:56122: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 01:12:06,521 [canton-env-ec-153] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::1220864c3a9b - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,521 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,521 [canton-env-ec-91] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:7439dbe66eac906d7099dbf606d32d7f - Domain 'mydomain' disconnected because sequencer client was closed
2025-08-24 01:12:06,525 [canton-env-ec-153] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,525 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,533 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 01:12:06,533 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 01:12:06,533 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 01:12:06,533 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 01:12:06,534 [canton-env-ec-153] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,534 [canton-env-ec-153] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 01:12:06,535 [canton-env-ec-153] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 01:12:06,537 [canton-env-ec-153] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 01:12:06,538 [canton-env-ec-153] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,538 [canton-env-ec-153] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,542 [canton-env-ec-153] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 01:12:06,545 [canton-env-ec-153] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,546 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,546 [canton-env-ec-153] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,547 [canton-env-ec-153] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,547 [canton-env-ec-65] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 01:12:06,548 [canton-env-ec-152] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:c8c35959b9d97ea7d16e1ef413f6aa49 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:35700: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 01:12:06,552 [canton-env-ec-153] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,552 [canton-env-ec-153] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 01:12:06,552 [canton-env-ec-91] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 01:12:06,553 [canton-env-ec-65] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:759a4dcf279b342d4d0735561e11a4cf - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:35720: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 01:12:06,556 [canton-env-ec-153] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 01:12:06,556 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 01:12:06,556 [canton-env-ec-153] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 01:12:06,564 [canton-env-ec-153] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,565 [canton-env-ec-153] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 01:12:06,565 [canton-env-ec-153] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 01:12:06,571 [canton-env-ec-153] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 01:12:06,572 [canton-env-ec-153] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,572 [canton-env-ec-153] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 01:12:06,591 [canton-env-ec-37] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 01:12:06,610 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 01:12:06,610 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 05:13:50,489 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 05:13:51,267 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=yes
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 05:13:51,287 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 05:13:51,494 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 05:13:51,572 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 05:13:51,631 [main] INFO  c.d.c.e.CommunityEnvironment tid:e15fe58cd0f273b33f2a26e5d50d14ee - Automatically starting all instances
2025-08-24 05:13:51,691 [canton-env-ec-36] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 05:13:51,691 [canton-env-ec-38] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 05:13:51,713 [canton-env-ec-36] INFO  c.d.c.r.DbStorage:participant1 tid:5c0cb2c7be32297808afddfff9f9dd42 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 05:13:51,713 [canton-env-ec-38] INFO  c.d.c.resource.DbStorage:mydomain tid:717c54d51c36ed93e7d81886b7976060 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 05:13:51,741 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 05:13:51,742 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 05:13:52,316 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 05:13:52,316 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 05:13:52,510 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 05:13:52,510 [canton-env-ec-38] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 05:13:52,526 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 05:13:52,527 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 05:13:52,527 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 05:13:52,526 [canton-env-ec-38] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 05:13:52,527 [canton-env-ec-38] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 05:13:52,527 [canton-env-ec-38] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 05:13:52,602 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 05:13:52,602 [canton-env-ec-38] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 05:13:52,610 [canton-env-ec-38] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 05:13:52,610 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 05:13:52,655 [canton-env-ec-38] INFO  c.d.c.r.CommunityDbMigrations:mydomain - There are 9 pending migrations for the db that is at version 0. Performing migration before start.
2025-08-24 05:13:52,655 [canton-env-ec-36] INFO  c.d.c.r.CommunityDbMigrations:participant1 - There are 9 pending migrations for the db that is at version 0. Performing migration before start.
2025-08-24 05:13:52,663 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 05:13:52,665 [canton-env-ec-38] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 05:13:52,674 [canton-env-ec-36] INFO  o.f.c.internal.command.DbValidate - Successfully validated 9 migrations (execution time 00:00.007s)
2025-08-24 05:13:52,675 [canton-env-ec-38] INFO  o.f.c.internal.command.DbValidate - Successfully validated 9 migrations (execution time 00:00.007s)
2025-08-24 05:13:52,685 [canton-env-ec-36] INFO  o.f.c.i.s.JdbcTableSchemaHistory - Creating Schema History table "public"."flyway_schema_history" ...
2025-08-24 05:13:52,687 [canton-env-ec-38] INFO  o.f.c.i.s.JdbcTableSchemaHistory - Creating Schema History table "public"."flyway_schema_history" ...
2025-08-24 05:13:52,728 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "public": << Empty Schema >>
2025-08-24 05:13:52,728 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "public": << Empty Schema >>
2025-08-24 05:13:52,818 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "1.1 - initial"
2025-08-24 05:13:52,821 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "1.1 - initial"
2025-08-24 05:13:53,292 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "2 - changes for 2.3"
2025-08-24 05:13:53,292 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "2 - changes for 2.3"
2025-08-24 05:13:53,315 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "3 - changes for 2.4"
2025-08-24 05:13:53,318 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "3 - changes for 2.4"
2025-08-24 05:13:53,341 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "4 - changes for 2.5"
2025-08-24 05:13:53,341 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "4 - changes for 2.5"
2025-08-24 05:13:53,363 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "5 - changes for 2.6"
2025-08-24 05:13:53,363 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "5 - changes for 2.6"
2025-08-24 05:13:53,378 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "6 - changes for 2.7"
2025-08-24 05:13:53,380 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "6 - changes for 2.7"
2025-08-24 05:13:53,411 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "7 - changes for 2.8"
2025-08-24 05:13:53,413 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "7 - changes for 2.8"
2025-08-24 05:13:53,430 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "8 - changes for 2.9"
2025-08-24 05:13:53,431 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "8 - changes for 2.9"
2025-08-24 05:13:53,443 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "9 - changes for 2.10"
2025-08-24 05:13:53,443 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "9 - changes for 2.10"
2025-08-24 05:13:53,452 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Successfully applied 9 migrations to schema "public", now at version v9 (execution time 00:00.729s)
2025-08-24 05:13:53,454 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Successfully applied 9 migrations to schema "public", now at version v9 (execution time 00:00.732s)
2025-08-24 05:13:53,455 [canton-env-ec-36] INFO  c.d.c.r.CommunityDbMigrations:participant1 - Applied 9 migrations successfully
2025-08-24 05:13:53,457 [canton-env-ec-38] INFO  c.d.c.r.CommunityDbMigrations:mydomain - Applied 9 migrations successfully
2025-08-24 05:13:53,459 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 05:13:53,459 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 05:13:53,464 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 05:13:53,467 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 05:13:53,502 [canton-env-ec-38] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 05:13:53,505 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 05:13:53,537 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 05:13:53,634 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 05:13:53,636 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 05:13:53,657 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 05:13:53,771 [canton-env-ec-66] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:13:53,771 [canton-env-ec-64] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:13:53,779 [canton-env-ec-38] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),None,Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 05:13:53,780 [canton-env-ec-36] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),None,Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 05:13:54,185 [canton-env-ec-38] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 05:13:54,194 [canton-env-ec-36] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 05:13:54,262 [canton-env-ec-64] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Node is not initialized yet. Performing automated default initialization.
2025-08-24 05:13:54,287 [canton-env-ec-36] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Node is not initialized yet. Performing automated default initialization.
2025-08-24 05:13:54,767 [canton-env-ec-37] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:5dca944e2b51d3d5f1c9f94b68db94c1 - Applied topology transaction Add NamespaceDelegation(12208472822e..., SigningPublicKey(id = 12208472822e..., format = Tink, scheme = Ed25519), true) at 2025-08-24T05:13:54.765527Z
2025-08-24 05:13:54,770 [canton-env-ec-65] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:87515d1d501d37241b6b53eaec0e69b3 - Applied topology transaction Add NamespaceDelegation(1220afab1ca3..., SigningPublicKey(id = 1220afab1ca3..., format = Tink, scheme = Ed25519), true) at 2025-08-24T05:13:54.765600Z
2025-08-24 05:13:54,883 [canton-env-ec-37] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:87515d1d501d37241b6b53eaec0e69b3 - Applied topology transaction Replace DomainParametersChange(
  mydomain::1220afab1ca3...,
  DynamicDomainParameters(
    participant response timeout = 30s,
    mediator reaction timeout = 30s,
    transfer exclusivity timeout = 1m,
    topology change delay = 0.25s,
    ledger time record time tolerance = 1m,
    mediator deduplication timeout = 2m,
    reconciliation interval = 1m,
    max rate per participant = 1000000,
    max request size = 10485760,
    catchup config = CatchUpConfig(catchUpIntervalSkip = 5, nrIntervalsToTriggerCatchUp = 2)
  )
) at 2025-08-24T05:13:54.880552Z
2025-08-24 05:13:54,921 [canton-env-ec-64] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:87515d1d501d37241b6b53eaec0e69b3 - Applied topology transaction Add OwnerToKeyMapping(DOM::mydomain::1220afab1ca3..., SigningPublicKey(id = 12209b443c21..., format = Tink, scheme = Ed25519)) at 2025-08-24T05:13:54.919785Z
2025-08-24 05:13:55,022 [canton-env-ec-37] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain - Applied topology transaction Add OwnerToKeyMapping(MED::mydomain::1220afab1ca3..., SigningPublicKey(id = 1220f6603a99..., format = Tink, scheme = Ed25519)) at 2025-08-24T05:13:55.018582Z
2025-08-24 05:13:55,059 [canton-env-ec-38] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain - Applied topology transaction Add MediatorDomainState(Both, mydomain::1220afab1ca3..., MED::mydomain::1220afab1ca3...) at 2025-08-24T05:13:55.057443Z
2025-08-24 05:13:55,084 [canton-env-ec-35] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 05:13:55,088 [canton-env-ec-66] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (4 ms)
2025-08-24 05:13:55,106 [canton-env-ec-65] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain - Applied topology transaction Add OwnerToKeyMapping(SEQ::mydomain::1220afab1ca3..., SigningPublicKey(id = 1220f8fc1dc6..., format = Tink, scheme = Ed25519)) at 2025-08-24T05:13:55.104456Z
2025-08-24 05:13:55,125 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Initializing node with id NodeId(mydomain::1220afab1ca3...)
2025-08-24 05:13:55,208 [canton-env-ec-35] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:ab2666c5385d9442fa53ab301c24f276 - Recovering published timely rejections
2025-08-24 05:13:55,228 [canton-env-ec-38] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:ab2666c5385d9442fa53ab301c24f276 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 05:13:55,230 [canton-env-ec-38] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:ab2666c5385d9442fa53ab301c24f276 - Fetch unpublished in log ParticipantEventLogId(index = 0), from None (exclusive) up to None (inclusive)
2025-08-24 05:13:55,378 [canton-env-ec-66] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 05:13:55,424 [canton-env-ec-64] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 05:13:55,426 [canton-env-ec-64] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 05:13:55,461 [canton-env-ec-35] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 05:13:55,800 [canton-env-ec-35] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 05:13:55,873 [canton-env-ec-90] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - No event found up to 0001-01-01T00:00:00Z. Resubscribing from the beginning.
2025-08-24 05:13:55,875 [canton-env-ec-90] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 0001-01-01T00:00:00.000001Z on
2025-08-24 05:13:55,913 [canton-env-ec-64] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 05:13:55,951 [canton-env-ec-35] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::1220afab1ca3... at timestamp None; next counter 0
2025-08-24 05:13:55,970 [canton-env-ec-35] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:13:55,977 [canton-env-ec-64] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 05:13:55,979 [canton-env-ec-64] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 05:13:56,071 [canton-env-ec-64] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 05:13:56,081 [canton-env-ec-90] INFO  c.d.c.d.i.TopologyManagementInitialization$:domain=mydomain - Sending initial topology transactions to domain members Set(DOM::mydomain::1220afab1ca3..., MED::mydomain::1220afab1ca3...) as DOM::mydomain::1220afab1ca3...
2025-08-24 05:13:56,114 [canton-env-ec-64] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.039s)
2025-08-24 05:13:56,123 [canton-env-ec-64] INFO  o.f.c.i.s.JdbcTableSchemaHistory - Creating Schema History table "ledger_api"."flyway_schema_history" ...
2025-08-24 05:13:56,131 [canton-env-ec-35] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T06:13:56.118612Z
2025-08-24 05:13:56,187 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": << Empty Schema >>
2025-08-24 05:13:56,191 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "1 - Init"
2025-08-24 05:13:56,201 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::1220afab1ca3... subscribes from counter=0
2025-08-24 05:13:56,261 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "2.0 - Contract divulgence"
2025-08-24 05:13:56,275 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "2.1 - Rebuild Acs"
2025-08-24 05:13:56,281 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "3 - Recompute Key Hash"
2025-08-24 05:13:56,286 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "4.0 - Add parties"
2025-08-24 05:13:56,290 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:c9931325cffcdb403b27d64fa4bd751c - 'DOM::mydomain::1220afab1ca3...' sends request with id 'cf0e8f89-f9ed-4e55-aa53-be5ec202030c' of size 1815 bytes with 2 envelopes.
2025-08-24 05:13:56,301 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "4.1 - Collect Parties"
2025-08-24 05:13:56,307 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "5 - Add packages"
2025-08-24 05:13:56,322 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "6 - External Ledger Offset"
2025-08-24 05:13:56,327 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "7 - Command deduplication"
2025-08-24 05:13:56,335 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "8 - Contract Divulgence"
2025-08-24 05:13:56,345 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "9 - Contract Divulgence"
2025-08-24 05:13:56,359 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "10.0 - Extract Event Data"
2025-08-24 05:13:56,374 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "10.1 - Populate Event Data"
2025-08-24 05:13:56,382 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "10.2 - Extract Event Data"
2025-08-24 05:13:56,393 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "11 - Disclosures index"
2025-08-24 05:13:56,407 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "12 - Add configuration"
2025-08-24 05:13:56,408 [canton-env-ec-90] INFO  c.d.c.d.s.s.SequencerReader$EventsReader:domain=mydomain/subscriber=DOM::mydomain::1220afab1ca3 tid:c9931325cffcdb403b27d64fa4bd751c - Using approximate topology snapshot at 0001-01-01T00:00:00.000001Z for desired timestamp 2025-08-24T05:13:56.326823Z
2025-08-24 05:13:56,431 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "13 - Party entries"
2025-08-24 05:13:56,451 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "14 - Package entries"
2025-08-24 05:13:56,452 [canton-env-ec-35] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::1220afab1ca3...
2025-08-24 05:13:56,464 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "15 - Loosen transaction check"
2025-08-24 05:13:56,472 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "16 - Create command completions table"
2025-08-24 05:13:56,484 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "17 - Command deduplication"
2025-08-24 05:13:56,495 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "18 - Backfill completions"
2025-08-24 05:13:56,501 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "19 - Fix Completions"
2025-08-24 05:13:56,508 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "20 - Events new schema"
2025-08-24 05:13:56,519 [canton-env-ec-38] INFO  c.d.c.t.p.DomainTopologyTransactionMessageValidator$Impl:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Using approximate topology snapshot at 0001-01-01T00:00:00.000001Z for desired timestamp 2025-08-24T05:13:56.326823Z
2025-08-24 05:13:56,542 [canton-env-ec-36] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Updated topology change delay from=0s to 0.25s
2025-08-24 05:13:56,545 [canton-env-ec-36] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 1/6 Add NamespaceDelegation(1220afab1ca3..., SigningPublicKey(id = 1220afab1ca3..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,548 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "21 - Stable offsets"
2025-08-24 05:13:56,549 [canton-env-ec-36] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 2/6 Replace DomainParametersChange(
  mydomain::1220afab1ca3...,
  DynamicDomainParameters(
    participant response timeout = 30s,
    mediator reaction timeout = 30s,
    transfer exclusivity timeout = 1m,
    topology change delay = 0.25s,
    ledger time record time tolerance = 1m,
    mediator deduplication timeout = 2m,
    reconciliation interval = 1m,
    max rate per participant = 1000000,
    max request size = 10485760,
    catchup config = CatchUpConfig(catchUpIntervalSkip = 5, nrIntervalsToTriggerCatchUp = 2)
  )
) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,551 [canton-env-ec-36] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 3/6 Add OwnerToKeyMapping(DOM::mydomain::1220afab1ca3..., SigningPublicKey(id = 12209b443c21..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,552 [canton-env-ec-36] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 4/6 Add OwnerToKeyMapping(MED::mydomain::1220afab1ca3..., SigningPublicKey(id = 1220f6603a99..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,552 [canton-env-ec-36] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 5/6 Add MediatorDomainState(Both, mydomain::1220afab1ca3..., MED::mydomain::1220afab1ca3...) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,553 [canton-env-ec-36] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 6/6 Add OwnerToKeyMapping(SEQ::mydomain::1220afab1ca3..., SigningPublicKey(id = 1220f8fc1dc6..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,681 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "22 - Remove maximum record time"
2025-08-24 05:13:56,682 [canton-env-ec-66] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:13:56,692 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "23 - Delete checkpoints"
2025-08-24 05:13:56,708 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "24 - Stable offsets archival"
2025-08-24 05:13:56,739 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "25 - Backfill Participant Events"
2025-08-24 05:13:56,753 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "26.0 - Contracts new schema"
2025-08-24 05:13:56,781 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "26.1 - Fill create argument"
2025-08-24 05:13:56,787 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "26.2 - Contract create arg not null"
2025-08-24 05:13:56,797 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "27 - Events table fixes"
2025-08-24 05:13:56,812 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "28 - Fix key hashes"
2025-08-24 05:13:56,812 [canton-env-ec-58] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - No event found up to 0001-01-01T00:00:00Z. Resubscribing from the beginning.
2025-08-24 05:13:56,812 [canton-env-ec-58] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 0001-01-01T00:00:00.000001Z on
2025-08-24 05:13:56,819 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "29 - Fix participant events"
2025-08-24 05:13:56,822 [canton-env-ec-103] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::1220afab1ca3... at timestamp None; next counter 0
2025-08-24 05:13:56,823 [canton-env-ec-103] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:13:56,829 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "30 - Drop old schema"
2025-08-24 05:13:56,836 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: drop cascades to 5 other objects
2025-08-24 05:13:56,838 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: drop cascades to constraint contract_divulgences_contract_id_fkey1 on table contract_divulgences
2025-08-24 05:13:56,852 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "31 - Event witnesses single table"
2025-08-24 05:13:56,874 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "32.0 - Drop archived contracts"
2025-08-24 05:13:56,880 [canton-env-ec-35] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T06:13:56.876795Z
2025-08-24 05:13:56,882 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "32.1 - Fix key hashes"
2025-08-24 05:13:56,891 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::1220afab1ca3... subscribes from counter=0
2025-08-24 05:13:56,893 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "33 - Add witnesses to participant events"
2025-08-24 05:13:56,905 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "34 - Parties is local"
2025-08-24 05:13:56,912 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "35 - event sequential id"
2025-08-24 05:13:56,925 [canton-env-ec-35] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::1220afab1ca3...
2025-08-24 05:13:56,940 [canton-env-ec-38] INFO  c.d.c.d.m.Mediator:domain=mydomain/node=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Caught up with batch with counter=0 with sequencer with 614 ms delay
2025-08-24 05:13:56,942 [canton-env-ec-38] INFO  c.d.c.t.p.DomainTopologyTransactionMessageValidator$Impl:domain=mydomain/node=mediator/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Using approximate topology snapshot at 0001-01-01T00:00:00.000001Z for desired timestamp 2025-08-24T05:13:56.326823Z
2025-08-24 05:13:56,944 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "36 - drop participant id"
2025-08-24 05:13:56,956 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "37 - add participant id to parameters"
2025-08-24 05:13:56,964 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "38 - Update value versions"
2025-08-24 05:13:56,966 [canton-env-ec-37] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Updated topology change delay from=0s to 0.25s
2025-08-24 05:13:56,968 [canton-env-ec-58] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 1/6 Add NamespaceDelegation(1220afab1ca3..., SigningPublicKey(id = 1220afab1ca3..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,969 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "39 - Participant-pruning"
2025-08-24 05:13:56,969 [canton-env-ec-58] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 2/6 Replace DomainParametersChange(
  mydomain::1220afab1ca3...,
  DynamicDomainParameters(
    participant response timeout = 30s,
    mediator reaction timeout = 30s,
    transfer exclusivity timeout = 1m,
    topology change delay = 0.25s,
    ledger time record time tolerance = 1m,
    mediator deduplication timeout = 2m,
    reconciliation interval = 1m,
    max rate per participant = 1000000,
    max request size = 10485760,
    catchup config = CatchUpConfig(catchUpIntervalSkip = 5, nrIntervalsToTriggerCatchUp = 2)
  )
) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,970 [canton-env-ec-58] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 3/6 Add OwnerToKeyMapping(DOM::mydomain::1220afab1ca3..., SigningPublicKey(id = 12209b443c21..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,971 [canton-env-ec-58] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 4/6 Add OwnerToKeyMapping(MED::mydomain::1220afab1ca3..., SigningPublicKey(id = 1220f6603a99..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,971 [canton-env-ec-58] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 5/6 Add MediatorDomainState(Both, mydomain::1220afab1ca3..., MED::mydomain::1220afab1ca3...) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,972 [canton-env-ec-58] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:c9931325cffcdb403b27d64fa4bd751c - Storing topology transaction 6/6 Add OwnerToKeyMapping(SEQ::mydomain::1220afab1ca3..., SigningPublicKey(id = 1220f8fc1dc6..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:13:56.326823Z) (epsilon=0 ms)
2025-08-24 05:13:56,975 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "40 - multiple submitters"
2025-08-24 05:13:57,010 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "41 - hash indices"
2025-08-24 05:13:57,020 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "42 - Convert hash indices"
2025-08-24 05:13:57,032 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "43 - explicit compression"
2025-08-24 05:13:57,041 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "44 - offset as text"
2025-08-24 05:13:57,115 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "45 - fix large size index issues"
2025-08-24 05:13:57,134 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "100.0 - Append only schema"
2025-08-24 05:13:57,174 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "100.1 - Append only cleanup"
2025-08-24 05:13:57,176 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: drop cascades to constraint participant_contract_witnesses_contract_id_fkey on table participant_contract_witnesses
2025-08-24 05:13:57,204 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "100.2 - Append only indices"
2025-08-24 05:13:57,274 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "100.3 - Append only vacuum" [non-transactional]
2025-08-24 05:13:57,441 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_authid", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,442 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_subscription", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,442 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_database", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,442 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_db_role_setting", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,442 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_tablespace", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,443 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_auth_members", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,443 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shdepend", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,443 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shdescription", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,443 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_replication_origin", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,443 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shseclabel", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,443 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_parameter_acl", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:13:57,452 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "101 - drop configuration from parameters table"
2025-08-24 05:13:57,460 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "102 - add initialization indices"
2025-08-24 05:13:57,474 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "103 - remove duplicate index"
2025-08-24 05:13:57,482 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "104 - rename packages size column"
2025-08-24 05:13:57,492 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "105 - drop unique index constraints"
2025-08-24 05:13:57,519 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "106 - add rejection status proto column"
2025-08-24 05:13:57,528 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "107 - parameters table cleanup"
2025-08-24 05:13:57,538 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "108 - drop parties"
2025-08-24 05:13:57,552 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "109 - Add all divulgence pruning offset"
2025-08-24 05:13:57,561 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "110 - add deduplication info to completions"
2025-08-24 05:13:57,575 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "111 - timestamp to bigint"
2025-08-24 05:13:57,703 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "112 - add string interning"
2025-08-24 05:13:57,726 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "113 - enable string interning"
2025-08-24 05:13:57,735 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,736 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,738 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,739 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,741 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,742 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,743 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,744 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,745 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,746 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,747 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,748 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,749 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,750 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,751 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,752 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,752 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,753 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,754 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,755 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,756 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 05:13:57,783 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "114 - activate string interning for parties and templates"
2025-08-24 05:13:57,784 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------+
| ?column?                                  |
+-------------------------------------------+
| Interning data migration - Preparation... |
+-------------------------------------------+

2025-08-24 05:13:57,789 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------------------+
| ?column?                                              |
+-------------------------------------------------------+
| Interning data migration - Migrating party_entries... |
+-------------------------------------------------------+

2025-08-24 05:13:57,794 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------------------------------------+
| ?column?                                                                |
+-------------------------------------------------------------------------+
| Interning data migration - Migrating participant_command_completions... |
+-------------------------------------------------------------------------+

2025-08-24 05:13:57,797 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 05:13:57,798 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 05:13:57,801 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 05:13:57,803 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------------------------------------------------+
| ?column?                                                              |
+-----------------------------------------------------------------------+
| Interning data migration - Migrating participant_events_divulgence... |
+-----------------------------------------------------------------------+

2025-08-24 05:13:57,806 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 05:13:57,807 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 05:13:57,817 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 05:13:57,819 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------------------------------+
| ?column?                                                          |
+-------------------------------------------------------------------+
| Interning data migration - Migrating participant_events_create... |
+-------------------------------------------------------------------+

2025-08-24 05:13:57,823 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 05:13:57,825 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 05:13:57,840 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 05:13:57,843 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------------------------------------------+
| ?column?                                                                      |
+-------------------------------------------------------------------------------+
| Interning data migration - Migrating participant_events_consuming_exercise... |
+-------------------------------------------------------------------------------+

2025-08-24 05:13:57,849 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 05:13:57,852 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 05:13:57,869 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 05:13:57,872 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------------------------------------------------------------+
| ?column?                                                                          |
+-----------------------------------------------------------------------------------+
| Interning data migration - Migrating participant_events_non_consuming_exercise... |
+-----------------------------------------------------------------------------------+

2025-08-24 05:13:57,876 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 05:13:57,878 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 05:13:57,879 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: identifier "participant_events_non_consuming_exercise_event_sequential_id_old" will be truncated to "participant_events_non_consuming_exercise_event_sequential_id_o" (SQL State: 42622 - Error Code: 0)
2025-08-24 05:13:57,881 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: identifier "participant_events_non_consuming_exercise_transaction_id_idx_old" will be truncated to "participant_events_non_consuming_exercise_transaction_id_idx_ol" (SQL State: 42622 - Error Code: 0)
2025-08-24 05:13:57,891 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 05:13:57,894 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------------+
| ?column?                              |
+---------------------------------------+
| Interning data migration - Cleanup... |
+---------------------------------------+

2025-08-24 05:13:57,901 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| Interning data migration - Done |
+---------------------------------+

2025-08-24 05:13:57,915 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "116 - add filter table"
2025-08-24 05:13:57,917 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +--------------------------------------------------------------------------+
| ?column?                                                                 |
+--------------------------------------------------------------------------+
| Add Filter Table: Migrating to participant_events_create_filter table... |
+--------------------------------------------------------------------------+

2025-08-24 05:13:57,918 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------------+
| ?column?                              |
+---------------------------------------+
| Add Filter Table: Creating indexes... |
+---------------------------------------+

2025-08-24 05:13:57,927 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------+
| ?column?                |
+-------------------------+
| Add Filter Table: Done. |
+-------------------------+

2025-08-24 05:13:57,936 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "117 - vacuum analyze" [non-transactional]
2025-08-24 05:14:03,102 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_authid", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,103 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_subscription", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,103 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_database", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,103 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_db_role_setting", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,103 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_tablespace", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,103 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_auth_members", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,104 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shdepend", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,104 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shdescription", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,104 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_replication_origin", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,104 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shseclabel", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,104 [canton-env-ec-64] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_parameter_acl", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 05:14:03,111 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "117.1 - drop event id indexes"
2025-08-24 05:14:03,120 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "118 - add user managment"
2025-08-24 05:14:03,149 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "119 - add transaction metering"
2025-08-24 05:14:03,151 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------------------+
| ?column?                                    |
+---------------------------------------------+
| Add Transaction Metering: Creating table... |
+---------------------------------------------+

2025-08-24 05:14:03,155 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------------------------+
| ?column?                                      |
+-----------------------------------------------+
| Add Transaction Metering: Creating indexes... |
+-----------------------------------------------+

2025-08-24 05:14:03,159 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| Add Transaction Metering: Done. |
+---------------------------------+

2025-08-24 05:14:03,168 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "120 - non optional ledger end"
2025-08-24 05:14:03,179 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "121 - drop participant side command deduplication"
2025-08-24 05:14:03,189 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "122 - participant metering"
2025-08-24 05:14:03,191 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +--------------------------------------------------------------------------+
| ?column?                                                                 |
+--------------------------------------------------------------------------+
| Harmonise the transaction_metering columns with other examples in the db |
+--------------------------------------------------------------------------+

2025-08-24 05:14:03,193 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +--------------------------------------------+
| ?column?                                   |
+--------------------------------------------+
| Add Metering Parameters: Creating table... |
+--------------------------------------------+

2025-08-24 05:14:03,198 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------------------+
| ?column?                                    |
+---------------------------------------------+
| Add Participant Metering: Creating table... |
+---------------------------------------------+

2025-08-24 05:14:03,203 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------------------------+
| ?column?                                      |
+-----------------------------------------------+
| Add Participant Metering: Creating indexes... |
+-----------------------------------------------+

2025-08-24 05:14:03,206 [canton-env-ec-64] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| Add Participant Metering: Done. |
+---------------------------------+

2025-08-24 05:14:03,214 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "123 - remove events view"
2025-08-24 05:14:03,226 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "124 - modifiable users and parties"
2025-08-24 05:14:03,261 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "125 - creates driver metadata"
2025-08-24 05:14:03,269 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "126 - identity provider config"
2025-08-24 05:14:03,285 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "127 - etq completions"
2025-08-24 05:14:03,298 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "128 - etq tables and indexes"
2025-08-24 05:14:03,344 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "129 - identity provider id users parties"
2025-08-24 05:14:03,359 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "130 - etq data migration"
2025-08-24 05:14:03,374 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "131 - etq drop tx id indexes"
2025-08-24 05:14:03,384 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "132 - audience idp config"
2025-08-24 05:14:03,391 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "133 - add domain id"
2025-08-24 05:14:03,403 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "134 - add reassignment events"
2025-08-24 05:14:03,453 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "135 - add indexes for incomplete reassignment queries"
2025-08-24 05:14:03,470 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "136 - add key maintainers"
2025-08-24 05:14:03,479 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "137 - add trace context"
2025-08-24 05:14:03,488 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "138 - drop ledger id"
2025-08-24 05:14:03,495 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "139 - nullable transfer submitter"
2025-08-24 05:14:03,504 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "140 - add package name"
2025-08-24 05:14:03,513 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "141 - add package name"
2025-08-24 05:14:03,521 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "142 - change party collation to binary"
2025-08-24 05:14:03,536 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "143 - add choice package id"
2025-08-24 05:14:03,545 [canton-env-ec-64] INFO  o.f.core.internal.command.DbMigrate - Successfully applied 99 migrations to schema "ledger_api", now at version v143 (execution time 00:07.363s)
2025-08-24 05:14:03,549 [canton-env-ec-64] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 99 steps.
2025-08-24 05:14:03,609 [canton-env-ec-66] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 05:14:03,618 [canton-env-ec-38] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 05:14:03,622 [canton-env-ec-38] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 05:14:03,635 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 05:14:03,654 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 05:14:03,655 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 05:14:03,664 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 05:14:03,664 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 05:14:03,667 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 05:14:03,667 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 05:14:03,674 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 05:14:03,678 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 05:14:03,707 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 05:14:03,732 [canton-env-ec-66] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09
2025-08-24 05:14:03,735 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Initializing new database for participantId 'participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'
2025-08-24 05:14:03,790 [canton-env-ec-66] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes()),0,0)
2025-08-24 05:14:03,800 [canton-env-ec-66] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09`
2025-08-24 05:14:03,802 [canton-env-ec-66] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 05:14:03,807 [canton-env-ec-103] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (10 ms)
2025-08-24 05:14:03,829 [canton-env-ec-38] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (21 ms)
2025-08-24 05:14:03,833 [canton-env-ec-66] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes()).
2025-08-24 05:14:03,921 [canton-env-ec-38] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 05:14:03,921 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 05:14:03,969 [input-mapping-pool-0] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:ab2666c5385d9442fa53ab301c24f276 - Storing at offset=000000000000000001 ConfigurationChanged(
  recordTime = 2025-08-24T05:13:55.263558Z,
  configuration = Configuration(generation = 1, maxDeduplicationDuration = 168h, timeModel = LedgerTimeModel(avgTransactionLatency = 0s, minSkew = 8760h, maxSkew = 8760h)),
  ...
)
2025-08-24 05:14:03,974 [canton-env-ec-103] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 05:14:04,022 [canton-env-ec-66] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 05:14:04,030 [canton-env-ec-66] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup did not find any configuration. Looking for new ledger configurations from the ledger beginning.
2025-08-24 05:14:04,066 [canton-env-ec-65] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - New ledger configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) found at Absolute(000000000000000001)
2025-08-24 05:14:04,333 [canton-env-ec-58] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over plain text with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 05:14:04,334 [canton-env-ec-65] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 05:14:04,405 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:b804fb8464682dd697b4ba88414b77fc - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 05:14:05,808 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0d467e10b404c4debd7a56c18f0977d5 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 05:14:05,809 [canton-env-ec-154] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2f46c46c74384c97f0030fd727b70d23 - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 05:14:05,808 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:92880de5af988a4673555f74ec6cc142 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 05:14:05,809 [canton-env-ec-90] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e22d356a050718f6517b4273ff415254 - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 05:14:05,810 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:223f75d3eecd9b713f67d0bf1d91573d - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 05:14:05,810 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8d6e32e19829e3c72485dbed2208ecd7 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 05:14:05,811 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c0632352616116ff48936805912523b6 - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 05:14:05,811 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9360bfdcd55eb757f6f326b68a00f2bf - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 05:14:05,813 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2e0a6074b8f087b509c082eb10bd500b - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 05:14:05,813 [canton-env-ec-152] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a0b6753def0d3ed6306ae29eb532370b - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 05:14:05,816 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2320f3c172cba563e4b79b3a3b79009b - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 05:14:05,817 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6c818e3b49abc846a6c0e8ab6442eb02 - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 05:14:05,817 [canton-env-ec-152] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:83d59b48e40c97bc1ec6925e2c653297 - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 05:14:05,818 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0da53402e37d7672f56bf957cabb3423 - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 05:14:05,819 [canton-env-ec-155] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9d0ed02528f977fd2788ec21c0774d6d - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 05:14:05,816 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7dd9960fe626733b0c3e991974d82be7 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 05:14:05,820 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d04657a8c7b37d808ddd46c836a05f4c - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 05:14:05,818 [canton-env-ec-103] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9f6e81b8e70e0442ab0b167a4624c2b2 - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 05:14:05,822 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8700775509e1c2a6e7b45a9df54ee4c3 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 05:14:05,817 [canton-env-ec-154] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d28517988f9874dd1022a0246844974a - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 05:14:05,846 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8db33e86f859e047d315dc2cf22485bd - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 05:14:05,846 [canton-env-ec-103] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:850d4c60cbb22b404e7c8ff30149ddfc - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 05:14:05,847 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:209caefd3e0bd2ec0279316ac22f247f - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 05:14:05,846 [canton-env-ec-154] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d192fcb88b71a98e9e846df34a7f3e5c - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 05:14:05,848 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b5b4d505e5ebea02dd977e4872f588d1 - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 05:14:05,846 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:20a5ee0ea7b0a240a1c43d5cf3837a81 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 05:14:05,845 [canton-env-ec-90] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b4ccb1e8681f58cb2c8565186cb845c7 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 05:14:05,850 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:68e2e35a15bab99ce9f634bcd11079fb - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 05:14:05,851 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1031c97d87f17ac3032c19d202714e98 - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 05:14:05,851 [canton-env-ec-155] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c1e3ad7776db49ad8b27c6b2c5bd55f9 - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 05:14:05,849 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d5481e289b19aa8d010588a1ce0ea439 - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 05:14:05,848 [canton-env-ec-152] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ae9e8fe989a5ca1a2632d011414abbd7 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 05:14:07,874 [input-mapping-pool-1] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:4656962edb61120637b3d5144ca38245 - Storing at offset=000000000000000002 PublicPackageUpload(
  recordTime = 2025-08-24T05:14:07.638267Z,
  archives = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  ),
  sourceDescription = 'AdminWorkflowsWithVacuuming'
)
2025-08-24 05:14:08,218 [canton-env-ec-90] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:4656962edb61120637b3d5144ca38245 - Applied topology transaction Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) at 2025-08-24T05:14:08.217192Z
2025-08-24 05:14:08,256 [canton-env-ec-152] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:a8bbf1d08844c2b2dda76e2b33a3c450 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 05:14:08,275 [canton-env-ec-90] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:bdcf8d93836ea17ddcc403b9b24a5a95 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 05:14:08,365 [canton-env-ec-158] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:0e51cc6ae1bfe8ca6e0d3dba009636d5 - Received request for transactions, startExclusive -> '000000000000000002', endInclusive -> , filters -> {participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 05:14:08,443 [canton-env-ec-153] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:5dca944e2b51d3d5f1c9f94b68db94c1 - Applied topology transaction Add OwnerToKeyMapping(PAR::participant1::12208472822e..., SigningPublicKey(id = 1220062a9ab0..., format = Tink, scheme = Ed25519)) at 2025-08-24T05:14:08.442649Z
2025-08-24 05:14:08,472 [canton-env-ec-158] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:5dca944e2b51d3d5f1c9f94b68db94c1 - Applied topology transaction Add OwnerToKeyMapping(PAR::participant1::12208472822e..., EncryptionPublicKey(id = 122095c1aaf7..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) at 2025-08-24T05:14:08.471151Z
2025-08-24 05:14:08,477 [canton-env-ec-153] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Initializing node with id NodeId(participant1::12208472822e...)
2025-08-24 05:14:08,492 [canton-env-ec-153] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:e15fe58cd0f273b33f2a26e5d50d14ee - Reconnecting to domains List(). Already connected: Set()
2025-08-24 05:14:08,496 [canton-env-ec-154] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:e15fe58cd0f273b33f2a26e5d50d14ee - Successfully re-connected to domains List()
2025-08-24 05:14:08,498 [main] INFO  c.d.c.e.CommunityEnvironment tid:e15fe58cd0f273b33f2a26e5d50d14ee - Successfully started all nodes
2025-08-24 05:17:25,182 [canton-env-ec-35] INFO  c.d.c.p.a.DomainConnectivityService:participant=participant1 tid:6e929103a37b7f48944696bba1e37904 - Registering mydomain with DomainConnectionConfig(
  domain = Domain 'mydomain',
  sequencerConnections = Sequencer 'DefaultSequencer' -> GrpcSequencerConnection(endpoints = http://0.0.0.0:5018, transportSecurity = false),
  manualConnect = true
)
2025-08-24 05:17:25,249 [canton-env-ec-161] INFO  c.d.c.c.d.g.SequencerInfoLoader:participant=participant1 tid:8f9662f62697279774437f09cafc4cec - Version handshake with sequencer Sequencer 'DefaultSequencer' and domain using protocol version 7 succeeded.
2025-08-24 05:17:25,388 [canton-env-ec-161] INFO  c.d.c.c.d.g.SequencerInfoLoader:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Version handshake with sequencer Sequencer 'DefaultSequencer' and domain using protocol version 7 succeeded.
2025-08-24 05:17:25,555 [canton-env-ec-65] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Applied topology transaction Add ParticipantState(To, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) at 2025-08-24T05:17:25.554091Z
2025-08-24 05:17:25,644 [canton-env-ec-153] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - No event found up to 0001-01-01T00:00:00Z. Resubscribing from the beginning.
2025-08-24 05:17:25,645 [canton-env-ec-153] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Processing events from the SequencedEventStore from 0001-01-01T00:00:00.000001Z on
2025-08-24 05:17:25,648 [canton-env-ec-153] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Starting subscription for alias=SEQ::mydomain::1220afab1ca3... at timestamp None; next counter 0
2025-08-24 05:17:25,649 [canton-env-ec-153] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::1220afab1ca3 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:25,657 [canton-env-ec-152] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - UNM::f38f22e095d43d099ef6b6db27ea773f4e8abb32::12208472822e... subscribes from counter=0
2025-08-24 05:17:25,740 [canton-env-ec-38] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'UNM::f38f22e095d43d099ef6b6db27ea773f4e8abb32::12208472822e...' sends request with id '2a4ddf74-af48-476f-b007-f0931d2fadf3' of size 1430 bytes with 1 envelopes.
2025-08-24 05:17:25,764 [canton-env-ec-38] INFO  c.d.c.d.s.s.SequencerReader$EventsReader:domain=mydomain/subscriber=UNM::f38f22e095d43d099ef6b6db27ea773f4e8abb32::12208472822e tid:51149b66b0325eaee8beec5e7a6983ef - Using approximate topology snapshot at 2025-08-24T05:13:56.326824Z for desired timestamp 2025-08-24T05:17:25.748918Z
2025-08-24 05:17:25,870 [canton-env-ec-161] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Applied topology transaction Add NamespaceDelegation(12208472822e..., SigningPublicKey(id = 12208472822e..., format = Tink, scheme = Ed25519), true) at 2025-08-24T05:17:25.869636Z
2025-08-24 05:17:25,890 [canton-env-ec-161] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Applied topology transaction Add OwnerToKeyMapping(PAR::participant1::12208472822e..., SigningPublicKey(id = 1220062a9ab0..., format = Tink, scheme = Ed25519)) at 2025-08-24T05:17:25.889782Z
2025-08-24 05:17:25,904 [canton-env-ec-65] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Applied topology transaction Add OwnerToKeyMapping(PAR::participant1::12208472822e..., EncryptionPublicKey(id = 122095c1aaf7..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) at 2025-08-24T05:17:25.903297Z
2025-08-24 05:17:25,926 [canton-env-ec-38] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'DOM::mydomain::1220afab1ca3...' sends request with id '420e60f4-6258-4411-9993-655579962c7d' of size 928 bytes with 1 envelopes.
2025-08-24 05:17:25,927 [canton-env-ec-90] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Setting participant PAR::participant1::12208472822e... state to ParticipantAttributes(Submission,Ordinary)
2025-08-24 05:17:25,927 [canton-env-ec-90] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Applied topology transaction Add ParticipantState(From, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) at 2025-08-24T05:17:25.926338Z
2025-08-24 05:17:25,942 [canton-env-ec-35] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Applied topology transaction Add ParticipantState(To, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) at 2025-08-24T05:17:25.941691Z
2025-08-24 05:17:25,946 [canton-env-ec-65] INFO  c.d.c.d.t.RequestProcessingStrategy$Impl:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Successfully onboarded PAR::participant1::12208472822e...
2025-08-24 05:17:25,957 [canton-env-ec-65] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Register topology request by UNM::f38f22e095d43d099ef6b6db27ea773f4e8abb32::12208472822e... for PAR::participant1::12208472822e... yielded
  Accepted -> Add NamespaceDelegation(12208472822e..., SigningPublicKey(id = 12208472822e..., format = Tink, scheme = Ed25519), true)
  Accepted -> Add OwnerToKeyMapping(PAR::participant1::12208472822e..., SigningPublicKey(id = 1220062a9ab0..., format = Tink, scheme = Ed25519))
  Accepted -> Add OwnerToKeyMapping(PAR::participant1::12208472822e..., EncryptionPublicKey(id = 122095c1aaf7..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM))
  Accepted -> Add ParticipantState(To, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary)
2025-08-24 05:17:25,979 [canton-env-ec-38] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/1 Add NamespaceDelegation(12208472822e..., SigningPublicKey(id = 12208472822e..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T05:17:26.184791Z) (epsilon=250 ms)
2025-08-24 05:17:25,985 [canton-env-ec-38] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'DOM::mydomain::1220afab1ca3...' sends request with id 'c87e8040-aebd-4e6d-87c6-24c00dff9967' of size 559 bytes with 1 envelopes.
2025-08-24 05:17:25,989 [canton-env-ec-152] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/1 Add NamespaceDelegation(12208472822e..., SigningPublicKey(id = 12208472822e..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T05:17:26.184791Z) (epsilon=250 ms)
2025-08-24 05:17:25,994 [canton-env-ec-38] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'DOM::mydomain::1220afab1ca3...' sends request with id '1f3a8654-41de-46c8-b0b8-f871daf1b788' of size 1570 bytes with 1 envelopes.
2025-08-24 05:17:26,045 [canton-env-ec-157] INFO  c.d.c.s.c.ResilientSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 05:17:26,046 [canton-env-ec-157] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::1220afab1ca3 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 05:17:26,075 [canton-env-ec-90] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/3 Add OwnerToKeyMapping(PAR::participant1::12208472822e..., SigningPublicKey(id = 1220062a9ab0..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:17:26.254739Z) (epsilon=250 ms)
2025-08-24 05:17:26,075 [canton-env-ec-90] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 2/3 Add OwnerToKeyMapping(PAR::participant1::12208472822e..., EncryptionPublicKey(id = 122095c1aaf7..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) with ts=EffectiveTime(2025-08-24T05:17:26.254739Z) (epsilon=250 ms)
2025-08-24 05:17:26,076 [canton-env-ec-90] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 3/3 Add ParticipantState(From, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T05:17:26.254739Z) (epsilon=250 ms)
2025-08-24 05:17:26,076 [canton-env-ec-161] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - GRPC subscription successfully closed due to client shutdown.
2025-08-24 05:17:26,075 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/3 Add OwnerToKeyMapping(PAR::participant1::12208472822e..., SigningPublicKey(id = 1220062a9ab0..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:17:26.254739Z) (epsilon=250 ms)
2025-08-24 05:17:26,077 [canton-env-ec-90] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Request c.d.c.d.a.v.SequencerService/SubscribeUnauthenticatedVersioned by /[0:0:0:0:0:0:0:1%0]:53284: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 05:17:26,078 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 2/3 Add OwnerToKeyMapping(PAR::participant1::12208472822e..., EncryptionPublicKey(id = 122095c1aaf7..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) with ts=EffectiveTime(2025-08-24T05:17:26.254739Z) (epsilon=250 ms)
2025-08-24 05:17:26,079 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 3/3 Add ParticipantState(From, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T05:17:26.254739Z) (epsilon=250 ms)
2025-08-24 05:17:26,080 [canton-env-ec-152] INFO  c.d.c.d.t.MemberTopologyCatchup:domain=mydomain/identity tid:51149b66b0325eaee8beec5e7a6983ef - Participant PAR::participant1::12208472822e... is changing from None to Some(Submission), requires to catch up 10
2025-08-24 05:17:26,098 [canton-env-ec-37] INFO  c.d.c.c.d.g.GrpcSequencerConnectClient:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - The operation 'verify active' was not successful. New kind of error: no success error (request infinite retries). Retrying after 0.5s. Result: Right(false). 
2025-08-24 05:17:26,102 [canton-env-ec-154] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'DOM::mydomain::1220afab1ca3...' sends request with id '47c66cda-9763-45b8-af3a-725095c4f2ec' of size 2694 bytes with 1 envelopes.
2025-08-24 05:17:26,163 [canton-env-ec-157] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'DOM::mydomain::1220afab1ca3...' sends request with id '585c5cba-7641-46be-839a-4951ac53f787' of size 948 bytes with 1 envelopes.
2025-08-24 05:17:26,212 [canton-env-ec-155] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/1 Add ParticipantState(To, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T05:17:26.420291Z) (epsilon=250 ms)
2025-08-24 05:17:26,217 [canton-env-ec-152] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/1 Add ParticipantState(To, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T05:17:26.420291Z) (epsilon=250 ms)
2025-08-24 05:17:26,603 [canton-env-ec-152] INFO  c.d.c.c.d.g.GrpcSequencerConnectClient:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Now retrying operation 'verify active'. 
2025-08-24 05:17:26,655 [canton-env-ec-90] INFO  c.d.c.p.s.SyncDomainEphemeralStateFactoryImpl:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Computed starting points: ProcessingStartingPoints(
  clean replay = MessageCleanReplayStartingPoint(next request counter = 0, next sequencer counter = 0, prenext timestamp = 0001-01-01T00:00:00Z),
  processing = MessageProcessingStartingPoint(next request counter = 0, next sequencer counter = 0, prenext timestamp = 0001-01-01T00:00:00Z),
  rewound sequencer counter prehead = None()
)
2025-08-24 05:17:26,656 [canton-env-ec-90] INFO  c.d.c.s.d.DbCursorPreheadStore:participant=participant1/domain-alias=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Rewinding prehead to None
2025-08-24 05:17:26,838 [canton-env-ec-90] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 - Will use parallelism 8 when computing ACS commitments
2025-08-24 05:17:26,848 [canton-env-ec-38] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 - Initialized from stored snapshot at RecordTime(timestamp = 0001-01-01T00:00:00Z, tieBreaker = -9223372036854775808) (might be incomplete)
2025-08-24 05:17:26,849 [canton-env-ec-90] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:26,849 [canton-env-ec-152] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 - Initialized the ACS commitment processor queue
2025-08-24 05:17:26,850 [canton-env-ec-90] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:26,850 [canton-env-ec-90] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:26,850 [canton-env-ec-90] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:26,851 [canton-env-ec-90] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:26,851 [canton-env-ec-90] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Connected to domain and starting synchronisation: Domain 'mydomain'
2025-08-24 05:17:26,884 [canton-env-ec-90] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Found 0 repair requests at request counters Seq()
2025-08-24 05:17:26,894 [canton-env-ec-152] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - No event found up to 0001-01-01T00:00:00Z. Resubscribing from the beginning.
2025-08-24 05:17:26,894 [canton-env-ec-152] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Processing events from the SequencedEventStore from 0001-01-01T00:00:00.000001Z on
2025-08-24 05:17:26,903 [canton-env-ec-158] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Starting subscription for alias=SEQ::mydomain::1220afab1ca3... at timestamp None; next counter 0
2025-08-24 05:17:26,904 [canton-env-ec-158] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::1220afab1ca3 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:26,905 [canton-env-ec-158] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:26,906 [canton-env-ec-158] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:26,940 [canton-env-ec-153] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::12208472822e... authenticated new token with expiry 2025-08-24T06:17:26.936545Z
2025-08-24 05:17:26,947 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - PAR::participant1::12208472822e... subscribes from counter=0
2025-08-24 05:17:26,972 [canton-env-ec-90] INFO  c.d.c.s.c.SequencedEventValidatorImpl:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::1220afab1ca3...
2025-08-24 05:17:26,983 [canton-env-ec-153] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Caught up with batch with counter=0 with sequencer with 872 ms delay
2025-08-24 05:17:26,984 [canton-env-ec-153] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Replaying requests 0 up to clean prehead -1
2025-08-24 05:17:26,984 [canton-env-ec-153] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Replaying or processing locally stored events with sequencer counters 0 to -1
2025-08-24 05:17:27,010 [canton-env-ec-152] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Processing event at sc=0, ts=2025-08-24T05:17:26.111877Z, with contents=DomainTopologyTransactionMessage
2025-08-24 05:17:27,013 [canton-env-ec-154] INFO  c.d.c.t.p.DomainTopologyTransactionMessageValidator$Impl:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Using approximate topology snapshot at 0001-01-01T00:00:00.000001Z for desired timestamp 2025-08-24T05:17:26.111877Z
2025-08-24 05:17:27,054 [canton-env-ec-37] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Updated topology change delay from=0s to 0.25s
2025-08-24 05:17:27,055 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/10 Add NamespaceDelegation(1220afab1ca3..., SigningPublicKey(id = 1220afab1ca3..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,056 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 2/10 Add OwnerToKeyMapping(DOM::mydomain::1220afab1ca3..., SigningPublicKey(id = 12209b443c21..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,057 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 3/10 Add OwnerToKeyMapping(MED::mydomain::1220afab1ca3..., SigningPublicKey(id = 1220f6603a99..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,058 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 4/10 Add OwnerToKeyMapping(SEQ::mydomain::1220afab1ca3..., SigningPublicKey(id = 1220f8fc1dc6..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,059 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 5/10 Add NamespaceDelegation(12208472822e..., SigningPublicKey(id = 12208472822e..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,060 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 6/10 Add ParticipantState(From, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,060 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 7/10 Add MediatorDomainState(Both, mydomain::1220afab1ca3..., MED::mydomain::1220afab1ca3...) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,061 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 8/10 Add OwnerToKeyMapping(PAR::participant1::12208472822e..., SigningPublicKey(id = 1220062a9ab0..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,062 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 9/10 Add OwnerToKeyMapping(PAR::participant1::12208472822e..., EncryptionPublicKey(id = 122095c1aaf7..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,063 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 10/10 Replace DomainParametersChange(
  mydomain::1220afab1ca3...,
  DynamicDomainParameters(
    participant response timeout = 30s,
    mediator reaction timeout = 30s,
    transfer exclusivity timeout = 1m,
    topology change delay = 0.25s,
    ledger time record time tolerance = 1m,
    mediator deduplication timeout = 2m,
    reconciliation interval = 1m,
    max rate per participant = 1000000,
    max request size = 10485760,
    catchup config = CatchUpConfig(catchUpIntervalSkip = 5, nrIntervalsToTriggerCatchUp = 2)
  )
) with ts=EffectiveTime(2025-08-24T05:17:26.111877Z) (epsilon=0 ms)
2025-08-24 05:17:27,113 [canton-env-ec-157] INFO  c.d.c.p.t.c.MissingKeysAlerter:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Domain mydomain::1220afab1ca3... update my participant permission as of 2025-08-24T05:17:26.111877Z to Submission, Ordinary
2025-08-24 05:17:27,166 [canton-env-ec-37] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Processing event at sc=1, ts=2025-08-24T05:17:26.170291Z, with contents=DomainTopologyTransactionMessage
2025-08-24 05:17:27,166 [input-mapping-pool-2] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing at offset=000000000000000003 PartyAddedToParticipant(recordTime = 2025-08-24T05:17:27.120705Z, party = participant1::12208472822e..., displayName = '', participantId = participant1::12208472822e..., ...)
2025-08-24 05:17:27,176 [canton-env-ec-161] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/1 Add ParticipantState(To, mydomain::1220afab1ca3..., PAR::participant1::12208472822e..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T05:17:26.420291Z) (epsilon=250 ms)
2025-08-24 05:17:27,489 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'PAR::participant1::12208472822e...' sends request with id '3691057e-debb-4589-ab1c-844ba02d3cf5' of size 2097 bytes with 1 envelopes.
2025-08-24 05:17:27,541 [canton-env-ec-158] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Processing event at sc=2, ts=2025-08-24T05:17:27.499987Z, messageId=3691057e-debb-4589-ab1c-844ba02d3cf5, with contents=Seq()
2025-08-24 05:17:27,549 [canton-env-ec-153] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Applied topology transaction Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) at 2025-08-24T05:17:27.548226Z
2025-08-24 05:17:27,554 [canton-env-ec-154] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Register topology request by PAR::participant1::12208472822e... for PAR::participant1::12208472822e... yielded
  Accepted -> Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
)
2025-08-24 05:17:27,570 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'DOM::mydomain::1220afab1ca3...' sends request with id '7f7f7bc0-f123-429f-a429-b164a65db6d6' of size 536 bytes with 1 envelopes.
2025-08-24 05:17:27,572 [canton-env-ec-158] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'DOM::mydomain::1220afab1ca3...' sends request with id '0f5e92b0-2c26-4781-bef0-e6f7e04f8386' of size 2230 bytes with 1 envelopes.
2025-08-24 05:17:27,604 [canton-env-ec-157] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Processing event at sc=3, ts=2025-08-24T05:17:27.577270Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 05:17:27,615 [canton-env-ec-161] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Processing event at sc=4, ts=2025-08-24T05:17:27.580301Z, with contents=DomainTopologyTransactionMessage
2025-08-24 05:17:27,615 [canton-env-ec-157] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:27,616 [canton-env-ec-157] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:27,617 [canton-env-ec-157] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 05:17:27,621 [canton-env-ec-152] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T05:17:27.830301Z) (epsilon=250 ms)
2025-08-24 05:17:27,625 [canton-env-ec-152] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T05:17:27.830301Z) (epsilon=250 ms)
2025-08-24 05:17:27,630 [canton-env-ec-161] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:51149b66b0325eaee8beec5e7a6983ef - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T05:17:27.830301Z) (epsilon=250 ms)
2025-08-24 05:17:27,661 [canton-env-ec-152] INFO  c.d.c.p.a.s.ApiActiveContractsService:participant=participant1 tid:0c6451d9be46bdaf0a6a2ea567e3a04b - Received request for active contracts: GetActiveContractsRequest(participant1,Some(TransactionFilter(Map(participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09 -> Filters(Some(InclusiveFilters(Vector(Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,PingProposal), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Ping), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Pong), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Explode), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Merge), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Collapse)),Vector(),Vector())))))),false,), filters -> {participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09: ['65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Pong', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Collapse', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:PingProposal', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Explode', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Merge', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Ping']}.
2025-08-24 05:17:28,134 [canton-env-ec-161] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - 'PAR::participant1::12208472822e...' sends request with id 'tick-5992b17a-b63a-4ce5-a743-db5bc25c7f64' of size 343 bytes with 0 envelopes.
2025-08-24 05:18:53,970 [canton-env-ec-157] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:777cca81e6297079fdcfb62d3edb2007 - Applied topology transaction Add PartyToParticipant(Both, NewBank::12208472822e..., PAR::participant1::12208472822e..., Submission) at 2025-08-24T05:18:53.970191Z
2025-08-24 05:18:53,990 [canton-env-ec-154] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:7a7eea840c384e7c901eec9c766d31d0 - 'PAR::participant1::12208472822e...' sends request with id '149a592a-315b-4167-acdd-5a747639d0d3' of size 837 bytes with 1 envelopes.
2025-08-24 05:18:54,030 [canton-env-ec-35] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7a7eea840c384e7c901eec9c766d31d0 - Processing event at sc=6, ts=2025-08-24T05:18:53.997649Z, messageId=149a592a-315b-4167-acdd-5a747639d0d3, with contents=Seq()
2025-08-24 05:18:54,037 [canton-env-ec-157] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:7a7eea840c384e7c901eec9c766d31d0 - Applied topology transaction Add PartyToParticipant(Both, NewBank::12208472822e..., PAR::participant1::12208472822e..., Submission) at 2025-08-24T05:18:54.036603Z
2025-08-24 05:18:54,041 [canton-env-ec-161] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:7a7eea840c384e7c901eec9c766d31d0 - Register topology request by PAR::participant1::12208472822e... for PAR::participant1::12208472822e... yielded
  Accepted -> Add PartyToParticipant(Both, NewBank::12208472822e..., PAR::participant1::12208472822e..., Submission)
2025-08-24 05:18:54,053 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:7a7eea840c384e7c901eec9c766d31d0 - 'DOM::mydomain::1220afab1ca3...' sends request with id '6799edc8-6f7e-428c-854c-d0f31e790e36' of size 534 bytes with 1 envelopes.
2025-08-24 05:18:54,058 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:7a7eea840c384e7c901eec9c766d31d0 - 'DOM::mydomain::1220afab1ca3...' sends request with id 'be1b86ab-afc9-4b0a-bece-38fbe0483111' of size 949 bytes with 1 envelopes.
2025-08-24 05:18:54,097 [canton-env-ec-158] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7a7eea840c384e7c901eec9c766d31d0 - Processing event at sc=7, ts=2025-08-24T05:18:54.058908Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 05:18:54,104 [canton-env-ec-90] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7a7eea840c384e7c901eec9c766d31d0 - Processing event at sc=8, ts=2025-08-24T05:18:54.065458Z, with contents=DomainTopologyTransactionMessage
2025-08-24 05:18:54,108 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:7a7eea840c384e7c901eec9c766d31d0 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBank::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:18:54.315458Z) (epsilon=250 ms)
2025-08-24 05:18:54,112 [canton-env-ec-158] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:7a7eea840c384e7c901eec9c766d31d0 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBank::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:18:54.315458Z) (epsilon=250 ms)
2025-08-24 05:18:54,119 [canton-env-ec-90] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:7a7eea840c384e7c901eec9c766d31d0 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBank::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:18:54.315458Z) (epsilon=250 ms)
2025-08-24 05:18:54,412 [input-mapping-pool-3] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:7a7eea840c384e7c901eec9c766d31d0 - Storing at offset=000000000000000004 PartyAddedToParticipant(recordTime = 2025-08-24T05:18:54.374149Z, party = NewBank::12208472822e..., displayName = '', participantId = participant1::12208472822e..., ...)
2025-08-24 05:18:59,501 [canton-env-ec-158] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:2a7c38cd21eebb6d4c2a0d44cf2b2d40 - Applied topology transaction Add PartyToParticipant(Both, NewAlice::12208472822e..., PAR::participant1::12208472822e..., Submission) at 2025-08-24T05:18:59.501674Z
2025-08-24 05:18:59,525 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:35b3a1386694da019ec5e77f4e6adf44 - 'PAR::participant1::12208472822e...' sends request with id 'c50b4063-4731-4cdc-b3d0-3a5f0917d40b' of size 847 bytes with 1 envelopes.
2025-08-24 05:18:59,565 [canton-env-ec-58] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:35b3a1386694da019ec5e77f4e6adf44 - Processing event at sc=9, ts=2025-08-24T05:18:59.533721Z, messageId=c50b4063-4731-4cdc-b3d0-3a5f0917d40b, with contents=Seq()
2025-08-24 05:18:59,572 [canton-env-ec-154] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:35b3a1386694da019ec5e77f4e6adf44 - Applied topology transaction Add PartyToParticipant(Both, NewAlice::12208472822e..., PAR::participant1::12208472822e..., Submission) at 2025-08-24T05:18:59.571814Z
2025-08-24 05:18:59,575 [canton-env-ec-36] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:35b3a1386694da019ec5e77f4e6adf44 - Register topology request by PAR::participant1::12208472822e... for PAR::participant1::12208472822e... yielded
  Accepted -> Add PartyToParticipant(Both, NewAlice::12208472822e..., PAR::participant1::12208472822e..., Submission)
2025-08-24 05:18:59,586 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:35b3a1386694da019ec5e77f4e6adf44 - 'DOM::mydomain::1220afab1ca3...' sends request with id 'c21233af-6db2-4096-bea0-b9d11dbfed59' of size 536 bytes with 1 envelopes.
2025-08-24 05:18:59,593 [canton-env-ec-154] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:35b3a1386694da019ec5e77f4e6adf44 - 'DOM::mydomain::1220afab1ca3...' sends request with id '92f2cbfc-01fc-46c5-a5b1-4e68c0a72c4b' of size 957 bytes with 1 envelopes.
2025-08-24 05:18:59,614 [canton-env-ec-154] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:35b3a1386694da019ec5e77f4e6adf44 - Processing event at sc=10, ts=2025-08-24T05:18:59.591981Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 05:18:59,622 [canton-env-ec-155] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:35b3a1386694da019ec5e77f4e6adf44 - Processing event at sc=11, ts=2025-08-24T05:18:59.599552Z, with contents=DomainTopologyTransactionMessage
2025-08-24 05:18:59,626 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:35b3a1386694da019ec5e77f4e6adf44 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewAlice::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:18:59.849552Z) (epsilon=250 ms)
2025-08-24 05:18:59,629 [canton-env-ec-153] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:35b3a1386694da019ec5e77f4e6adf44 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewAlice::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:18:59.849552Z) (epsilon=250 ms)
2025-08-24 05:18:59,632 [canton-env-ec-154] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:35b3a1386694da019ec5e77f4e6adf44 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewAlice::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:18:59.849552Z) (epsilon=250 ms)
2025-08-24 05:18:59,927 [input-mapping-pool-4] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:35b3a1386694da019ec5e77f4e6adf44 - Storing at offset=000000000000000005 PartyAddedToParticipant(recordTime = 2025-08-24T05:18:59.888109Z, party = NewAlice::12208472822e..., displayName = '', participantId = participant1::12208472822e..., ...)
2025-08-24 05:19:07,609 [canton-env-ec-58] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:1f8b1c9c62892260d86ce6b6b777a6bc - Applied topology transaction Add PartyToParticipant(Both, NewBob::12208472822e..., PAR::participant1::12208472822e..., Submission) at 2025-08-24T05:19:07.609286Z
2025-08-24 05:19:07,633 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:74f468920f307398f63c806f70ccc6a8 - 'PAR::participant1::12208472822e...' sends request with id 'efb88214-e1dc-44e2-9b7d-729c34a24c7c' of size 836 bytes with 1 envelopes.
2025-08-24 05:19:07,669 [canton-env-ec-153] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:74f468920f307398f63c806f70ccc6a8 - Processing event at sc=12, ts=2025-08-24T05:19:07.640699Z, messageId=efb88214-e1dc-44e2-9b7d-729c34a24c7c, with contents=Seq()
2025-08-24 05:19:07,675 [canton-env-ec-158] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:74f468920f307398f63c806f70ccc6a8 - Applied topology transaction Add PartyToParticipant(Both, NewBob::12208472822e..., PAR::participant1::12208472822e..., Submission) at 2025-08-24T05:19:07.674625Z
2025-08-24 05:19:07,677 [canton-env-ec-90] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:74f468920f307398f63c806f70ccc6a8 - Register topology request by PAR::participant1::12208472822e... for PAR::participant1::12208472822e... yielded
  Accepted -> Add PartyToParticipant(Both, NewBob::12208472822e..., PAR::participant1::12208472822e..., Submission)
2025-08-24 05:19:07,689 [canton-env-ec-161] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:74f468920f307398f63c806f70ccc6a8 - 'DOM::mydomain::1220afab1ca3...' sends request with id '1e9e26af-004c-4e68-bf1a-6a50eef39505' of size 534 bytes with 1 envelopes.
2025-08-24 05:19:07,691 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:74f468920f307398f63c806f70ccc6a8 - 'DOM::mydomain::1220afab1ca3...' sends request with id '879458e6-6742-44b8-a062-4568c701a447' of size 954 bytes with 1 envelopes.
2025-08-24 05:19:07,712 [canton-env-ec-58] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:74f468920f307398f63c806f70ccc6a8 - Processing event at sc=13, ts=2025-08-24T05:19:07.693920Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 05:19:07,720 [canton-env-ec-90] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:74f468920f307398f63c806f70ccc6a8 - Processing event at sc=14, ts=2025-08-24T05:19:07.696450Z, with contents=DomainTopologyTransactionMessage
2025-08-24 05:19:07,726 [canton-env-ec-37] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:74f468920f307398f63c806f70ccc6a8 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBob::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:19:07.946450Z) (epsilon=250 ms)
2025-08-24 05:19:07,733 [canton-env-ec-90] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:74f468920f307398f63c806f70ccc6a8 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBob::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:19:07.946450Z) (epsilon=250 ms)
2025-08-24 05:19:07,733 [canton-env-ec-58] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:74f468920f307398f63c806f70ccc6a8 - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBob::12208472822e..., PAR::participant1::12208472822e..., Submission) with ts=EffectiveTime(2025-08-24T05:19:07.946450Z) (epsilon=250 ms)
2025-08-24 05:19:08,031 [input-mapping-pool-5] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:74f468920f307398f63c806f70ccc6a8 - Storing at offset=000000000000000006 PartyAddedToParticipant(recordTime = 2025-08-24T05:19:07.997340Z, party = NewBob::12208472822e..., displayName = '', participantId = participant1::12208472822e..., ...)
2025-08-24 05:20:03,744 [daml.index.db.threadpool.connection.indexer-0] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes()),2025-08-24T05:00:00Z)
2025-08-24 05:20:03,747 [daml.index.db.threadpool.connection.indexer-0] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes()),2025-08-24T05:00:00Z)
2025-08-24 05:20:24,349 [input-mapping-pool-6] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:73a74e3bf3666754ac0e95a7475ac6d8 - Storing at offset=000000000000000007 PublicPackageUpload(
  recordTime = 2025-08-24T05:20:24.225181Z,
  archives = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  ),
  sourceDescription = 'RWA'
)
2025-08-24 05:20:24,482 [canton-env-ec-157] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:73a74e3bf3666754ac0e95a7475ac6d8 - Applied topology transaction Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) at 2025-08-24T05:20:24.481723Z
2025-08-24 05:20:24,515 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:9c06eb295f8199c84a26173f05a9eb17 - 'PAR::participant1::12208472822e...' sends request with id '024035e4-54ff-4483-ab50-332da90e4398' of size 1984 bytes with 1 envelopes.
2025-08-24 05:20:24,547 [canton-env-ec-154] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:9c06eb295f8199c84a26173f05a9eb17 - Processing event at sc=15, ts=2025-08-24T05:20:24.521388Z, messageId=024035e4-54ff-4483-ab50-332da90e4398, with contents=Seq()
2025-08-24 05:20:24,554 [canton-env-ec-37] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:9c06eb295f8199c84a26173f05a9eb17 - Applied topology transaction Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) at 2025-08-24T05:20:24.553762Z
2025-08-24 05:20:24,560 [canton-env-ec-153] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:9c06eb295f8199c84a26173f05a9eb17 - Register topology request by PAR::participant1::12208472822e... for PAR::participant1::12208472822e... yielded
  Accepted -> Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
)
2025-08-24 05:20:24,569 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:9c06eb295f8199c84a26173f05a9eb17 - 'DOM::mydomain::1220afab1ca3...' sends request with id '703e69c9-df51-4649-92ea-83bc951bece4' of size 536 bytes with 1 envelopes.
2025-08-24 05:20:24,573 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:9c06eb295f8199c84a26173f05a9eb17 - 'DOM::mydomain::1220afab1ca3...' sends request with id 'a7c1e118-7e69-45f3-acdc-a391a1173c5d' of size 2113 bytes with 1 envelopes.
2025-08-24 05:20:24,596 [canton-env-ec-157] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:9c06eb295f8199c84a26173f05a9eb17 - Processing event at sc=16, ts=2025-08-24T05:20:24.575322Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 05:20:24,604 [canton-env-ec-103] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:9c06eb295f8199c84a26173f05a9eb17 - Processing event at sc=17, ts=2025-08-24T05:20:24.579879Z, with contents=DomainTopologyTransactionMessage
2025-08-24 05:20:24,606 [canton-env-ec-154] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:9c06eb295f8199c84a26173f05a9eb17 - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T05:20:24.829879Z) (epsilon=250 ms)
2025-08-24 05:20:24,612 [canton-env-ec-90] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:9c06eb295f8199c84a26173f05a9eb17 - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T05:20:24.829879Z) (epsilon=250 ms)
2025-08-24 05:20:24,612 [canton-env-ec-58] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:9c06eb295f8199c84a26173f05a9eb17 - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::12208472822e...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T05:20:24.829879Z) (epsilon=250 ms)
2025-08-24 05:20:24,937 [canton-env-ec-90] INFO  c.d.c.p.a.s.a.ApiPackageManagementService:participant=participant1 tid:25ec14f53257671ec6c2c8dd36663094 - Listing known packages.
2025-08-24 05:30:59,465 [canton-env-ec-35] INFO  c.d.c.p.a.s.c.CommandSubmissionServiceImpl:participant=participant1 tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Phase 1 started: Submitting commands for interpretation: Commands(
  commandId = 821599fe-2d8f-495c-8ad5-b6f325f7e324,
  submissionId = ec2d2eca-cd77-4a32-9ef9-3ae9976ab5a8,
  applicationId = CantonConsole,
  actAs = NewBank::12208472822e...,
  submittedAt = 2025-08-24T05:30:59.459506Z,
  ledgerEffectiveTime = 2025-08-24T05:30:59.459496Z,
  deduplicationPeriod = (duration=PT168H),
  ...
).
2025-08-24 05:31:00,919 [canton-env-ec-154] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Phase 1 completed: Submitting 4 envelopes for Transaction request, submitters NewBank::12208472822e..., command-id 821599fe-2d8f-495c-8ad5-b6f325f7e324
2025-08-24 05:31:00,940 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:61f2299f41db5e5e3f7e52a9891fe0b8 - 'PAR::participant1::12208472822e...' sends request with id 'f5e513e0-3182-428b-b968-053f30fe5efa' of size 3834 bytes with 4 envelopes.
2025-08-24 05:31:01,016 [canton-env-ec-36] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Processing event at sc=18, ts=2025-08-24T05:31:00.956805Z, messageId=f5e513e0-3182-428b-b968-053f30fe5efa, with contents=Seq(
  RootHashMessage(root hash = SHA-256:83f22bc55e49..., payload size = 0),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:dc978e7ad759...), view type = TransactionViewType, size = 1053),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:29ee5c316282...), view type = TransactionViewType, size = 1054)
)
2025-08-24 05:31:01,035 [canton-env-ec-153] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Phase 3: Validating Transaction request=2025-08-24T05:31:00.956805Z with 2 envelope(s)
2025-08-24 05:31:01,123 [canton-env-ec-35] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Phase 2: Registered request=2025-08-24T05:31:00.956805Z with 2 view(s). Initial state: Map(
  ViewPosition(L) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::12208472822e... -> 1, threshold = 1), Quorum(confirmers = participant1::12208472822e... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::12208472822e... -> ConsortiumVotingState(), participant1::12208472822e... -> ConsortiumVotingState()),
    rejections = Seq()
  ),
  ViewPosition(R) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::12208472822e... -> 1, threshold = 1), Quorum(confirmers = participant1::12208472822e... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::12208472822e... -> ConsortiumVotingState(), participant1::12208472822e... -> ConsortiumVotingState()),
    rejections = Seq()
  )
)
2025-08-24 05:31:01,422 [canton-env-ec-158] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Phase 4: Sending for request=2025-08-24T05:31:00.956805Z with msgId=cbf5b73d-109c-4ddd-a002-28901734fc11 approved=2, rejected=0
2025-08-24 05:31:01,437 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:61f2299f41db5e5e3f7e52a9891fe0b8 - 'PAR::participant1::12208472822e...' sends request with id 'cbf5b73d-109c-4ddd-a002-28901734fc11' of size 840 bytes with 2 envelopes.
2025-08-24 05:31:01,468 [canton-env-ec-36] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Processing event at sc=19, ts=2025-08-24T05:31:01.443797Z, messageId=cbf5b73d-109c-4ddd-a002-28901734fc11, with contents=Seq()
2025-08-24 05:31:01,490 [canton-env-ec-103] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Phase 5: Received responses for request=RequestId(2025-08-24T05:31:00.956805Z): ParticipantResponse(sender = PAR::participant1::12208472822e..., ts = 2025-08-24T05:31:01.443797Z, approved = 2)
2025-08-24 05:31:01,530 [canton-env-ec-65] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Phase 6: Finalized request=RequestId(2025-08-24T05:31:00.956805Z) with verdict Approve
2025-08-24 05:31:01,554 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:61f2299f41db5e5e3f7e52a9891fe0b8 - 'MED::mydomain::1220afab1ca3...' sends request with id 'mmid:verdict:1756013460956805' of size 716 bytes with 1 envelopes.
2025-08-24 05:31:01,598 [canton-env-ec-65] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Processing event at sc=20, ts=2025-08-24T05:31:01.562157Z, with contents=SignedProtocolMessage(
  TransactionResultMessage(requestId = 2025-08-24T05:31:00.956805Z, verdict = Approve, rootHash = SHA-256:83f22bc55e49..., domainId = mydomain::1220afab1ca3...),
  signatures = Signature(signature = 76fa7b227893, signedBy = 1220f6603a99...)
)
2025-08-24 05:31:01,790 [input-mapping-pool-7] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:61f2299f41db5e5e3f7e52a9891fe0b8 - Phase 7: Storing at offset=000000000000000008 TransactionAccepted(
  recordTime = 2025-08-24T05:31:00.956805Z,
  transactionId = 122083f22bc55e4946d19355802e96c8fba03b06fce70bf15c5dd226fa9d84760f57,
  transactionMeta = TransactionMeta(ledgerEffectiveTime = 2025-08-24T05:30:59.459496Z, submissionTime = 2025-08-24T05:30:59.459496Z, domainId = mydomain::1220afab1ca3..., ...),
  completion = CompletionInfo(
    actAs = NewBank::12208472822e...,
    commandId = 821599fe-2d8f-495c-8ad5-b6f325f7e324,
    applicationId = CantonConsole,
    deduplication period = (offset=Offset(Bytes(000000000000000001))),
    submissionId = Some(ec2d2eca-cd77-4a32-9ef9-3ae9976ab5a8),
    ...
  ),
  nodes = 2,
  roots = 2,
  ...
)
2025-08-24 05:31:01,872 [canton-env-ec-158] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '122083f22bc55e4946d19355802e96c8fba03b06fce70bf15c5dd226fa9d84760f57', parties -> ['NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:31:01,996 [canton-env-ec-90] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:33561d90379492fc78d3f710fccf586b - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '122083f22bc55e4946d19355802e96c8fba03b06fce70bf15c5dd226fa9d84760f57', parties -> ['NewAlice::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:31:02,032 [canton-env-ec-35] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:5a0aa3b386e969bbf49804e09e70cbb0 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '122083f22bc55e4946d19355802e96c8fba03b06fce70bf15c5dd226fa9d84760f57', parties -> ['NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:31:08,344 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiActiveContractsService:participant=participant1 tid:936c6d8f5fd973f54eb18f992bfc6aec - Received request for active contracts: GetActiveContractsRequest(,Some(TransactionFilter(Map(NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09 -> Filters(None)))),true,), filters -> {NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09: 'all-templates'}.
2025-08-24 05:31:31,218 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:61f2299f41db5e5e3f7e52a9891fe0b8 - 'MED::mydomain::1220afab1ca3...' sends request with id 'tick-dc7bc792-a42d-4b0e-a550-bfe8933e59fc' of size 339 bytes with 0 envelopes.
2025-08-24 05:31:39,669 [canton-env-ec-103] INFO  c.d.c.p.a.s.c.CommandSubmissionServiceImpl:participant=participant1 tid:3c8ee0b2a9014704faea041c92ebd9ce - Phase 1 started: Submitting commands for interpretation: Commands(
  commandId = ba19808a-63f6-42a2-b80e-95954b8b12a7,
  submissionId = 5125b78e-6aca-4609-86e8-42ade5da5798,
  applicationId = CantonConsole,
  actAs = NewBank::12208472822e...,
  submittedAt = 2025-08-24T05:31:39.668643Z,
  ledgerEffectiveTime = 2025-08-24T05:31:39.668642Z,
  deduplicationPeriod = (duration=PT168H),
  ...
).
2025-08-24 05:31:39,944 [canton-env-ec-36] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:3c8ee0b2a9014704faea041c92ebd9ce - Phase 1 completed: Submitting 4 envelopes for Transaction request, submitters NewBank::12208472822e..., command-id ba19808a-63f6-42a2-b80e-95954b8b12a7
2025-08-24 05:31:39,956 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:3c8ee0b2a9014704faea041c92ebd9ce - 'PAR::participant1::12208472822e...' sends request with id '9beec2f1-bac0-4aa0-aca8-0e22405c3dc9' of size 4403 bytes with 4 envelopes.
2025-08-24 05:31:39,995 [canton-env-ec-158] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:3c8ee0b2a9014704faea041c92ebd9ce - Processing event at sc=21, ts=2025-08-24T05:31:39.963731Z, messageId=9beec2f1-bac0-4aa0-aca8-0e22405c3dc9, with contents=Seq(
  RootHashMessage(root hash = SHA-256:b14380f4b673..., payload size = 0),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:9320d1645cbe...), view type = TransactionViewType, size = 1318),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:8a33f6f46f3a...), view type = TransactionViewType, size = 1313)
)
2025-08-24 05:31:39,999 [canton-env-ec-90] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:3c8ee0b2a9014704faea041c92ebd9ce - Phase 3: Validating Transaction request=2025-08-24T05:31:39.963731Z with 2 envelope(s)
2025-08-24 05:31:40,008 [canton-env-ec-90] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:3c8ee0b2a9014704faea041c92ebd9ce - Phase 2: Registered request=2025-08-24T05:31:39.963731Z with 2 view(s). Initial state: Map(
  ViewPosition(L) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::12208472822e... -> 1, threshold = 1), Quorum(confirmers = participant1::12208472822e... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::12208472822e... -> ConsortiumVotingState(), participant1::12208472822e... -> ConsortiumVotingState()),
    rejections = Seq()
  ),
  ViewPosition(R) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::12208472822e... -> 1, threshold = 1), Quorum(confirmers = participant1::12208472822e... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::12208472822e... -> ConsortiumVotingState(), participant1::12208472822e... -> ConsortiumVotingState()),
    rejections = Seq()
  )
)
2025-08-24 05:31:40,113 [canton-env-ec-157] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:3c8ee0b2a9014704faea041c92ebd9ce - Phase 4: Sending for request=2025-08-24T05:31:39.963731Z with msgId=6f6b030a-b4f0-486a-98b7-d848d6cbbfd7 approved=2, rejected=0
2025-08-24 05:31:40,122 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:3c8ee0b2a9014704faea041c92ebd9ce - 'PAR::participant1::12208472822e...' sends request with id '6f6b030a-b4f0-486a-98b7-d848d6cbbfd7' of size 839 bytes with 2 envelopes.
2025-08-24 05:31:40,154 [canton-env-ec-153] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:3c8ee0b2a9014704faea041c92ebd9ce - Processing event at sc=22, ts=2025-08-24T05:31:40.129721Z, messageId=6f6b030a-b4f0-486a-98b7-d848d6cbbfd7, with contents=Seq()
2025-08-24 05:31:40,156 [canton-env-ec-154] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:3c8ee0b2a9014704faea041c92ebd9ce - Phase 5: Received responses for request=RequestId(2025-08-24T05:31:39.963731Z): ParticipantResponse(sender = PAR::participant1::12208472822e..., ts = 2025-08-24T05:31:40.129721Z, approved = 2)
2025-08-24 05:31:40,169 [canton-env-ec-154] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:3c8ee0b2a9014704faea041c92ebd9ce - Phase 6: Finalized request=RequestId(2025-08-24T05:31:39.963731Z) with verdict Approve
2025-08-24 05:31:40,182 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:3c8ee0b2a9014704faea041c92ebd9ce - 'MED::mydomain::1220afab1ca3...' sends request with id 'mmid:verdict:1756013499963731' of size 717 bytes with 1 envelopes.
2025-08-24 05:31:40,209 [canton-env-ec-65] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:3c8ee0b2a9014704faea041c92ebd9ce - Processing event at sc=23, ts=2025-08-24T05:31:40.187810Z, with contents=SignedProtocolMessage(
  TransactionResultMessage(requestId = 2025-08-24T05:31:39.963731Z, verdict = Approve, rootHash = SHA-256:b14380f4b673..., domainId = mydomain::1220afab1ca3...),
  signatures = Signature(signature = f82a9f9e9049, signedBy = 1220f6603a99...)
)
2025-08-24 05:31:40,298 [input-mapping-pool-8] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:3c8ee0b2a9014704faea041c92ebd9ce - Phase 7: Storing at offset=000000000000000009 TransactionAccepted(
  recordTime = 2025-08-24T05:31:39.963731Z,
  transactionId = 1220b14380f4b673706f07cfd474869db187cc746e6e940e3d63e8522d998e28334a,
  transactionMeta = TransactionMeta(ledgerEffectiveTime = 2025-08-24T05:31:39.668642Z, submissionTime = 2025-08-24T05:31:39.668642Z, domainId = mydomain::1220afab1ca3..., ...),
  completion = CompletionInfo(
    actAs = NewBank::12208472822e...,
    commandId = ba19808a-63f6-42a2-b80e-95954b8b12a7,
    applicationId = CantonConsole,
    deduplication period = (offset=Offset(Bytes(000000000000000001))),
    submissionId = Some(5125b78e-6aca-4609-86e8-42ade5da5798),
    ...
  ),
  nodes = 6,
  roots = 2,
  ...
)
2025-08-24 05:31:40,326 [canton-env-ec-154] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220b14380f4b673706f07cfd474869db187cc746e6e940e3d63e8522d998e28334a', parties -> ['NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:31:40,367 [canton-env-ec-158] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:e842bc0a5cc0a9ef24d90a5b1641eeef - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220b14380f4b673706f07cfd474869db187cc746e6e940e3d63e8522d998e28334a', parties -> ['NewAlice::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:31:40,426 [canton-env-ec-37] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:adaf27affe083a033ec89352738f0d65 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220b14380f4b673706f07cfd474869db187cc746e6e940e3d63e8522d998e28334a', parties -> ['NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:31:59,718 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:61f2299f41db5e5e3f7e52a9891fe0b8 - 'PAR::participant1::12208472822e...' sends request with id 'tick-96091fd8-e718-4204-bcdb-f88c20abf198' of size 342 bytes with 0 envelopes.
2025-08-24 05:32:01,215 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:61f2299f41db5e5e3f7e52a9891fe0b8 - 'PAR::participant1::12208472822e...' sends request with id 'tick-d787efc1-2c4e-45f1-95c8-56a9724c06cc' of size 343 bytes with 0 envelopes.
2025-08-24 05:32:10,222 [canton-env-ec-154] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:3c8ee0b2a9014704faea041c92ebd9ce - 'MED::mydomain::1220afab1ca3...' sends request with id 'tick-d8490317-01af-42dd-95e1-3f949de068bd' of size 338 bytes with 0 envelopes.
2025-08-24 05:32:39,927 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:61f2299f41db5e5e3f7e52a9891fe0b8 - 'PAR::participant1::12208472822e...' sends request with id 'tick-c78c7918-04ad-4b28-9eba-7344b35fbeb1' of size 342 bytes with 0 envelopes.
2025-08-24 05:32:40,467 [canton-env-ec-90] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:61f2299f41db5e5e3f7e52a9891fe0b8 - 'PAR::participant1::12208472822e...' sends request with id 'tick-0484a304-ca04-4368-bd17-448d20c16c0f' of size 343 bytes with 0 envelopes.
2025-08-24 05:32:42,849 [canton-env-ec-153] INFO  c.d.c.p.a.s.c.CommandSubmissionServiceImpl:participant=participant1 tid:152ad37910a3d993da9b8c696c2a2c16 - Phase 1 started: Submitting commands for interpretation: Commands(
  commandId = 93db82b2-4925-42b0-b63c-658c04fd2d8a,
  submissionId = 1d1afdc6-15b7-46c5-ade2-5d98b97a6456,
  applicationId = CantonConsole,
  actAs = NewBank::12208472822e...,
  submittedAt = 2025-08-24T05:32:42.847335Z,
  ledgerEffectiveTime = 2025-08-24T05:32:42.847330Z,
  deduplicationPeriod = (duration=PT168H),
  ...
).
2025-08-24 05:32:42,964 [canton-env-ec-157] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:152ad37910a3d993da9b8c696c2a2c16 - Phase 1 completed: Submitting 4 envelopes for Transaction request, submitters NewBank::12208472822e..., command-id 93db82b2-4925-42b0-b63c-658c04fd2d8a
2025-08-24 05:32:42,974 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:152ad37910a3d993da9b8c696c2a2c16 - 'PAR::participant1::12208472822e...' sends request with id 'b2db8269-5e4d-492f-8c7a-34498f26042c' of size 4752 bytes with 4 envelopes.
2025-08-24 05:32:43,006 [canton-env-ec-153] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:152ad37910a3d993da9b8c696c2a2c16 - Processing event at sc=28, ts=2025-08-24T05:32:42.981985Z, messageId=b2db8269-5e4d-492f-8c7a-34498f26042c, with contents=Seq(
  RootHashMessage(root hash = SHA-256:fd4ceba59973..., payload size = 0),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:a33291cce991...), view type = TransactionViewType, size = 1481),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:0a50165f05f2...), view type = TransactionViewType, size = 1497)
)
2025-08-24 05:32:43,010 [canton-env-ec-103] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:152ad37910a3d993da9b8c696c2a2c16 - Phase 3: Validating Transaction request=2025-08-24T05:32:42.981985Z with 2 envelope(s)
2025-08-24 05:32:43,024 [canton-env-ec-37] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:152ad37910a3d993da9b8c696c2a2c16 - Phase 2: Registered request=2025-08-24T05:32:42.981985Z with 2 view(s). Initial state: Map(
  ViewPosition(L) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::12208472822e... -> 1, threshold = 1), Quorum(confirmers = participant1::12208472822e... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::12208472822e... -> ConsortiumVotingState(), participant1::12208472822e... -> ConsortiumVotingState()),
    rejections = Seq()
  ),
  ViewPosition(R) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::12208472822e... -> 1, threshold = 1), Quorum(confirmers = participant1::12208472822e... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::12208472822e... -> ConsortiumVotingState(), participant1::12208472822e... -> ConsortiumVotingState()),
    rejections = Seq()
  )
)
2025-08-24 05:32:43,079 [canton-env-ec-157] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:152ad37910a3d993da9b8c696c2a2c16 - Phase 4: Sending for request=2025-08-24T05:32:42.981985Z with msgId=79c2195b-c765-4f1a-ac51-676c70923f65 approved=2, rejected=0
2025-08-24 05:32:43,087 [canton-env-ec-103] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:152ad37910a3d993da9b8c696c2a2c16 - 'PAR::participant1::12208472822e...' sends request with id '79c2195b-c765-4f1a-ac51-676c70923f65' of size 841 bytes with 2 envelopes.
2025-08-24 05:32:43,113 [canton-env-ec-103] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:152ad37910a3d993da9b8c696c2a2c16 - Processing event at sc=29, ts=2025-08-24T05:32:43.093743Z, messageId=79c2195b-c765-4f1a-ac51-676c70923f65, with contents=Seq()
2025-08-24 05:32:43,116 [canton-env-ec-65] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:152ad37910a3d993da9b8c696c2a2c16 - Phase 5: Received responses for request=RequestId(2025-08-24T05:32:42.981985Z): ParticipantResponse(sender = PAR::participant1::12208472822e..., ts = 2025-08-24T05:32:43.093743Z, approved = 2)
2025-08-24 05:32:43,130 [canton-env-ec-103] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:152ad37910a3d993da9b8c696c2a2c16 - Phase 6: Finalized request=RequestId(2025-08-24T05:32:42.981985Z) with verdict Approve
2025-08-24 05:32:43,138 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:152ad37910a3d993da9b8c696c2a2c16 - 'MED::mydomain::1220afab1ca3...' sends request with id 'mmid:verdict:1756013562981985' of size 714 bytes with 1 envelopes.
2025-08-24 05:32:43,165 [canton-env-ec-36] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:152ad37910a3d993da9b8c696c2a2c16 - Processing event at sc=30, ts=2025-08-24T05:32:43.143454Z, with contents=SignedProtocolMessage(
  TransactionResultMessage(requestId = 2025-08-24T05:32:42.981985Z, verdict = Approve, rootHash = SHA-256:fd4ceba59973..., domainId = mydomain::1220afab1ca3...),
  signatures = Signature(signature = fc006f92270e, signedBy = 1220f6603a99...)
)
2025-08-24 05:32:43,231 [input-mapping-pool-9] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:152ad37910a3d993da9b8c696c2a2c16 - Phase 7: Storing at offset=00000000000000000a TransactionAccepted(
  recordTime = 2025-08-24T05:32:42.981985Z,
  transactionId = 1220fd4ceba599734b6bf3781c7bd00c618690808b686292f7c1fd7e79e8051c4fce,
  transactionMeta = TransactionMeta(ledgerEffectiveTime = 2025-08-24T05:32:42.847330Z, submissionTime = 2025-08-24T05:32:42.847330Z, domainId = mydomain::1220afab1ca3..., ...),
  completion = CompletionInfo(
    actAs = NewBank::12208472822e...,
    commandId = 93db82b2-4925-42b0-b63c-658c04fd2d8a,
    applicationId = CantonConsole,
    deduplication period = (offset=Offset(Bytes(000000000000000001))),
    submissionId = Some(1d1afdc6-15b7-46c5-ade2-5d98b97a6456),
    ...
  ),
  nodes = 10,
  roots = 2,
  ...
)
2025-08-24 05:32:43,255 [canton-env-ec-36] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220fd4ceba599734b6bf3781c7bd00c618690808b686292f7c1fd7e79e8051c4fce', parties -> ['NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:32:43,295 [canton-env-ec-154] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:501caba2e26fddcb0837a61ed170c8cf - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220fd4ceba599734b6bf3781c7bd00c618690808b686292f7c1fd7e79e8051c4fce', parties -> ['NewAlice::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:32:43,354 [canton-env-ec-154] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:4bd19c37826a000a0d641eec61176e81 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220fd4ceba599734b6bf3781c7bd00c618690808b686292f7c1fd7e79e8051c4fce', parties -> ['NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:33:07,182 [canton-env-ec-36] INFO  c.d.c.p.a.s.c.CommandSubmissionServiceImpl:participant=participant1 tid:7d6df3454ed46c2d8f40ac107a323635 - Phase 1 started: Submitting commands for interpretation: Commands(
  commandId = 1c157cb8-8da1-4df1-9378-077796a777ba,
  submissionId = 8dd6c921-5dca-4ba5-b1cf-a767d3ba71b3,
  applicationId = CantonConsole,
  actAs = NewBank::12208472822e...,
  submittedAt = 2025-08-24T05:33:07.181171Z,
  ledgerEffectiveTime = 2025-08-24T05:33:07.181169Z,
  deduplicationPeriod = (duration=PT168H),
  ...
).
2025-08-24 05:33:07,262 [canton-env-ec-90] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7d6df3454ed46c2d8f40ac107a323635 - Phase 1 completed: Submitting 4 envelopes for Transaction request, submitters NewBank::12208472822e..., command-id 1c157cb8-8da1-4df1-9378-077796a777ba
2025-08-24 05:33:07,271 [canton-env-ec-157] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:7d6df3454ed46c2d8f40ac107a323635 - 'PAR::participant1::12208472822e...' sends request with id '94494ecc-3b24-434b-8d4c-801f933d3e38' of size 4416 bytes with 4 envelopes.
2025-08-24 05:33:07,305 [canton-env-ec-36] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7d6df3454ed46c2d8f40ac107a323635 - Processing event at sc=31, ts=2025-08-24T05:33:07.277618Z, messageId=94494ecc-3b24-434b-8d4c-801f933d3e38, with contents=Seq(
  RootHashMessage(root hash = SHA-256:711e04587d54..., payload size = 0),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:1d26c1a6fb44...), view type = TransactionViewType, size = 1324),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:85b7ccd2707a...), view type = TransactionViewType, size = 1322)
)
2025-08-24 05:33:07,308 [canton-env-ec-36] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7d6df3454ed46c2d8f40ac107a323635 - Phase 3: Validating Transaction request=2025-08-24T05:33:07.277618Z with 2 envelope(s)
2025-08-24 05:33:07,309 [canton-env-ec-65] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:7d6df3454ed46c2d8f40ac107a323635 - Phase 2: Registered request=2025-08-24T05:33:07.277618Z with 2 view(s). Initial state: Map(
  ViewPosition(L) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::12208472822e... -> 1, threshold = 1), Quorum(confirmers = participant1::12208472822e... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::12208472822e... -> ConsortiumVotingState(), participant1::12208472822e... -> ConsortiumVotingState()),
    rejections = Seq()
  ),
  ViewPosition(R) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::12208472822e... -> 1, threshold = 1), Quorum(confirmers = participant1::12208472822e... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::12208472822e... -> ConsortiumVotingState(), participant1::12208472822e... -> ConsortiumVotingState()),
    rejections = Seq()
  )
)
2025-08-24 05:33:07,358 [canton-env-ec-153] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7d6df3454ed46c2d8f40ac107a323635 - Phase 4: Sending for request=2025-08-24T05:33:07.277618Z with msgId=b726f6fb-3bf6-41a6-8bf0-af3342d62e3e approved=2, rejected=0
2025-08-24 05:33:07,367 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:7d6df3454ed46c2d8f40ac107a323635 - 'PAR::participant1::12208472822e...' sends request with id 'b726f6fb-3bf6-41a6-8bf0-af3342d62e3e' of size 840 bytes with 2 envelopes.
2025-08-24 05:33:07,397 [canton-env-ec-36] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7d6df3454ed46c2d8f40ac107a323635 - Processing event at sc=32, ts=2025-08-24T05:33:07.374202Z, messageId=b726f6fb-3bf6-41a6-8bf0-af3342d62e3e, with contents=Seq()
2025-08-24 05:33:07,398 [canton-env-ec-65] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:7d6df3454ed46c2d8f40ac107a323635 - Phase 5: Received responses for request=RequestId(2025-08-24T05:33:07.277618Z): ParticipantResponse(sender = PAR::participant1::12208472822e..., ts = 2025-08-24T05:33:07.374202Z, approved = 2)
2025-08-24 05:33:07,408 [canton-env-ec-37] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:7d6df3454ed46c2d8f40ac107a323635 - Phase 6: Finalized request=RequestId(2025-08-24T05:33:07.277618Z) with verdict Approve
2025-08-24 05:33:07,419 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:7d6df3454ed46c2d8f40ac107a323635 - 'MED::mydomain::1220afab1ca3...' sends request with id 'mmid:verdict:1756013587277618' of size 712 bytes with 1 envelopes.
2025-08-24 05:33:07,447 [canton-env-ec-153] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::1220afab1ca3 tid:7d6df3454ed46c2d8f40ac107a323635 - Processing event at sc=33, ts=2025-08-24T05:33:07.424071Z, with contents=SignedProtocolMessage(
  TransactionResultMessage(requestId = 2025-08-24T05:33:07.277618Z, verdict = Approve, rootHash = SHA-256:711e04587d54..., domainId = mydomain::1220afab1ca3...),
  signatures = Signature(signature = 6d08e1706c10, signedBy = 1220f6603a99...)
)
2025-08-24 05:33:07,509 [input-mapping-pool-10] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:7d6df3454ed46c2d8f40ac107a323635 - Phase 7: Storing at offset=00000000000000000b TransactionAccepted(
  recordTime = 2025-08-24T05:33:07.277618Z,
  transactionId = 1220711e04587d540b09944c8b72602554eb9df7cff17aca74c6ee8170d458c1476b,
  transactionMeta = TransactionMeta(ledgerEffectiveTime = 2025-08-24T05:33:07.181169Z, submissionTime = 2025-08-24T05:33:07.181169Z, domainId = mydomain::1220afab1ca3..., ...),
  completion = CompletionInfo(
    actAs = NewBank::12208472822e...,
    commandId = 1c157cb8-8da1-4df1-9378-077796a777ba,
    applicationId = CantonConsole,
    deduplication period = (offset=Offset(Bytes(000000000000000001))),
    submissionId = Some(8dd6c921-5dca-4ba5-b1cf-a767d3ba71b3),
    ...
  ),
  nodes = 6,
  roots = 2,
  ...
)
2025-08-24 05:33:07,526 [canton-env-ec-155] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220711e04587d540b09944c8b72602554eb9df7cff17aca74c6ee8170d458c1476b', parties -> ['NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:33:07,562 [canton-env-ec-35] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:e0619fab199bba052f1a3a8997cc8723 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220711e04587d540b09944c8b72602554eb9df7cff17aca74c6ee8170d458c1476b', parties -> ['NewAlice::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'NewBank::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09', 'participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:33:07,612 [canton-env-ec-65] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:b03860c1fd4aff14071f159652eb4c57 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220711e04587d540b09944c8b72602554eb9df7cff17aca74c6ee8170d458c1476b', parties -> ['NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'].
2025-08-24 05:33:13,240 [canton-env-ec-103] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:152ad37910a3d993da9b8c696c2a2c16 - 'MED::mydomain::1220afab1ca3...' sends request with id 'tick-ab470959-c843-48db-85fb-b7e4c0255f08' of size 339 bytes with 0 envelopes.
2025-08-24 05:33:18,259 [canton-env-ec-103] INFO  c.d.c.p.a.s.ApiActiveContractsService:participant=participant1 tid:b11843d0a6cd8e972fb8d3313ce45e9b - Received request for active contracts: GetActiveContractsRequest(,Some(TransactionFilter(Map(NewAlice::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09 -> Filters(None)))),true,), filters -> {NewAlice::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09: 'all-templates'}.
2025-08-24 05:33:37,534 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:152ad37910a3d993da9b8c696c2a2c16 - 'MED::mydomain::1220afab1ca3...' sends request with id 'tick-450f97b1-9721-431b-9e98-c8dd21d1f0f9' of size 338 bytes with 0 envelopes.
2025-08-24 05:33:38,686 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiActiveContractsService:participant=participant1 tid:afb4b88866e891831fadc8964af6e2fb - Received request for active contracts: GetActiveContractsRequest(,Some(TransactionFilter(Map(NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09 -> Filters(None)))),true,), filters -> {NewBob::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09: 'all-templates'}.
2025-08-24 05:33:43,104 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:152ad37910a3d993da9b8c696c2a2c16 - 'PAR::participant1::12208472822e...' sends request with id 'tick-28f43149-0bba-45a5-aeb9-c8f165611467' of size 343 bytes with 0 envelopes.
2025-08-24 05:34:07,440 [canton-env-ec-103] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:152ad37910a3d993da9b8c696c2a2c16 - 'PAR::participant1::12208472822e...' sends request with id 'tick-883ef3d5-ae8d-4063-b2e1-a9e61bc5b7b0' of size 342 bytes with 0 envelopes.
2025-08-24 06:02:03,740 [daml.index.db.threadpool.connection.indexer-0] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T06:00:00Z)
2025-08-24 06:02:03,748 [daml.index.db.threadpool.connection.indexer-0] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T06:00:00Z)
2025-08-24 06:13:36,142 [canton-env-ec-158] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T07:13:36.140333Z
2025-08-24 06:13:36,904 [canton-env-ec-37] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T07:13:36.902295Z
2025-08-24 06:13:56,125 [canton-env-ec-103] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 06:13:56,141 [canton-env-ec-157] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 06:13:56,198 [canton-env-ec-103] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::1220afab1ca3... subscribes from counter=21
2025-08-24 06:13:56,879 [canton-env-ec-158] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 06:13:56,880 [canton-env-ec-103] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 06:13:56,935 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::1220afab1ca3... subscribes from counter=24
2025-08-24 06:17:06,966 [canton-env-ec-157] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::12208472822e... authenticated new token with expiry 2025-08-24T07:17:06.962885Z
2025-08-24 06:17:26,938 [canton-env-ec-153] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - The sequencer subscription has been terminated by the server.
2025-08-24 06:17:26,939 [canton-env-ec-157] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 06:17:26,993 [canton-env-ec-158] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - PAR::participant1::12208472822e... subscribes from counter=35
2025-08-24 06:18:28,250 [ha-polling-checker-timer-thread] INFO  c.d.c.p.i.h.PollingChecker:participant=participant1 - Check failed (FATAL: terminating connection due to administrator command). Calling KillSwitch/abort.
2025-08-24 06:18:28,250 [ha-polling-checker-timer-thread] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Abort called! (check failed, killSwitch aborted)
2025-08-24 06:18:28,251 [ha-polling-checker-timer-thread] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Abort call delegated! (check failed, killSwitch aborted)
2025-08-24 06:18:28,261 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 06:18:28,268 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 06:18:28,269 [canton-env-ec-35] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished with failure: check failed, killSwitch aborted
2025-08-24 06:18:28,270 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 06:18:28,271 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 06:18:28,272 [canton-env-ec-35] ERROR c.d.c.p.i.RecoveringIndexer:participant=participant1 - Error while running indexer, restart scheduled after 10 seconds
java.lang.Exception: check failed, killSwitch aborted
	at com.digitalasset.canton.platform.indexer.ha.PollingChecker.$anonfun$checkInternal$1(PollingChecker.scala:80)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
	at scala.concurrent.package$.blocking(package.scala:124)
	at com.digitalasset.canton.platform.indexer.ha.PollingChecker.checkInternal(PollingChecker.scala:73)
	at com.digitalasset.canton.platform.indexer.ha.PollingChecker.$anonfun$scheduledCheck$1(PollingChecker.scala:69)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
	at scala.concurrent.package$.blocking(package.scala:124)
	at com.digitalasset.canton.platform.indexer.ha.PollingChecker.com$digitalasset$canton$platform$indexer$ha$PollingChecker$$scheduledCheck(PollingChecker.scala:65)
	at com.digitalasset.canton.platform.indexer.ha.PollingChecker$$anon$1.$anonfun$run$1(PollingChecker.scala:41)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at com.digitalasset.canton.platform.indexer.ha.PollingChecker$$anon$1.run(PollingChecker.scala:41)
	at java.base/java.util.TimerThread.mainLoop(Timer.java:566)
	at java.base/java.util.TimerThread.run(Timer.java:516)
Caused by: org.postgresql.util.PSQLException: FATAL: terminating connection due to administrator command
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:371)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:502)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:419)
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194)
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137)
	at anorm.Sql.$anonfun$resultSet$2(Anorm.scala:97)
	at resource.DefaultManagedResource.open(AbstractManagedResource.scala:107)
	at resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:84)
	at resource.ManagedResourceOperations$$anon$1.$anonfun$acquireFor$1(ManagedResourceOperations.scala:47)
	at resource.AbstractManagedResource.$anonfun$acquireFor$1(AbstractManagedResource.scala:85)
	at scala.util.control.Exception$Catch.$anonfun$either$1(Exception.scala:251)
	at scala.util.control.Exception$Catch.apply(Exception.scala:227)
	at scala.util.control.Exception$Catch.either(Exception.scala:251)
	at resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:85)
	at resource.ManagedResourceOperations$$anon$1.acquireFor(ManagedResourceOperations.scala:47)
	at resource.DeferredExtractableManagedResource.acquireFor(AbstractManagedResource.scala:25)
	at resource.ManagedResourceOperations.apply(ManagedResourceOperations.scala:31)
	at resource.ManagedResourceOperations.apply$(ManagedResourceOperations.scala:31)
	at resource.DeferredExtractableManagedResource.apply(AbstractManagedResource.scala:22)
	at resource.ManagedResourceOperations.acquireAndGet(ManagedResourceOperations.scala:29)
	at resource.ManagedResourceOperations.acquireAndGet$(ManagedResourceOperations.scala:29)
	at resource.DeferredExtractableManagedResource.acquireAndGet(AbstractManagedResource.scala:22)
	at anorm.Sql$.$anonfun$asTry$1(Anorm.scala:263)
	at scala.util.Try$.apply(Try.scala:210)
	at anorm.Sql$.asTry(Anorm.scala:263)
	at anorm.WithResult.asTry(SqlResult.scala:215)
	at anorm.WithResult.asTry$(SqlResult.scala:213)
	at anorm.SimpleSql.asTry(SimpleSql.scala:6)
	at anorm.WithResult.as(SqlResult.scala:205)
	at anorm.WithResult.as$(SqlResult.scala:204)
	at anorm.SimpleSql.as(SimpleSql.scala:6)
	at com.digitalasset.canton.platform.store.backend.postgresql.PostgresDBLockStorageBackend$.tryAcquire(PostgresDBLockStorageBackend.scala:23)
	at com.digitalasset.canton.platform.indexer.ha.HaCoordinator$$anon$1.acquireLock$1(HaCoordinator.scala:107)
	at com.digitalasset.canton.platform.indexer.ha.HaCoordinator$$anon$1.acquireMainLock$1(HaCoordinator.scala:114)
	at com.digitalasset.canton.platform.indexer.ha.HaCoordinator$$anon$1.$anonfun$protectedExecution$18(HaCoordinator.scala:156)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at com.digitalasset.canton.platform.indexer.ha.PollingChecker.$anonfun$checkInternal$1(PollingChecker.scala:74)
	... 15 common frames omitted
2025-08-24 06:18:28,276 [canton-env-ec-35] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 06:18:28,321 [DataSourceConnectionProvider-api-server#healthPoller] INFO  com.zaxxer.hikari.pool.PoolBase - daml.index.db.connection.api-server - Failed to validate connection org.postgresql.jdbc.PgConnection@7ad0591c (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:28,323 [DataSourceConnectionProvider-api-server#healthPoller] INFO  com.zaxxer.hikari.pool.PoolBase - daml.index.db.connection.api-server - Failed to validate connection org.postgresql.jdbc.PgConnection@7028326c (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:28,324 [DataSourceConnectionProvider-api-server#healthPoller] INFO  com.zaxxer.hikari.pool.PoolBase - daml.index.db.connection.api-server - Failed to validate connection org.postgresql.jdbc.PgConnection@352aa20d (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:28,325 [DataSourceConnectionProvider-api-server#healthPoller] INFO  com.zaxxer.hikari.pool.PoolBase - daml.index.db.connection.api-server - Failed to validate connection org.postgresql.jdbc.PgConnection@4dca150d (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:29,824 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@2cc2c422 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:29,826 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@7285456b (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:29,826 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@6994c7ea (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:29,827 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@53cded2e (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:29,828 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@5e90fff9 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:29,829 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@666575d (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:29,829 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@49827bec (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:29,830 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@751a5b6f (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:30,159 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-participant1-4 - Failed to validate connection org.postgresql.jdbc.PgConnection@2c2c1d02 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:30,160 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-participant1-4 - Failed to validate connection org.postgresql.jdbc.PgConnection@42c32e9a (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:30,161 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-participant1-4 - Failed to validate connection org.postgresql.jdbc.PgConnection@6525d48e (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:30,162 [canton-env-ec-35] INFO  com.zaxxer.hikari.pool.PoolBase - slick-participant1-4 - Failed to validate connection org.postgresql.jdbc.PgConnection@292d0125 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 06:18:38,307 [canton-env-ec-35] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Restarting Indexer Server
2025-08-24 06:18:38,308 [canton-env-ec-35] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 06:18:38,308 [canton-env-ec-35] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Restarted Indexer Server
2025-08-24 06:18:38,338 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 06:18:38,338 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 06:18:38,340 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 06:18:38,340 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 06:18:38,340 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 06:18:38,341 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 06:18:38,343 [canton-env-ec-58] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 06:18:38,371 [canton-env-ec-58] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 06:18:38,388 [canton-env-ec-58] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09
2025-08-24 06:18:38,392 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09'
2025-08-24 06:18:38,418 [canton-env-ec-36] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(00000000000000000b)),16,7)
2025-08-24 06:18:38,419 [canton-env-ec-36] INFO  c.d.c.p.DispatcherState:participant=participant1 - Stopping active Ledger API offset dispatcher.
2025-08-24 06:18:38,426 [canton-env-ec-36] INFO  c.d.c.p.DispatcherState:participant=participant1 - SERVICE_NOT_RUNNING(1,0): Ledger API offset dispatcher is not running. err-context:{location=DispatcherState.scala:137, service_name=Ledger API offset dispatcher} 
2025-08-24 06:18:38,464 [canton-env-ec-36] INFO  c.d.c.p.DispatcherState:participant=participant1 - SERVICE_NOT_RUNNING(1,0): Ledger API offset dispatcher is not running. err-context:{location=DispatcherState.scala:137, service_name=Ledger API offset dispatcher} 
2025-08-24 06:18:38,467 [canton-env-ec-37] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:0e51cc6ae1bfe8ca6e0d3dba009636d5 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:40882: failed with UNAVAILABLE/SERVICE_NOT_RUNNING(1,0): Ledger API offset dispatcher is not running.
2025-08-24 06:18:38,468 [canton-env-ec-103] WARN  o.a.p.s.s.RestartWithBackoffSource - Restarting stream due to failure [1]: com.daml.error.ErrorCode$LoggedApiException: UNAVAILABLE: SERVICE_NOT_RUNNING(1,0): Ledger API offset dispatcher is not running.
2025-08-24 06:18:38,472 [canton-env-ec-36] INFO  c.d.c.p.a.ResilientTransactionsSubscription:participant=participant1 - Successfully closed ledger subscription admin-ping for PingService [com.digitalasset.canton.participant.ledger.api.client.LedgerConnection$$anon$1$$anon$2@2dd6f42a] closed successfully
2025-08-24 06:18:38,480 [canton-env-ec-36] WARN  c.d.c.p.a.ResilientTransactionsSubscription:participant=participant1 - Ledger subscription admin-ping for PingService failed with an error
io.grpc.StatusRuntimeException: UNAVAILABLE: SERVICE_NOT_RUNNING(1,0): Ledger API offset dispatcher is not running.
	at io.grpc.Status.asRuntimeException(Status.java:532)
	at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at io.opentelemetry.instrumentation.grpc.v1_6.TracingClientInterceptor$TracingClientCall$TracingClientCallListener.onClose(TracingClientInterceptor.java:161)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 06:18:38,481 [canton-env-ec-36] INFO  c.d.c.p.a.ResilientTransactionsSubscription:participant=participant1 - Detected an error.
io.grpc.StatusRuntimeException: UNAVAILABLE: SERVICE_NOT_RUNNING(1,0): Ledger API offset dispatcher is not running.
	at io.grpc.Status.asRuntimeException(Status.java:532)
	at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at io.opentelemetry.instrumentation.grpc.v1_6.TracingClientInterceptor$TracingClientCall$TracingClientCallListener.onClose(TracingClientInterceptor.java:161)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 06:18:38,482 [canton-env-ec-36] INFO  c.d.c.p.a.ResilientTransactionsSubscription:participant=participant1 - The operation 'restartable-PingService-admin-ping' has failed with an exception. New kind of error: unknown error (request infinite retries). Retrying after 1s. 
2025-08-24 06:18:38,693 [canton-env-ec-58] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (226 ms)
2025-08-24 06:18:38,694 [canton-env-ec-157] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (1 ms)
2025-08-24 06:18:38,696 [canton-env-ec-157] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(00000000000000000b)).
2025-08-24 06:18:38,699 [canton-env-ec-157] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 06:18:38,699 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 06:18:39,482 [canton-env-ec-153] INFO  c.d.c.p.a.ResilientTransactionsSubscription:participant=participant1 - Now retrying operation 'restartable-PingService-admin-ping'. 
2025-08-24 06:18:39,490 [canton-env-ec-157] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:3cee2c2884bae2f4d7aba066afd91de5 - Received request for transactions, startExclusive -> '000000000000000002', endInclusive -> , filters -> {participant1::12208472822e0aed84aee2d4e1b4f69da79e697e13d923309d109fb5bbc27df63c09: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 07:00:38,396 [daml.index.db.threadpool.connection.indexer-0] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T07:00:00Z)
2025-08-24 07:00:38,399 [daml.index.db.threadpool.connection.indexer-0] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T07:00:00Z)
2025-08-24 07:13:16,170 [canton-env-ec-37] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T08:13:16.168629Z
2025-08-24 07:13:16,931 [canton-env-ec-35] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T08:13:16.929252Z
2025-08-24 07:13:36,143 [canton-env-ec-58] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 07:13:36,143 [canton-env-ec-35] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 07:13:36,198 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::1220afab1ca3... subscribes from counter=21
2025-08-24 07:13:36,905 [canton-env-ec-58] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 07:13:36,905 [canton-env-ec-36] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 07:13:36,959 [canton-env-ec-158] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::1220afab1ca3... subscribes from counter=24
2025-08-24 07:16:46,994 [canton-env-ec-58] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::12208472822e... authenticated new token with expiry 2025-08-24T08:16:46.990481Z
2025-08-24 07:17:06,965 [canton-env-ec-155] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - The sequencer subscription has been terminated by the server.
2025-08-24 07:17:06,966 [canton-env-ec-58] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 07:17:07,021 [canton-env-ec-103] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - PAR::participant1::12208472822e... subscribes from counter=35
2025-08-24 08:00:38,395 [daml.index.db.threadpool.connection.indexer-9] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T08:00:00Z)
2025-08-24 08:00:38,398 [daml.index.db.threadpool.connection.indexer-9] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T08:00:00Z)
2025-08-24 08:12:56,201 [canton-env-ec-58] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T09:12:56.199927Z
2025-08-24 08:12:56,952 [canton-env-ec-103] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T09:12:56.951008Z
2025-08-24 08:13:16,171 [canton-env-ec-35] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 08:13:16,171 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 08:13:16,227 [canton-env-ec-155] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::1220afab1ca3... subscribes from counter=21
2025-08-24 08:13:16,932 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 08:13:16,933 [canton-env-ec-155] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 08:13:16,988 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::1220afab1ca3... subscribes from counter=24
2025-08-24 08:16:27,022 [canton-env-ec-157] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::12208472822e... authenticated new token with expiry 2025-08-24T09:16:27.018722Z
2025-08-24 08:16:46,992 [canton-env-ec-157] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - The sequencer subscription has been terminated by the server.
2025-08-24 08:16:46,992 [canton-env-ec-153] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 08:16:47,047 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - PAR::participant1::12208472822e... subscribes from counter=35
2025-08-24 09:00:38,393 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T09:00:00Z)
2025-08-24 09:00:38,395 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T09:00:00Z)
2025-08-24 09:12:36,231 [canton-env-ec-37] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T10:12:36.229178Z
2025-08-24 09:12:36,978 [canton-env-ec-103] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T10:12:36.976680Z
2025-08-24 09:12:56,202 [canton-env-ec-158] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 09:12:56,203 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 09:12:56,258 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::1220afab1ca3... subscribes from counter=21
2025-08-24 09:12:56,954 [canton-env-ec-153] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 09:12:56,954 [canton-env-ec-58] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 09:12:57,010 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::1220afab1ca3... subscribes from counter=24
2025-08-24 09:16:07,050 [canton-env-ec-36] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::12208472822e... authenticated new token with expiry 2025-08-24T10:16:07.048429Z
2025-08-24 09:16:27,021 [canton-env-ec-157] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - The sequencer subscription has been terminated by the server.
2025-08-24 09:16:27,022 [canton-env-ec-35] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 09:16:27,077 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - PAR::participant1::12208472822e... subscribes from counter=35
2025-08-24 10:00:38,396 [daml.index.db.threadpool.connection.indexer-8] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T10:00:00Z)
2025-08-24 10:00:38,398 [daml.index.db.threadpool.connection.indexer-8] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T10:00:00Z)
2025-08-24 10:12:16,256 [canton-env-ec-37] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T11:12:16.254224Z
2025-08-24 10:12:17,001 [canton-env-ec-158] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::1220afab1ca3... authenticated new token with expiry 2025-08-24T11:12:16.999089Z
2025-08-24 10:12:36,231 [canton-env-ec-58] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 10:12:36,231 [canton-env-ec-158] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 10:12:36,286 [canton-env-ec-103] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::1220afab1ca3... subscribes from counter=21
2025-08-24 10:12:36,980 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 10:12:36,980 [canton-env-ec-103] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 10:12:37,035 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::1220afab1ca3... subscribes from counter=24
2025-08-24 10:15:47,082 [canton-env-ec-155] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::12208472822e... authenticated new token with expiry 2025-08-24T11:15:47.079418Z
2025-08-24 10:16:07,052 [canton-env-ec-36] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - The sequencer subscription has been terminated by the server.
2025-08-24 10:16:07,052 [canton-env-ec-35] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::1220afab1ca3 tid:51149b66b0325eaee8beec5e7a6983ef - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 10:16:07,108 [canton-env-ec-155] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - PAR::participant1::12208472822e... subscribes from counter=35
2025-08-24 10:54:37,557 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 10:54:37,560 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 10:54:37,563 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,564 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,564 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,565 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,574 [canton-env-ec-58] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 10:54:37,578 [canton-env-ec-155] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:3cee2c2884bae2f4d7aba066afd91de5 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:40882: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 10:54:37,593 [canton-env-ec-37] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 10:54:37,604 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 10:54:37,607 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 10:54:37,608 [canton-env-ec-155] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 10:54:37,608 [canton-env-ec-155] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 10:54:37,609 [canton-env-ec-155] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 10:54:37,612 [canton-env-ec-158] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 10:54:37,624 [canton-env-ec-158] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 10:54:37,624 [canton-env-ec-158] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 10:54:37,624 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 10:54:37,625 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 10:54:37,625 [canton-env-ec-158] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 10:54:37,626 [canton-env-ec-158] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 10:54:37,630 [canton-env-ec-157] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 10:54:37,631 [canton-env-ec-157] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 10:54:37,637 [canton-env-ec-58] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::1220afab1ca3 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,637 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,639 [canton-env-ec-58] INFO  c.d.c.s.c.ResilientSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,639 [canton-env-ec-58] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::1220afab1ca3 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,639 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,641 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::1220afab1ca3 - GRPC subscription successfully closed due to client shutdown.
2025-08-24 10:54:37,641 [canton-env-ec-155] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:51149b66b0325eaee8beec5e7a6983ef - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:47926: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 10:54:37,643 [canton-env-ec-58] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::1220afab1ca3 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,643 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,643 [canton-env-ec-103] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:51149b66b0325eaee8beec5e7a6983ef - Domain 'mydomain' disconnected because sequencer client was closed
2025-08-24 10:54:37,647 [canton-env-ec-58] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,647 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,656 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 10:54:37,656 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 10:54:37,656 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 10:54:37,656 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 10:54:37,657 [canton-env-ec-58] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,657 [canton-env-ec-58] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 10:54:37,658 [canton-env-ec-58] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 10:54:37,660 [canton-env-ec-58] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 10:54:37,660 [canton-env-ec-58] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,661 [canton-env-ec-58] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,663 [canton-env-ec-58] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 10:54:37,665 [canton-env-ec-58] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,665 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,665 [canton-env-ec-58] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,666 [canton-env-ec-58] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,666 [canton-env-ec-35] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 10:54:37,666 [canton-env-ec-158] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:0647c9a24015e068196efaeae04f0985 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:33014: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 10:54:37,669 [canton-env-ec-58] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,669 [canton-env-ec-58] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 10:54:37,669 [canton-env-ec-157] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 10:54:37,669 [canton-env-ec-155] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:f3a621740ec660a828d6c4ae823eef83 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:45224: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 10:54:37,675 [canton-env-ec-58] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 10:54:37,675 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 10:54:37,675 [canton-env-ec-58] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 10:54:37,689 [canton-env-ec-58] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,690 [canton-env-ec-58] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 10:54:37,691 [canton-env-ec-58] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 10:54:37,697 [canton-env-ec-58] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 10:54:37,698 [canton-env-ec-58] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,698 [canton-env-ec-58] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 10:54:37,725 [canton-env-ec-157] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 10:54:37,746 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 10:54:37,746 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 11:02:18,410 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 11:02:19,357 [main] ERROR c.d.canton.CantonCommunityApp$ - GENERIC_CONFIG_ERROR(8,0): Cannot convert configuration to a config of class com.digitalasset.canton.config.CantonCommunityConfig. Failures are:
  at 'canton.domains.mydomain.admin-api.tls.client-auth':
    - (config/canton-production.conf: 93) Expected type OBJECT. Found STRING instead.
  at 'canton.domains.mydomain.admin-api.tls.enabled':
    - (config/canton-production.conf: 89) Unknown key.
  at 'canton.domains.mydomain.public-api.tls.enabled':
    - (config/canton-production.conf: 78) Unknown key.
  at 'canton.domains.mydomain.public-api.tls.trust-collection-file':
    - (config/canton-production.conf: 81) Unknown key.
  at 'canton.participants.participant1.admin-api.tls.client-auth':
    - (config/canton-production.conf: 43) Expected type OBJECT. Found STRING instead.
  at 'canton.participants.participant1.admin-api.tls.enabled':
    - (config/canton-production.conf: 39) Unknown key.
  at 'canton.participants.participant1.ledger-api.tls.client-auth':
    - (config/canton-production.conf: 55) Expected type OBJECT. Found STRING instead.
  at 'canton.participants.participant1.ledger-api.tls.enabled':
    - (config/canton-production.conf: 51) Unknown key.
 err-context:{location=CantonConfig.scala:1651} 
2025-08-24 11:02:19,361 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 11:02:19,362 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 11:02:19,362 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 11:30:56,076 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 11:30:57,144 [main] ERROR c.d.canton.CantonCommunityApp$ - GENERIC_CONFIG_ERROR(8,0): Cannot convert configuration to a config of class com.digitalasset.canton.config.CantonCommunityConfig. Failures are:
  at 'canton.domains.mydomain.admin-api.tls.client-auth':
    - (config/canton-production.conf: 88) Expected type OBJECT. Found STRING instead.
  at 'canton.domains.mydomain.public-api.tls.trust-collection-file':
    - (config/canton-production.conf: 77) Unknown key.
  at 'canton.participants.participant1.admin-api.tls.client-auth':
    - (config/canton-production.conf: 41) Expected type OBJECT. Found STRING instead.
  at 'canton.participants.participant1.ledger-api.tls.client-auth':
    - (config/canton-production.conf: 52) Expected type OBJECT. Found STRING instead.
 err-context:{location=CantonConfig.scala:1651} 
2025-08-24 11:30:57,148 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 11:30:57,149 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 11:30:57,149 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 11:36:04,274 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 11:36:05,250 [main] ERROR c.d.canton.CantonCommunityApp$ - GENERIC_CONFIG_ERROR(8,0): Cannot convert configuration to a config of class com.digitalasset.canton.config.CantonCommunityConfig. Failures are:
  at 'canton.domains.mydomain.public-api.tls.trust-collection-file':
    - (config/canton-production.conf: 81) Unknown key.
 err-context:{location=CantonConfig.scala:1651} 
2025-08-24 11:36:05,254 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 11:36:05,255 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 11:36:05,256 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 11:36:51,937 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 11:36:53,083 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 11:36:53,105 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 11:36:53,312 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 11:36:53,393 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 11:36:53,459 [main] INFO  c.d.c.e.CommunityEnvironment tid:bc0ab5c3f138a0d18cd3fa41921e1f30 - Automatically starting all instances
2025-08-24 11:36:53,518 [canton-env-ec-38] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 11:36:53,518 [canton-env-ec-35] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 11:36:53,544 [canton-env-ec-38] INFO  c.d.c.resource.DbStorage:mydomain tid:da436ffd96faee9dd5260411676df69d - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 11:36:53,545 [canton-env-ec-35] INFO  c.d.c.r.DbStorage:participant1 tid:ca709e5866aa4a59947032bdf2504b57 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 11:36:53,578 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 11:36:53,578 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 11:36:55,015 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 11:36:55,015 [canton-env-ec-38] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 11:36:55,049 [canton-env-ec-38] WARN  c.d.c.resource.DbStorage:mydomain tid:da436ffd96faee9dd5260411676df69d - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 11:36:55,049 [canton-env-ec-35] WARN  c.d.c.r.DbStorage:participant1 tid:ca709e5866aa4a59947032bdf2504b57 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 11:36:55,356 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 11:36:55,357 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 11:36:56,398 [canton-env-ec-38] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 11:36:56,407 [canton-env-ec-38] WARN  c.d.c.resource.DbStorage:mydomain tid:da436ffd96faee9dd5260411676df69d - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 11:36:56,414 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 11:36:56,422 [canton-env-ec-35] WARN  c.d.c.r.DbStorage:participant1 tid:ca709e5866aa4a59947032bdf2504b57 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 11:36:56,711 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 11:36:56,724 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 11:36:57,737 [canton-env-ec-38] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 11:36:57,745 [canton-env-ec-38] WARN  c.d.c.resource.DbStorage:mydomain tid:da436ffd96faee9dd5260411676df69d - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 11:36:57,764 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 11:36:57,769 [canton-env-ec-35] WARN  c.d.c.r.DbStorage:participant1 tid:ca709e5866aa4a59947032bdf2504b57 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 11:36:58,048 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 11:36:58,071 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 11:36:59,071 [canton-env-ec-38] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 11:36:59,082 [canton-env-ec-38] ERROR c.d.c.resource.DbStorage:mydomain tid:da436ffd96faee9dd5260411676df69d - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 11:36:59,094 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 11:36:59,098 [canton-env-ec-35] ERROR c.d.c.r.DbStorage:participant1 tid:ca709e5866aa4a59947032bdf2504b57 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 11:36:59,117 [main] ERROR c.d.c.e.CommunityEnvironment tid:bc0ab5c3f138a0d18cd3fa41921e1f30 - Failed to start mydomain: failed to migrate database of mydomain: DatabaseError(
  Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

)
2025-08-24 11:36:59,119 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 11:36:59,122 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 11:36:59,172 [canton-env-ec-53] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 11:36:59,200 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 11:36:59,200 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 12:43:10,877 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 12:43:11,887 [main] ERROR c.d.canton.CantonCommunityApp$ - GENERIC_CONFIG_ERROR(8,0): Cannot convert configuration to a config of class com.digitalasset.canton.config.CantonCommunityConfig. Failures are:
  at 'canton.domains.mydomain.public-api.tls.trust-collection-file':
    - (config/canton-production.conf: 81) Unknown key.
 err-context:{location=CantonConfig.scala:1651} 
2025-08-24 12:43:11,891 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 12:43:11,891 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 12:43:11,891 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 12:46:00,424 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 12:46:01,549 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=yes
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 12:46:01,571 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 12:46:01,812 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 12:46:01,926 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 12:46:02,004 [main] INFO  c.d.c.e.CommunityEnvironment tid:e4b7b35dfae72ede7cc377aff29fe0bc - Automatically starting all instances
2025-08-24 12:46:02,061 [canton-env-ec-36] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 12:46:02,061 [canton-env-ec-35] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 12:46:02,082 [canton-env-ec-35] INFO  c.d.c.r.DbStorage:participant1 tid:ff131550663de472e562edcc10f7a0d3 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:46:02,082 [canton-env-ec-36] INFO  c.d.c.resource.DbStorage:mydomain tid:ebc78d13e129cfe4692f8b917fa62587 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:46:02,113 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:46:02,113 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:46:03,596 [canton-env-ec-36] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:46:03,596 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:46:03,636 [canton-env-ec-36] WARN  c.d.c.resource.DbStorage:mydomain tid:ebc78d13e129cfe4692f8b917fa62587 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:46:03,636 [canton-env-ec-35] WARN  c.d.c.r.DbStorage:participant1 tid:ff131550663de472e562edcc10f7a0d3 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:46:03,943 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:46:03,943 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:46:04,996 [canton-env-ec-36] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:46:05,004 [canton-env-ec-36] WARN  c.d.c.resource.DbStorage:mydomain tid:ebc78d13e129cfe4692f8b917fa62587 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:46:05,028 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:46:05,036 [canton-env-ec-35] WARN  c.d.c.r.DbStorage:participant1 tid:ff131550663de472e562edcc10f7a0d3 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:46:05,308 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:46:05,339 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:46:06,333 [canton-env-ec-36] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:46:06,341 [canton-env-ec-36] WARN  c.d.c.resource.DbStorage:mydomain tid:ebc78d13e129cfe4692f8b917fa62587 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:46:06,373 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:46:06,377 [canton-env-ec-35] WARN  c.d.c.r.DbStorage:participant1 tid:ff131550663de472e562edcc10f7a0d3 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:46:06,644 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:46:06,680 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:46:07,682 [canton-env-ec-36] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:46:07,688 [canton-env-ec-36] ERROR c.d.c.resource.DbStorage:mydomain tid:ebc78d13e129cfe4692f8b917fa62587 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:46:07,714 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:46:07,719 [canton-env-ec-35] ERROR c.d.c.r.DbStorage:participant1 tid:ff131550663de472e562edcc10f7a0d3 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:46:07,749 [main] ERROR c.d.c.e.CommunityEnvironment tid:e4b7b35dfae72ede7cc377aff29fe0bc - Failed to start mydomain: failed to migrate database of mydomain: DatabaseError(
  Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

)
2025-08-24 12:46:07,752 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 12:46:07,756 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 12:46:07,798 [canton-env-ec-52] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 12:46:07,821 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 12:46:07,821 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 12:47:33,777 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 12:47:34,859 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=yes
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 12:47:34,880 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 12:47:35,087 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 12:47:35,168 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 12:47:35,239 [main] INFO  c.d.c.e.CommunityEnvironment tid:2288ebeadb609e3395bbacdae113349d - Automatically starting all instances
2025-08-24 12:47:35,296 [canton-env-ec-36] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 12:47:35,296 [canton-env-ec-37] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 12:47:35,324 [canton-env-ec-37] INFO  c.d.c.r.DbStorage:participant1 tid:b88f1e168684aac85c09a9d8068df8b2 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:47:35,324 [canton-env-ec-36] INFO  c.d.c.resource.DbStorage:mydomain tid:1f67b957a590dbd96d35701a0325beb4 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:47:35,356 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:47:35,356 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:47:36,870 [canton-env-ec-36] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:47:36,870 [canton-env-ec-37] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:47:36,917 [canton-env-ec-37] WARN  c.d.c.r.DbStorage:participant1 tid:b88f1e168684aac85c09a9d8068df8b2 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:47:36,917 [canton-env-ec-36] WARN  c.d.c.resource.DbStorage:mydomain tid:1f67b957a590dbd96d35701a0325beb4 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:47:37,228 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:47:37,228 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:47:38,270 [canton-env-ec-37] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:47:38,275 [canton-env-ec-37] WARN  c.d.c.r.DbStorage:participant1 tid:b88f1e168684aac85c09a9d8068df8b2 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:47:38,275 [canton-env-ec-36] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:47:38,284 [canton-env-ec-36] WARN  c.d.c.resource.DbStorage:mydomain tid:1f67b957a590dbd96d35701a0325beb4 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:47:38,578 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:47:38,587 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:47:39,614 [canton-env-ec-37] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:47:39,619 [canton-env-ec-37] WARN  c.d.c.r.DbStorage:participant1 tid:b88f1e168684aac85c09a9d8068df8b2 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:47:39,619 [canton-env-ec-36] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:47:39,627 [canton-env-ec-36] WARN  c.d.c.resource.DbStorage:mydomain tid:1f67b957a590dbd96d35701a0325beb4 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:47:39,921 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:47:39,929 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:47:40,942 [canton-env-ec-37] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:47:40,950 [canton-env-ec-37] ERROR c.d.c.r.DbStorage:participant1 tid:b88f1e168684aac85c09a9d8068df8b2 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:47:40,953 [canton-env-ec-36] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:47:40,961 [canton-env-ec-36] ERROR c.d.c.resource.DbStorage:mydomain tid:1f67b957a590dbd96d35701a0325beb4 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:47:40,992 [main] ERROR c.d.c.e.CommunityEnvironment tid:2288ebeadb609e3395bbacdae113349d - Failed to start mydomain: failed to migrate database of mydomain: DatabaseError(
  Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

)
2025-08-24 12:47:40,995 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 12:47:40,999 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 12:47:41,050 [canton-env-ec-49] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 12:47:41,084 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 12:47:41,084 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 12:48:05,676 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 12:48:06,702 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=yes
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 12:48:06,724 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 12:48:06,918 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 12:48:07,002 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 12:48:07,063 [main] INFO  c.d.c.e.CommunityEnvironment tid:53d640d517a3c5a170c911383576c768 - Automatically starting all instances
2025-08-24 12:48:07,120 [canton-env-ec-38] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 12:48:07,120 [canton-env-ec-35] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 12:48:07,140 [canton-env-ec-38] INFO  c.d.c.r.DbStorage:participant1 tid:2818722f18d5fbf687c1438a286f08cb - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:48:07,140 [canton-env-ec-35] INFO  c.d.c.resource.DbStorage:mydomain tid:625536898e6930554e0cdb776e05256c - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:48:07,166 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:48:07,166 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:48:08,678 [canton-env-ec-38] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:48:08,678 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:48:08,722 [canton-env-ec-35] WARN  c.d.c.resource.DbStorage:mydomain tid:625536898e6930554e0cdb776e05256c - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:48:08,722 [canton-env-ec-38] WARN  c.d.c.r.DbStorage:participant1 tid:2818722f18d5fbf687c1438a286f08cb - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:48:09,032 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:48:09,032 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:48:10,067 [canton-env-ec-38] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:48:10,071 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:48:10,076 [canton-env-ec-38] WARN  c.d.c.r.DbStorage:participant1 tid:2818722f18d5fbf687c1438a286f08cb - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:48:10,079 [canton-env-ec-35] WARN  c.d.c.resource.DbStorage:mydomain tid:625536898e6930554e0cdb776e05256c - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:48:10,380 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:48:10,381 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:48:11,413 [canton-env-ec-38] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:48:11,422 [canton-env-ec-38] WARN  c.d.c.r.DbStorage:participant1 tid:2818722f18d5fbf687c1438a286f08cb - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:48:11,422 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:48:11,431 [canton-env-ec-35] WARN  c.d.c.resource.DbStorage:mydomain tid:625536898e6930554e0cdb776e05256c - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:48:11,725 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:48:11,733 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:48:12,748 [canton-env-ec-38] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:48:12,756 [canton-env-ec-38] ERROR c.d.c.r.DbStorage:participant1 tid:2818722f18d5fbf687c1438a286f08cb - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:48:12,756 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:48:12,766 [canton-env-ec-35] ERROR c.d.c.resource.DbStorage:mydomain tid:625536898e6930554e0cdb776e05256c - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:48:12,795 [main] ERROR c.d.c.e.CommunityEnvironment tid:53d640d517a3c5a170c911383576c768 - Failed to start mydomain: failed to migrate database of mydomain: DatabaseError(
  Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

)
2025-08-24 12:48:12,797 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 12:48:12,802 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 12:48:12,862 [canton-env-ec-37] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 12:48:12,894 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 12:48:12,894 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 12:55:40,784 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 12:55:41,910 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=yes
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user="${env:CANTON_DB_USER}"
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 12:55:41,932 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 12:55:42,192 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 12:55:42,266 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 12:55:42,328 [main] INFO  c.d.c.e.CommunityEnvironment tid:caeeb5c5ab7468182721983848f1fd80 - Automatically starting all instances
2025-08-24 12:55:42,378 [canton-env-ec-35] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 12:55:42,378 [canton-env-ec-41] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 12:55:42,400 [canton-env-ec-35] INFO  c.d.c.resource.DbStorage:mydomain tid:d8eccc4a4d6372a8a5111e372a7b93fc - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:55:42,400 [canton-env-ec-41] INFO  c.d.c.r.DbStorage:participant1 tid:9b90c89728b6dd0d5345d2336be4b3f7 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:55:42,429 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:55:42,430 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:55:43,927 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:55:43,934 [canton-env-ec-41] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:55:43,977 [canton-env-ec-41] WARN  c.d.c.r.DbStorage:participant1 tid:9b90c89728b6dd0d5345d2336be4b3f7 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:55:43,977 [canton-env-ec-35] WARN  c.d.c.resource.DbStorage:mydomain tid:d8eccc4a4d6372a8a5111e372a7b93fc - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:55:44,284 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:55:44,284 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:55:45,325 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:55:45,327 [canton-env-ec-41] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:55:45,334 [canton-env-ec-41] WARN  c.d.c.r.DbStorage:participant1 tid:9b90c89728b6dd0d5345d2336be4b3f7 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:55:45,335 [canton-env-ec-35] WARN  c.d.c.resource.DbStorage:mydomain tid:d8eccc4a4d6372a8a5111e372a7b93fc - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:55:45,637 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:55:45,639 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:55:46,661 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:55:46,661 [canton-env-ec-41] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:55:46,665 [canton-env-ec-35] WARN  c.d.c.resource.DbStorage:mydomain tid:d8eccc4a4d6372a8a5111e372a7b93fc - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:55:46,668 [canton-env-ec-41] WARN  c.d.c.r.DbStorage:participant1 tid:9b90c89728b6dd0d5345d2336be4b3f7 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, retrying in 300ms: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:55:46,968 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:55:46,971 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:55:48,008 [canton-env-ec-35] ERROR com.zaxxer.hikari.pool.HikariPool - slick-mydomain-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:55:48,009 [canton-env-ec-41] ERROR com.zaxxer.hikari.pool.HikariPool - slick-participant1-2 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
2025-08-24 12:55:48,017 [canton-env-ec-35] ERROR c.d.c.resource.DbStorage:mydomain tid:d8eccc4a4d6372a8a5111e372a7b93fc - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:55:48,018 [canton-env-ec-41] ERROR c.d.c.r.DbStorage:participant1 tid:9b90c89728b6dd0d5345d2336be4b3f7 - Operation com.digitalasset.canton.resource.DbStorage.createDatabase failed, exhausted retries: Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

2025-08-24 12:55:48,055 [main] ERROR c.d.c.e.CommunityEnvironment tid:caeeb5c5ab7468182721983848f1fd80 - Failed to start mydomain: failed to migrate database of mydomain: DatabaseError(
  Failed to connect to database: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at com.zaxxer.hikari.pool.HikariPool.throwPoolInitializationException(HikariPool.java:576)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115)
	at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:81)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:92)
	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21)
	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47)
	at com.digitalasset.canton.resource.DbStorage$.createJdbcBackendDatabase(Storage.scala:764)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$4(Storage.scala:741)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retry$1(RetryEither.scala:42)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing(PerformUnlessClosing.scala:62)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.performUnlessClosing$(PerformUnlessClosing.scala:54)
	at com.digitalasset.canton.environment.ManagedNodes.performUnlessClosing(Nodes.scala:104)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1(RetryEither.scala:62)
	at com.digitalasset.canton.util.retry.RetryEither$.$anonfun$retryUnlessShutdown$1$adapted(RetryEither.scala:59)
	at cats.data.EitherTMonad.$anonfun$tailRecM$1(EitherT.scala:1212)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.go$1(UnlessShutdown.scala:96)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:101)
	at com.digitalasset.canton.lifecycle.UnlessShutdown$$anon$1.tailRecM(UnlessShutdown.scala:90)
	at cats.data.EitherTMonad.tailRecM(EitherT.scala:1211)
	at cats.data.EitherTMonad.tailRecM$(EitherT.scala:1209)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.data.EitherTInstances1$$anon$18.tailRecM(EitherT.scala:1102)
	at cats.syntax.FlatMapIdOps$.tailRecM$extension(flatMap.scala:135)
	at com.digitalasset.canton.util.retry.RetryEither$.retryUnlessShutdown(RetryEither.scala:59)
	at com.digitalasset.canton.util.retry.RetryEither$.retry(RetryEither.scala:41)
	at com.digitalasset.canton.resource.DbStorage$.$anonfun$createDatabase$1(Storage.scala:747)
	at com.digitalasset.canton.tracing.TraceContext$.withNewTraceContext(TraceContext.scala:97)
	at com.digitalasset.canton.resource.DbStorage$.createDatabase(Storage.scala:686)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb(DbMigrations.scala:78)
	at com.digitalasset.canton.resource.DbMigrations.withCreatedDb$(DbMigrations.scala:68)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withCreatedDb(DbMigrations.scala:293)
	at com.digitalasset.canton.resource.CommunityDbMigrations.withDb(DbMigrations.scala:305)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate(DbMigrations.scala:185)
	at com.digitalasset.canton.resource.DbMigrations.checkAndMigrate$(DbMigrations.scala:180)
	at com.digitalasset.canton.resource.CommunityDbMigrations.checkAndMigrate(DbMigrations.scala:293)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$checkMigration$1(Nodes.scala:330)
	at com.digitalasset.canton.environment.ManagedNodes.runIfUsingDatabase(Nodes.scala:301)
	at com.digitalasset.canton.environment.ManagedNodes.checkMigration(Nodes.scala:312)
	at com.digitalasset.canton.environment.ManagedNodes.$anonfun$startNode$1(Nodes.scala:160)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "${env:CANTON_DB_USER}"
	at org.postgresql.Driver$ConnectThread.getResult(Driver.java:397)
	at org.postgresql.Driver.connect(Driver.java:305)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:683)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:230)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:104)
	at org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:88)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
	... 47 more

)
2025-08-24 12:55:48,058 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 12:55:48,065 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 12:55:48,115 [canton-env-ec-36] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 12:55:48,148 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 12:55:48,148 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 12:56:54,426 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 12:56:55,448 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 12:56:55,474 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 12:56:55,720 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 12:56:55,823 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 12:56:55,890 [main] INFO  c.d.c.e.CommunityEnvironment tid:a794be004251092305f2a32c0642933b - Automatically starting all instances
2025-08-24 12:56:55,943 [canton-env-ec-36] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 12:56:55,943 [canton-env-ec-41] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 12:56:55,964 [canton-env-ec-36] INFO  c.d.c.resource.DbStorage:mydomain tid:bc6d13c537c1435c41b2d9d2270e7e92 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:56:55,964 [canton-env-ec-41] INFO  c.d.c.r.DbStorage:participant1 tid:98d8952f60128242d59472958d37c828 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:56:56,003 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:56:56,003 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:56:56,674 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Start completed.
2025-08-24 12:56:56,674 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Start completed.
2025-08-24 12:56:56,899 [canton-env-ec-41] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 12:56:56,899 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 12:56:56,909 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 12:56:56,909 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 12:56:56,909 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 12:56:56,909 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 12:56:56,910 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 12:56:56,910 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 12:56:56,984 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 12:56:56,984 [canton-env-ec-41] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 12:56:56,994 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:56:56,994 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:56:57,039 [canton-env-ec-36] INFO  c.d.c.r.CommunityDbMigrations:mydomain - There are 9 pending migrations for the db that is at version 0. Performing migration before start.
2025-08-24 12:56:57,041 [canton-env-ec-41] INFO  c.d.c.r.CommunityDbMigrations:participant1 - There are 9 pending migrations for the db that is at version 0. Performing migration before start.
2025-08-24 12:56:57,045 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:56:57,046 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:56:57,054 [canton-env-ec-41] INFO  o.f.c.internal.command.DbValidate - Successfully validated 9 migrations (execution time 00:00.005s)
2025-08-24 12:56:57,054 [canton-env-ec-36] INFO  o.f.c.internal.command.DbValidate - Successfully validated 9 migrations (execution time 00:00.006s)
2025-08-24 12:56:57,065 [canton-env-ec-41] INFO  o.f.c.i.s.JdbcTableSchemaHistory - Creating Schema History table "public"."flyway_schema_history" ...
2025-08-24 12:56:57,065 [canton-env-ec-36] INFO  o.f.c.i.s.JdbcTableSchemaHistory - Creating Schema History table "public"."flyway_schema_history" ...
2025-08-24 12:56:57,108 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "public": << Empty Schema >>
2025-08-24 12:56:57,108 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "public": << Empty Schema >>
2025-08-24 12:56:57,177 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "1.1 - initial"
2025-08-24 12:56:57,177 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "1.1 - initial"
2025-08-24 12:56:57,697 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "2 - changes for 2.3"
2025-08-24 12:56:57,697 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "2 - changes for 2.3"
2025-08-24 12:56:57,726 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "3 - changes for 2.4"
2025-08-24 12:56:57,727 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "3 - changes for 2.4"
2025-08-24 12:56:57,757 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "4 - changes for 2.5"
2025-08-24 12:56:57,759 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "4 - changes for 2.5"
2025-08-24 12:56:57,783 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "5 - changes for 2.6"
2025-08-24 12:56:57,786 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "5 - changes for 2.6"
2025-08-24 12:56:57,800 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "6 - changes for 2.7"
2025-08-24 12:56:57,800 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "6 - changes for 2.7"
2025-08-24 12:56:57,841 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "7 - changes for 2.8"
2025-08-24 12:56:57,841 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "7 - changes for 2.8"
2025-08-24 12:56:57,865 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "8 - changes for 2.9"
2025-08-24 12:56:57,866 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "8 - changes for 2.9"
2025-08-24 12:56:57,877 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "9 - changes for 2.10"
2025-08-24 12:56:57,879 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "public" to version "9 - changes for 2.10"
2025-08-24 12:56:57,884 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Successfully applied 9 migrations to schema "public", now at version v9 (execution time 00:00.782s)
2025-08-24 12:56:57,886 [canton-env-ec-41] INFO  c.d.c.r.CommunityDbMigrations:participant1 - Applied 9 migrations successfully
2025-08-24 12:56:57,889 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown initiated...
2025-08-24 12:56:57,891 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Successfully applied 9 migrations to schema "public", now at version v9 (execution time 00:00.790s)
2025-08-24 12:56:57,893 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown completed.
2025-08-24 12:56:57,893 [canton-env-ec-36] INFO  c.d.c.r.CommunityDbMigrations:mydomain - Applied 9 migrations successfully
2025-08-24 12:56:57,896 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown initiated...
2025-08-24 12:56:57,903 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown completed.
2025-08-24 12:56:57,935 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 12:56:57,939 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 12:56:57,974 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 12:56:58,091 [canton-env-ec-41] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 12:56:58,094 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 12:56:58,117 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 12:56:58,175 [canton-env-ec-37] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 12:56:58,175 [canton-env-ec-61] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 12:56:58,181 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 12:56:58,181 [canton-env-ec-41] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 12:56:58,827 [canton-env-ec-41] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 12:56:58,827 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 12:56:58,906 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Node is not initialized yet. Performing automated default initialization.
2025-08-24 12:56:58,906 [canton-env-ec-61] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Node is not initialized yet. Performing automated default initialization.
2025-08-24 12:56:59,367 [canton-env-ec-36] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:81e58366b2513b1930f4da19575a621c - Applied topology transaction Add NamespaceDelegation(1220b845dcf0..., SigningPublicKey(id = 1220b845dcf0..., format = Tink, scheme = Ed25519), true) at 2025-08-24T12:56:59.364611Z
2025-08-24 12:56:59,368 [canton-env-ec-60] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:092006f9de2aaf9f411348e4b15b8ec1 - Applied topology transaction Add NamespaceDelegation(122060f2dea2..., SigningPublicKey(id = 122060f2dea2..., format = Tink, scheme = Ed25519), true) at 2025-08-24T12:56:59.364722Z
2025-08-24 12:56:59,496 [canton-env-ec-70] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:092006f9de2aaf9f411348e4b15b8ec1 - Applied topology transaction Replace DomainParametersChange(
  mydomain::122060f2dea2...,
  DynamicDomainParameters(
    participant response timeout = 30s,
    mediator reaction timeout = 30s,
    transfer exclusivity timeout = 1m,
    topology change delay = 0.25s,
    ledger time record time tolerance = 1m,
    mediator deduplication timeout = 2m,
    reconciliation interval = 1m,
    max rate per participant = 1000000,
    max request size = 10485760,
    catchup config = CatchUpConfig(catchUpIntervalSkip = 5, nrIntervalsToTriggerCatchUp = 2)
  )
) at 2025-08-24T12:56:59.493376Z
2025-08-24 12:56:59,532 [canton-env-ec-69] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:092006f9de2aaf9f411348e4b15b8ec1 - Applied topology transaction Add OwnerToKeyMapping(DOM::mydomain::122060f2dea2..., SigningPublicKey(id = 1220289626ce..., format = Tink, scheme = Ed25519)) at 2025-08-24T12:56:59.528569Z
2025-08-24 12:56:59,584 [canton-env-ec-61] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain - Applied topology transaction Add OwnerToKeyMapping(MED::mydomain::122060f2dea2..., SigningPublicKey(id = 1220120ea179..., format = Tink, scheme = Ed25519)) at 2025-08-24T12:56:59.582710Z
2025-08-24 12:56:59,614 [canton-env-ec-91] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain - Applied topology transaction Add MediatorDomainState(Both, mydomain::122060f2dea2..., MED::mydomain::122060f2dea2...) at 2025-08-24T12:56:59.612337Z
2025-08-24 12:56:59,667 [canton-env-ec-61] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain - Applied topology transaction Add OwnerToKeyMapping(SEQ::mydomain::122060f2dea2..., SigningPublicKey(id = 122096c87f5e..., format = Tink, scheme = Ed25519)) at 2025-08-24T12:56:59.665359Z
2025-08-24 12:56:59,688 [canton-env-ec-60] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Initializing node with id NodeId(mydomain::122060f2dea2...)
2025-08-24 12:56:59,705 [canton-env-ec-69] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 12:56:59,713 [canton-env-ec-70] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (7 ms)
2025-08-24 12:56:59,810 [canton-env-ec-35] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:f5b1b250edfd4c3763617321622bd5d8 - Recovering published timely rejections
2025-08-24 12:56:59,829 [canton-env-ec-69] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:f5b1b250edfd4c3763617321622bd5d8 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 12:56:59,830 [canton-env-ec-69] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:f5b1b250edfd4c3763617321622bd5d8 - Fetch unpublished in log ParticipantEventLogId(index = 0), from None (exclusive) up to None (inclusive)
2025-08-24 12:57:00,008 [canton-env-ec-90] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 12:57:00,052 [canton-env-ec-36] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 12:57:00,057 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 12:57:00,094 [canton-env-ec-70] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 12:57:00,465 [canton-env-ec-60] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 12:57:00,523 [canton-env-ec-70] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. New kind of error: no success error (request infinite retries). Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:00,547 [canton-env-ec-35] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 12:57:00,597 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 12:57:00,600 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:57:00,705 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:57:00,738 [canton-env-ec-35] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.027s)
2025-08-24 12:57:00,748 [canton-env-ec-35] INFO  o.f.c.i.s.JdbcTableSchemaHistory - Creating Schema History table "ledger_api"."flyway_schema_history" ...
2025-08-24 12:57:00,798 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": << Empty Schema >>
2025-08-24 12:57:00,808 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "1 - Init"
2025-08-24 12:57:00,870 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "2.0 - Contract divulgence"
2025-08-24 12:57:00,889 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "2.1 - Rebuild Acs"
2025-08-24 12:57:00,903 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "3 - Recompute Key Hash"
2025-08-24 12:57:00,914 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "4.0 - Add parties"
2025-08-24 12:57:00,928 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "4.1 - Collect Parties"
2025-08-24 12:57:00,938 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "5 - Add packages"
2025-08-24 12:57:00,954 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "6 - External Ledger Offset"
2025-08-24 12:57:00,961 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "7 - Command deduplication"
2025-08-24 12:57:00,971 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "8 - Contract Divulgence"
2025-08-24 12:57:00,984 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "9 - Contract Divulgence"
2025-08-24 12:57:01,002 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "10.0 - Extract Event Data"
2025-08-24 12:57:01,025 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "10.1 - Populate Event Data"
2025-08-24 12:57:01,032 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "10.2 - Extract Event Data"
2025-08-24 12:57:01,041 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "11 - Disclosures index"
2025-08-24 12:57:01,053 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "12 - Add configuration"
2025-08-24 12:57:01,072 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "13 - Party entries"
2025-08-24 12:57:01,092 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "14 - Package entries"
2025-08-24 12:57:01,110 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "15 - Loosen transaction check"
2025-08-24 12:57:01,117 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "16 - Create command completions table"
2025-08-24 12:57:01,129 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "17 - Command deduplication"
2025-08-24 12:57:01,144 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "18 - Backfill completions"
2025-08-24 12:57:01,153 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "19 - Fix Completions"
2025-08-24 12:57:01,160 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "20 - Events new schema"
2025-08-24 12:57:01,205 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "21 - Stable offsets"
2025-08-24 12:57:01,343 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "22 - Remove maximum record time"
2025-08-24 12:57:01,354 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "23 - Delete checkpoints"
2025-08-24 12:57:01,365 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "24 - Stable offsets archival"
2025-08-24 12:57:01,390 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "25 - Backfill Participant Events"
2025-08-24 12:57:01,400 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "26.0 - Contracts new schema"
2025-08-24 12:57:01,434 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "26.1 - Fill create argument"
2025-08-24 12:57:01,443 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "26.2 - Contract create arg not null"
2025-08-24 12:57:01,452 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "27 - Events table fixes"
2025-08-24 12:57:01,465 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "28 - Fix key hashes"
2025-08-24 12:57:01,473 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "29 - Fix participant events"
2025-08-24 12:57:01,481 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "30 - Drop old schema"
2025-08-24 12:57:01,488 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: drop cascades to 5 other objects
2025-08-24 12:57:01,491 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: drop cascades to constraint contract_divulgences_contract_id_fkey1 on table contract_divulgences
2025-08-24 12:57:01,512 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "31 - Event witnesses single table"
2025-08-24 12:57:01,533 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "32.0 - Drop archived contracts"
2025-08-24 12:57:01,542 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "32.1 - Fix key hashes"
2025-08-24 12:57:01,552 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "33 - Add witnesses to participant events"
2025-08-24 12:57:01,566 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "34 - Parties is local"
2025-08-24 12:57:01,577 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "35 - event sequential id"
2025-08-24 12:57:01,608 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "36 - drop participant id"
2025-08-24 12:57:01,624 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "37 - add participant id to parameters"
2025-08-24 12:57:01,630 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "38 - Update value versions"
2025-08-24 12:57:01,635 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "39 - Participant-pruning"
2025-08-24 12:57:01,641 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "40 - multiple submitters"
2025-08-24 12:57:01,675 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "41 - hash indices"
2025-08-24 12:57:01,688 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "42 - Convert hash indices"
2025-08-24 12:57:01,702 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "43 - explicit compression"
2025-08-24 12:57:01,711 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "44 - offset as text"
2025-08-24 12:57:01,777 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "45 - fix large size index issues"
2025-08-24 12:57:01,791 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "100.0 - Append only schema"
2025-08-24 12:57:01,816 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "100.1 - Append only cleanup"
2025-08-24 12:57:01,818 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: drop cascades to constraint participant_contract_witnesses_contract_id_fkey on table participant_contract_witnesses
2025-08-24 12:57:01,840 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "100.2 - Append only indices"
2025-08-24 12:57:01,908 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "100.3 - Append only vacuum" [non-transactional]
2025-08-24 12:57:02,047 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_authid", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,048 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_subscription", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,048 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_database", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,048 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_db_role_setting", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,049 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_tablespace", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,049 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_auth_members", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,049 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shdepend", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,050 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shdescription", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,050 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_replication_origin", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,050 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shseclabel", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,051 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_parameter_acl", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:02,059 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "101 - drop configuration from parameters table"
2025-08-24 12:57:02,067 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "102 - add initialization indices"
2025-08-24 12:57:02,079 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "103 - remove duplicate index"
2025-08-24 12:57:02,088 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "104 - rename packages size column"
2025-08-24 12:57:02,098 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "105 - drop unique index constraints"
2025-08-24 12:57:02,117 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "106 - add rejection status proto column"
2025-08-24 12:57:02,130 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "107 - parameters table cleanup"
2025-08-24 12:57:02,141 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "108 - drop parties"
2025-08-24 12:57:02,157 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "109 - Add all divulgence pruning offset"
2025-08-24 12:57:02,165 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "110 - add deduplication info to completions"
2025-08-24 12:57:02,176 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "111 - timestamp to bigint"
2025-08-24 12:57:02,280 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "112 - add string interning"
2025-08-24 12:57:02,295 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "113 - enable string interning"
2025-08-24 12:57:02,305 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,307 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,308 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,309 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,311 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,312 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,312 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,313 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,314 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,314 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,315 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,316 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,317 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,317 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,318 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,319 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,320 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,320 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,321 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,322 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,323 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------------+
| insert_to_string_interning |
+----------------------------+
|                            |
+----------------------------+

2025-08-24 12:57:02,352 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "114 - activate string interning for parties and templates"
2025-08-24 12:57:02,354 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------+
| ?column?                                  |
+-------------------------------------------+
| Interning data migration - Preparation... |
+-------------------------------------------+

2025-08-24 12:57:02,359 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------------------+
| ?column?                                              |
+-------------------------------------------------------+
| Interning data migration - Migrating party_entries... |
+-------------------------------------------------------+

2025-08-24 12:57:02,364 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------------------------------------+
| ?column?                                                                |
+-------------------------------------------------------------------------+
| Interning data migration - Migrating participant_command_completions... |
+-------------------------------------------------------------------------+

2025-08-24 12:57:02,368 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 12:57:02,370 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 12:57:02,375 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 12:57:02,377 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------------------------------------------------+
| ?column?                                                              |
+-----------------------------------------------------------------------+
| Interning data migration - Migrating participant_events_divulgence... |
+-----------------------------------------------------------------------+

2025-08-24 12:57:02,381 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 12:57:02,383 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 12:57:02,392 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 12:57:02,395 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------------------------------+
| ?column?                                                          |
+-------------------------------------------------------------------+
| Interning data migration - Migrating participant_events_create... |
+-------------------------------------------------------------------+

2025-08-24 12:57:02,399 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 12:57:02,401 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 12:57:02,415 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 12:57:02,418 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------------------------------------------------------------+
| ?column?                                                                      |
+-------------------------------------------------------------------------------+
| Interning data migration - Migrating participant_events_consuming_exercise... |
+-------------------------------------------------------------------------------+

2025-08-24 12:57:02,422 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 12:57:02,424 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 12:57:02,437 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 12:57:02,441 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------------------------------------------------------------+
| ?column?                                                                          |
+-----------------------------------------------------------------------------------+
| Interning data migration - Migrating participant_events_non_consuming_exercise... |
+-----------------------------------------------------------------------------------+

2025-08-24 12:57:02,446 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------+
| ?column?                    |
+-----------------------------+
| DEBUG populate new table... |
+-----------------------------+

2025-08-24 12:57:02,448 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +----------------------+
| ?column?             |
+----------------------+
| DEBUG add indexes... |
+----------------------+

2025-08-24 12:57:02,449 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: identifier "participant_events_non_consuming_exercise_event_sequential_id_old" will be truncated to "participant_events_non_consuming_exercise_event_sequential_id_o" (SQL State: 42622 - Error Code: 0)
2025-08-24 12:57:02,451 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - DB: identifier "participant_events_non_consuming_exercise_transaction_id_idx_old" will be truncated to "participant_events_non_consuming_exercise_transaction_id_idx_ol" (SQL State: 42622 - Error Code: 0)
2025-08-24 12:57:02,461 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| DEBUG replace original table... |
+---------------------------------+

2025-08-24 12:57:02,463 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------------+
| ?column?                              |
+---------------------------------------+
| Interning data migration - Cleanup... |
+---------------------------------------+

2025-08-24 12:57:02,472 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| Interning data migration - Done |
+---------------------------------+

2025-08-24 12:57:02,489 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "116 - add filter table"
2025-08-24 12:57:02,492 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +--------------------------------------------------------------------------+
| ?column?                                                                 |
+--------------------------------------------------------------------------+
| Add Filter Table: Migrating to participant_events_create_filter table... |
+--------------------------------------------------------------------------+

2025-08-24 12:57:02,493 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------------+
| ?column?                              |
+---------------------------------------+
| Add Filter Table: Creating indexes... |
+---------------------------------------+

2025-08-24 12:57:02,503 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-------------------------+
| ?column?                |
+-------------------------+
| Add Filter Table: Done. |
+-------------------------+

2025-08-24 12:57:02,512 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "117 - vacuum analyze" [non-transactional]
2025-08-24 12:57:05,528 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:05,531 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:07,694 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_authid", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,695 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_subscription", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,695 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_database", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,695 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_db_role_setting", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,696 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_tablespace", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,696 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_auth_members", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,696 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shdepend", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,697 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shdescription", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,697 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_replication_origin", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,697 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_shseclabel", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,697 [canton-env-ec-35] WARN  o.f.c.i.s.DefaultSqlScriptExecutor - DB: permission denied to vacuum "pg_parameter_acl", skipping it (SQL State: 01000 - Error Code: 0)
2025-08-24 12:57:07,705 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "117.1 - drop event id indexes"
2025-08-24 12:57:07,718 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "118 - add user managment"
2025-08-24 12:57:07,748 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "119 - add transaction metering"
2025-08-24 12:57:07,749 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------------------+
| ?column?                                    |
+---------------------------------------------+
| Add Transaction Metering: Creating table... |
+---------------------------------------------+

2025-08-24 12:57:07,754 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------------------------+
| ?column?                                      |
+-----------------------------------------------+
| Add Transaction Metering: Creating indexes... |
+-----------------------------------------------+

2025-08-24 12:57:07,760 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| Add Transaction Metering: Done. |
+---------------------------------+

2025-08-24 12:57:07,768 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "120 - non optional ledger end"
2025-08-24 12:57:07,780 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "121 - drop participant side command deduplication"
2025-08-24 12:57:07,791 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "122 - participant metering"
2025-08-24 12:57:07,792 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +--------------------------------------------------------------------------+
| ?column?                                                                 |
+--------------------------------------------------------------------------+
| Harmonise the transaction_metering columns with other examples in the db |
+--------------------------------------------------------------------------+

2025-08-24 12:57:07,794 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +--------------------------------------------+
| ?column?                                   |
+--------------------------------------------+
| Add Metering Parameters: Creating table... |
+--------------------------------------------+

2025-08-24 12:57:07,799 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------------------+
| ?column?                                    |
+---------------------------------------------+
| Add Participant Metering: Creating table... |
+---------------------------------------------+

2025-08-24 12:57:07,804 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +-----------------------------------------------+
| ?column?                                      |
+-----------------------------------------------+
| Add Participant Metering: Creating indexes... |
+-----------------------------------------------+

2025-08-24 12:57:07,809 [canton-env-ec-35] INFO  o.f.c.i.s.DefaultSqlScriptExecutor - +---------------------------------+
| ?column?                        |
+---------------------------------+
| Add Participant Metering: Done. |
+---------------------------------+

2025-08-24 12:57:07,818 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "123 - remove events view"
2025-08-24 12:57:07,830 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "124 - modifiable users and parties"
2025-08-24 12:57:07,864 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "125 - creates driver metadata"
2025-08-24 12:57:07,874 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "126 - identity provider config"
2025-08-24 12:57:07,892 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "127 - etq completions"
2025-08-24 12:57:07,913 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "128 - etq tables and indexes"
2025-08-24 12:57:07,961 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "129 - identity provider id users parties"
2025-08-24 12:57:07,975 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "130 - etq data migration"
2025-08-24 12:57:07,992 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "131 - etq drop tx id indexes"
2025-08-24 12:57:08,006 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "132 - audience idp config"
2025-08-24 12:57:08,015 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "133 - add domain id"
2025-08-24 12:57:08,027 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "134 - add reassignment events"
2025-08-24 12:57:08,075 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "135 - add indexes for incomplete reassignment queries"
2025-08-24 12:57:08,093 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "136 - add key maintainers"
2025-08-24 12:57:08,100 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "137 - add trace context"
2025-08-24 12:57:08,136 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "138 - drop ledger id"
2025-08-24 12:57:08,144 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "139 - nullable transfer submitter"
2025-08-24 12:57:08,152 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "140 - add package name"
2025-08-24 12:57:08,159 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "141 - add package name"
2025-08-24 12:57:08,168 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "142 - change party collation to binary"
2025-08-24 12:57:08,181 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Migrating schema "ledger_api" to version "143 - add choice package id"
2025-08-24 12:57:08,190 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Successfully applied 99 migrations to schema "ledger_api", now at version v143 (execution time 00:07.396s)
2025-08-24 12:57:08,194 [canton-env-ec-35] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 99 steps.
2025-08-24 12:57:08,242 [canton-env-ec-91] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 12:57:08,252 [canton-env-ec-35] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 12:57:08,256 [canton-env-ec-35] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 12:57:08,267 [canton-env-ec-91] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 12:57:08,288 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 12:57:08,289 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 12:57:08,292 [canton-env-ec-91] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 12:57:08,297 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 12:57:08,297 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 12:57:08,300 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 12:57:08,306 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 12:57:08,311 [canton-env-ec-90] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 12:57:08,344 [canton-env-ec-90] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 12:57:08,363 [canton-env-ec-90] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 12:57:08,366 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Initializing new database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 12:57:08,406 [canton-env-ec-70] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 12:57:08,407 [canton-env-ec-70] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 12:57:08,410 [canton-env-ec-61] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes()),0,0)
2025-08-24 12:57:08,435 [canton-env-ec-90] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (18 ms)
2025-08-24 12:57:08,483 [canton-env-ec-61] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (47 ms)
2025-08-24 12:57:08,487 [canton-env-ec-35] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes()).
2025-08-24 12:57:08,570 [canton-env-ec-35] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 12:57:08,570 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 12:57:08,575 [canton-env-ec-61] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 12:57:08,622 [input-mapping-pool-0] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:f5b1b250edfd4c3763617321622bd5d8 - Storing at offset=000000000000000001 ConfigurationChanged(
  recordTime = 2025-08-24T12:56:59.863476Z,
  configuration = Configuration(generation = 1, maxDeduplicationDuration = 168h, timeModel = LedgerTimeModel(avgTransactionLatency = 0s, minSkew = 8760h, maxSkew = 8760h)),
  ...
)
2025-08-24 12:57:08,626 [canton-env-ec-60] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 12:57:08,636 [canton-env-ec-61] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup did not find any configuration. Looking for new ledger configurations from the ledger beginning.
2025-08-24 12:57:08,693 [canton-env-ec-90] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - New ledger configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) found at Absolute(000000000000000001)
2025-08-24 12:57:08,835 [canton-env-ec-90] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 12:57:08,836 [canton-env-ec-90] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 12:57:08,836 [canton-env-ec-90] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 12:57:08,968 [canton-env-ec-35] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 12:57:08,970 [canton-env-ec-69] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 12:57:09,013 [canton-env-ec-69] WARN  c.d.c.p.l.a.c.LedgerConnection$$anon$1:participant=participant1 - Creation of the ledger client
io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
	at io.grpc.Status.asRuntimeException(Status.java:532)
	at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:538)
	at io.opentelemetry.instrumentation.grpc.v1_6.TracingClientInterceptor$TracingClientCall$TracingClientCallListener.onClose(TracingClientInterceptor.java:161)
	at io.grpc.internal.DelayedClientCall$DelayedListener$3.run(DelayedClientCall.java:489)
	at io.grpc.internal.DelayedClientCall$DelayedListener.delayOrExecute(DelayedClientCall.java:453)
	at io.grpc.internal.DelayedClientCall$DelayedListener.onClose(DelayedClientCall.java:486)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.handshakeException(ReferenceCountedOpenSslEngine.java:1927)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.wrap(ReferenceCountedOpenSslEngine.java:848)
	at java.base/javax.net.ssl.SSLEngine.wrap(SSLEngine.java:564)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1131)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:975)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.access$1900(SslHandler.java:171)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1832)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1711)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1872)
	at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
	at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:405)
	at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.security.cert.CertificateException: No subject alternative names matching IP address 0.0.0.0 found
	at java.base/sun.security.util.HostnameChecker.matchIP(HostnameChecker.java:160)
	at java.base/sun.security.util.HostnameChecker.match(HostnameChecker.java:101)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:466)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:432)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:291)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144)
	at io.grpc.netty.shaded.io.netty.handler.ssl.EnhancingX509ExtendedTrustManager.checkServerTrusted(EnhancingX509ExtendedTrustManager.java:69)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:235)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:797)
	at io.grpc.netty.shaded.io.netty.internal.tcnative.CertificateVerifierTask.runTask(CertificateVerifierTask.java:36)
	at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:48)
	at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:42)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.runAndResetNeedTask(ReferenceCountedOpenSslEngine.java:1533)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.access$700(ReferenceCountedOpenSslEngine.java:94)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine$TaskDecorator.run(ReferenceCountedOpenSslEngine.java:1505)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.run(SslHandler.java:1889)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 common frames omitted
2025-08-24 12:57:10,532 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:10,535 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:15,536 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:15,539 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:20,540 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:20,543 [canton-env-ec-35] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:25,544 [canton-env-ec-35] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:25,546 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:30,547 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:30,549 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:35,550 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:35,553 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:40,554 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:40,557 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:45,557 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:45,560 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:50,561 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:50,563 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:57:55,564 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:57:55,567 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:00,567 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:00,571 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:05,571 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:05,574 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:10,574 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:10,577 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:15,578 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:15,580 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:20,580 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:20,583 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:25,584 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:25,587 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:30,588 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:30,590 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:35,591 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:35,594 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:40,595 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:40,598 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:45,599 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:45,602 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:50,603 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:50,605 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:58:51,388 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 12:58:52,377 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 12:58:52,397 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 12:58:52,604 [canton-env-ec-34] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 12:58:52,712 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 12:58:52,774 [main] INFO  c.d.c.e.CommunityEnvironment tid:27c40aedeba63aa5507f1f62b3bec3f0 - Automatically starting all instances
2025-08-24 12:58:52,826 [canton-env-ec-34] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 12:58:52,826 [canton-env-ec-35] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 12:58:52,848 [canton-env-ec-34] INFO  c.d.c.r.DbStorage:participant1 tid:3c53b1293ee55649fe8321e7e2a74a66 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:58:52,849 [canton-env-ec-35] INFO  c.d.c.resource.DbStorage:mydomain tid:c7a1023d7da8e37ab5a5b6ad16e0d714 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:58:52,873 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 12:58:52,874 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 12:58:53,399 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 12:58:53,399 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 12:58:53,608 [canton-env-ec-35] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 12:58:53,608 [canton-env-ec-34] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 12:58:53,617 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 12:58:53,618 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 12:58:53,618 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 12:58:53,617 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 12:58:53,618 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 12:58:53,619 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 12:58:53,702 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 12:58:53,702 [canton-env-ec-34] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 12:58:53,712 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:58:53,713 [canton-env-ec-34] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:58:53,773 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 12:58:53,774 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 12:58:53,778 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 12:58:53,780 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 12:58:53,830 [canton-env-ec-35] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 12:58:53,834 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 12:58:53,871 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 12:58:53,946 [canton-env-ec-34] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 12:58:53,948 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 12:58:53,970 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 12:58:54,039 [canton-env-ec-64] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 12:58:54,039 [canton-env-ec-57] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 12:58:54,047 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 12:58:54,047 [canton-env-ec-34] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 12:58:54,471 [main] ERROR c.d.c.e.CommunityEnvironment tid:27c40aedeba63aa5507f1f62b3bec3f0 - Failed to start mydomain: Failed to bind to address /0.0.0.0:5019
2025-08-24 12:58:54,477 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 12:58:54,482 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 12:58:54,530 [canton-env-ec-40] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 12:58:54,569 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 12:58:54,569 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 12:58:55,606 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:58:55,609 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:00,610 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:00,612 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:05,613 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:05,616 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:08,222 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 12:59:09,233 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 12:59:09,255 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 12:59:09,454 [canton-env-ec-34] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 12:59:09,533 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 12:59:09,594 [main] INFO  c.d.c.e.CommunityEnvironment tid:e267ee0e0c25b6c023a6c1576546c0d6 - Automatically starting all instances
2025-08-24 12:59:09,648 [canton-env-ec-35] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 12:59:09,648 [canton-env-ec-34] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 12:59:09,689 [canton-env-ec-34] INFO  c.d.c.resource.DbStorage:mydomain tid:9cd96692a1a83e09d2daf1b217d931ef - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:59:09,690 [canton-env-ec-35] INFO  c.d.c.r.DbStorage:participant1 tid:4bf5b3ee17292d772c9ebd4dd16351dd - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 12:59:09,741 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 12:59:09,741 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 12:59:10,285 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Start completed.
2025-08-24 12:59:10,285 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Start completed.
2025-08-24 12:59:10,482 [canton-env-ec-34] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 12:59:10,482 [canton-env-ec-35] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 12:59:10,491 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 12:59:10,491 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 12:59:10,492 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 12:59:10,491 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 12:59:10,492 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 12:59:10,492 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 12:59:10,617 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:10,619 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:10,665 [canton-env-ec-34] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 12:59:10,665 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 12:59:10,673 [canton-env-ec-34] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:59:10,674 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 12:59:10,737 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown initiated...
2025-08-24 12:59:10,737 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown initiated...
2025-08-24 12:59:10,742 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown completed.
2025-08-24 12:59:10,759 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown completed.
2025-08-24 12:59:10,820 [canton-env-ec-34] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 12:59:10,824 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 12:59:10,862 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 12:59:10,954 [canton-env-ec-35] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 12:59:10,956 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 12:59:10,988 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 12:59:11,053 [canton-env-ec-64] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 12:59:11,053 [canton-env-ec-65] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 12:59:11,062 [canton-env-ec-34] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 12:59:11,062 [canton-env-ec-35] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 12:59:11,497 [main] ERROR c.d.c.e.CommunityEnvironment tid:e267ee0e0c25b6c023a6c1576546c0d6 - Failed to start mydomain: Failed to bind to address /0.0.0.0:5019
2025-08-24 12:59:11,500 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 12:59:11,505 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 12:59:11,556 [canton-env-ec-66] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 12:59:11,587 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 12:59:11,587 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 12:59:15,620 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:15,623 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:20,623 [canton-env-ec-35] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:20,626 [canton-env-ec-35] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:25,627 [canton-env-ec-36] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:25,629 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:30,630 [canton-env-ec-90] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:30,634 [canton-env-ec-70] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:35,204 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 12:59:35,207 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 12:59:35,634 [canton-env-ec-35] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:35,637 [canton-env-ec-35] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:40,211 [Thread-0] INFO  c.d.canton.environment.DomainNodes - Task closing node-mydomain still not completed after 5 seconds. Continue waiting...
2025-08-24 12:59:40,638 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - Now retrying operation 'fetch handshake'. 
2025-08-24 12:59:40,641 [canton-env-ec-60] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:acbe0ea13e61c751a4366fc48940c55b - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 12:59:42,712 [Thread-0] INFO  c.d.canton.environment.DomainNodes - Task closing node-mydomain still not completed after 7500 milliseconds. Continue waiting...
2025-08-24 12:59:45,212 [Thread-0] INFO  c.d.canton.environment.DomainNodes - Task closing node-mydomain still not completed after 10 seconds. Continue waiting...
2025-08-24 12:59:45,221 [Thread-0] WARN  c.d.canton.environment.DomainNodes - Task closing node-mydomain did not complete within 10 seconds. Stack traces:
  Thread[#65,participant1-wallclock-0,5,main] is-daemon=false state=TIMED_WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
    java.base@21.0.8/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    java.base@21.0.8/java.lang.Thread.runWith(Thread.java:1596)
    java.base@21.0.8/java.lang.Thread.run(Thread.java:1583)

  Thread[#102,delay-util-0,5,main] is-daemon=true state=TIMED_WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
    java.base@21.0.8/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    java.base@21.0.8/java.lang.Thread.runWith(Thread.java:1596)
    java.base@21.0.8/java.lang.Thread.run(Thread.java:1583)

  Thread[#57,mydomain-wallclock-0,5,main] is-daemon=false state=TIMED_WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
    java.base@21.0.8/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    java.base@21.0.8/java.lang.Thread.runWith(Thread.java:1596)
    java.base@21.0.8/java.lang.Thread.run(Thread.java:1583)

2025-08-24 12:59:45,222 [Thread-0] WARN  c.d.canton.environment.DomainNodes - Closing 'AsyncCloseable(name=node-mydomain)' failed! Reason:
java.util.concurrent.TimeoutException: Task closing node-mydomain did not complete within 10 seconds.
	at com.digitalasset.canton.util.FutureUtil$.retry$1(FutureUtil.scala:300)
	at com.digitalasset.canton.util.FutureUtil$.noisyAwaitResultForTesting(FutureUtil.scala:304)
	at com.digitalasset.canton.util.FutureUtil$.noisyAwaitResult(FutureUtil.scala:205)
	at com.digitalasset.canton.lifecycle.AsyncCloseable.close(FlagCloseableAsync.scala:36)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$1(Lifecycle.scala:47)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at com.digitalasset.canton.lifecycle.Lifecycle$.stopSingle$1(Lifecycle.scala:47)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$7(Lifecycle.scala:62)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:61)
	at com.digitalasset.canton.lifecycle.FlagCloseableAsync.onClosed(FlagCloseableAsync.scala:22)
	at com.digitalasset.canton.lifecycle.FlagCloseableAsync.onClosed$(FlagCloseableAsync.scala:22)
	at com.digitalasset.canton.environment.ManagedNodes.onClosed(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.onFirstClose(PerformUnlessClosing.scala:227)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.onFirstClose$(PerformUnlessClosing.scala:192)
	at com.digitalasset.canton.environment.ManagedNodes.onFirstClose(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.OnShutdownRunner.close(OnShutdownRunner.scala:93)
	at com.digitalasset.canton.lifecycle.OnShutdownRunner.close$(OnShutdownRunner.scala:83)
	at com.digitalasset.canton.environment.ManagedNodes.com$digitalasset$canton$lifecycle$FlagCloseable$$super$close(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.FlagCloseable.close(FlagCloseable.scala:27)
	at com.digitalasset.canton.lifecycle.FlagCloseable.close$(FlagCloseable.scala:27)
	at com.digitalasset.canton.environment.ManagedNodes.close(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$1(Lifecycle.scala:47)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at com.digitalasset.canton.lifecycle.Lifecycle$.stopSingle$1(Lifecycle.scala:47)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$7(Lifecycle.scala:62)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:61)
	at com.digitalasset.canton.environment.Environment.$anonfun$close$1(Environment.scala:559)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
	at scala.concurrent.package$.blocking(package.scala:124)
	at com.digitalasset.canton.environment.Environment.close(Environment.scala:540)
	at com.digitalasset.canton.environment.Environment.close$(Environment.scala:540)
	at com.digitalasset.canton.environment.CommunityEnvironment.close(CommunityEnvironment.scala:33)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3(CantonAppDriver.scala:109)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3$adapted(CantonAppDriver.scala:109)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$2(CantonAppDriver.scala:109)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-08-24 12:59:45,225 [Thread-0] WARN  c.d.c.e.CommunityEnvironment - Closing 'com.digitalasset.canton.environment.DomainNodes@2078af17' failed! Reason:
com.digitalasset.canton.lifecycle.ShutdownFailedException: Unable to close 'AsyncCloseable(name=node-mydomain)'.
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$8(Lifecycle.scala:65)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:65)
	at com.digitalasset.canton.lifecycle.FlagCloseableAsync.onClosed(FlagCloseableAsync.scala:22)
	at com.digitalasset.canton.lifecycle.FlagCloseableAsync.onClosed$(FlagCloseableAsync.scala:22)
	at com.digitalasset.canton.environment.ManagedNodes.onClosed(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.onFirstClose(PerformUnlessClosing.scala:227)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.onFirstClose$(PerformUnlessClosing.scala:192)
	at com.digitalasset.canton.environment.ManagedNodes.onFirstClose(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.OnShutdownRunner.close(OnShutdownRunner.scala:93)
	at com.digitalasset.canton.lifecycle.OnShutdownRunner.close$(OnShutdownRunner.scala:83)
	at com.digitalasset.canton.environment.ManagedNodes.com$digitalasset$canton$lifecycle$FlagCloseable$$super$close(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.FlagCloseable.close(FlagCloseable.scala:27)
	at com.digitalasset.canton.lifecycle.FlagCloseable.close$(FlagCloseable.scala:27)
	at com.digitalasset.canton.environment.ManagedNodes.close(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$1(Lifecycle.scala:47)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at com.digitalasset.canton.lifecycle.Lifecycle$.stopSingle$1(Lifecycle.scala:47)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$7(Lifecycle.scala:62)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:61)
	at com.digitalasset.canton.environment.Environment.$anonfun$close$1(Environment.scala:559)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
	at scala.concurrent.package$.blocking(package.scala:124)
	at com.digitalasset.canton.environment.Environment.close(Environment.scala:540)
	at com.digitalasset.canton.environment.Environment.close$(Environment.scala:540)
	at com.digitalasset.canton.environment.CommunityEnvironment.close(CommunityEnvironment.scala:33)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3(CantonAppDriver.scala:109)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3$adapted(CantonAppDriver.scala:109)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$2(CantonAppDriver.scala:109)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-08-24 12:59:45,261 [canton-env-ec-69] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 12:59:45,287 [canton-env-ec-70] WARN  c.d.c.d.s.s.SequencerWriter:domain=mydomain tid:5c2d6a18e0f89e7052f80e1d95d39443 - Sequencer writer has completed with an unrecoverable error
org.apache.pekko.stream.AbruptTerminationException: Processor actor [Actor[pekko://canton-actor-system/system/Materializers/StreamSupervisor-0/flow-2-0-ignoreSink#2083801476]] terminated abruptly
2025-08-24 12:59:45,291 [canton-env-ec-60] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 12:59:45,308 [canton-env-ec-60] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 12:59:45,309 [canton-env-ec-60] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished with failure: Processor actor [Actor[pekko://canton-actor-system/system/Materializers/StreamSupervisor-0/flow-9-0-ignoreSink#-595242778]] terminated abruptly
2025-08-24 12:59:45,310 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 12:59:45,312 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 12:59:45,313 [canton-env-ec-60] ERROR c.d.c.p.i.RecoveringIndexer:participant=participant1 - Error while running indexer, restart scheduled after 10 seconds
org.apache.pekko.stream.AbruptTerminationException: Processor actor [Actor[pekko://canton-actor-system/system/Materializers/StreamSupervisor-0/flow-9-0-ignoreSink#-595242778]] terminated abruptly
2025-08-24 12:59:45,315 [canton-env-ec-60] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 12:59:45,322 [Thread-0] ERROR c.d.canton.CantonCommunityApp$ - Failed to shut down successfully.
com.digitalasset.canton.lifecycle.ShutdownFailedException: Unable to close 'com.digitalasset.canton.environment.DomainNodes@2078af17'.
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$8(Lifecycle.scala:65)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:65)
	at com.digitalasset.canton.environment.Environment.$anonfun$close$1(Environment.scala:559)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
	at scala.concurrent.package$.blocking(package.scala:124)
	at com.digitalasset.canton.environment.Environment.close(Environment.scala:540)
	at com.digitalasset.canton.environment.Environment.close$(Environment.scala:540)
	at com.digitalasset.canton.environment.CommunityEnvironment.close(CommunityEnvironment.scala:33)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3(CantonAppDriver.scala:109)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3$adapted(CantonAppDriver.scala:109)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$2(CantonAppDriver.scala:109)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-08-24 12:59:45,322 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 13:03:59,118 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 13:04:00,191 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 13:04:00,211 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 13:04:00,422 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 13:04:00,495 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 13:04:00,551 [main] INFO  c.d.c.e.CommunityEnvironment tid:7a857d5fc09ebce787b863eb41578924 - Automatically starting all instances
2025-08-24 13:04:00,607 [canton-env-ec-37] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 13:04:00,607 [canton-env-ec-36] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 13:04:00,643 [canton-env-ec-37] INFO  c.d.c.resource.DbStorage:mydomain tid:c3c838f78bcbbcf3d87e9a64536291b2 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:04:00,643 [canton-env-ec-36] INFO  c.d.c.r.DbStorage:participant1 tid:e9f903ba425e3353388acd6f3f466099 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:04:00,669 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 13:04:00,669 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 13:04:01,154 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Start completed.
2025-08-24 13:04:01,154 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Start completed.
2025-08-24 13:04:01,332 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:04:01,332 [canton-env-ec-37] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:04:01,342 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:04:01,342 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:04:01,342 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:04:01,342 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:04:01,343 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:04:01,343 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:04:01,416 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:04:01,416 [canton-env-ec-37] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 13:04:01,424 [canton-env-ec-37] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:04:01,424 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:04:01,479 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown initiated...
2025-08-24 13:04:01,479 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown initiated...
2025-08-24 13:04:01,485 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown completed.
2025-08-24 13:04:01,485 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown completed.
2025-08-24 13:04:01,530 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 13:04:01,534 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 13:04:01,558 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 13:04:01,646 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 13:04:01,648 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 13:04:01,673 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 13:04:01,726 [canton-env-ec-38] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:04:01,726 [canton-env-ec-65] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:04:01,733 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:04:01,733 [canton-env-ec-36] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:04:02,400 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:04:02,407 [canton-env-ec-36] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:04:02,503 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Node is not initialized yet. Performing automated default initialization.
2025-08-24 13:04:02,533 [canton-env-ec-69] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 13:04:03,073 [canton-env-ec-64] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 13:04:03,083 [canton-env-ec-36] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (9 ms)
2025-08-24 13:04:03,169 [canton-env-ec-35] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 13:04:03,172 [canton-env-ec-39] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:49a3eb24e68e278ad0b8d5040ca5a2b9 - Recovering published timely rejections
2025-08-24 13:04:03,191 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:49a3eb24e68e278ad0b8d5040ca5a2b9 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 13:04:03,194 [canton-env-ec-36] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:49a3eb24e68e278ad0b8d5040ca5a2b9 - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 0)) (exclusive) up to None (inclusive)
2025-08-24 13:04:03,206 [canton-env-ec-69] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 13:04:03,207 [canton-env-ec-69] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:04:03,411 [canton-env-ec-65] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 13:04:03,700 [canton-env-ec-39] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - The operation 'fetch handshake' was not successful. New kind of error: no success error (request infinite retries). Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 13:04:03,762 [canton-env-ec-39] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 13:04:03,851 [canton-env-ec-35] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 13:04:03,904 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:04:03,907 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:04:04,014 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:04:04,055 [canton-env-ec-35] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.036s)
2025-08-24 13:04:04,088 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 13:04:04,088 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 13:04:04,091 [canton-env-ec-35] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 13:04:04,137 [canton-env-ec-35] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 13:04:04,144 [canton-env-ec-39] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 13:04:04,148 [canton-env-ec-39] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 13:04:04,159 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 13:04:04,178 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:04:04,179 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 13:04:04,190 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 13:04:04,190 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 13:04:04,194 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 13:04:04,195 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 13:04:04,202 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:04:04,207 [canton-env-ec-64] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 13:04:04,249 [canton-env-ec-64] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 13:04:04,294 [canton-env-ec-36] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 13:04:04,301 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 13:04:04,319 [canton-env-ec-36] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 13:04:04,322 [canton-env-ec-36] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 13:04:04,336 [canton-env-ec-38] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000001)),0,0)
2025-08-24 13:04:04,355 [canton-env-ec-39] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (14 ms)
2025-08-24 13:04:04,382 [canton-env-ec-38] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (26 ms)
2025-08-24 13:04:04,384 [canton-env-ec-36] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000001)).
2025-08-24 13:04:04,451 [canton-env-ec-36] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 13:04:04,451 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 13:04:04,506 [canton-env-ec-37] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 13:04:04,539 [input-mapping-pool-0] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:49a3eb24e68e278ad0b8d5040ca5a2b9 - Storing at offset=000000000000000002 ConfigurationChanged(
  recordTime = 2025-08-24T13:04:03.221798Z,
  configuration = Configuration(generation = 1, maxDeduplicationDuration = 168h, timeModel = LedgerTimeModel(avgTransactionLatency = 0s, minSkew = 8760h, maxSkew = 8760h)),
  ...
)
2025-08-24 13:04:04,574 [canton-env-ec-69] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 13:04:04,593 [canton-env-ec-65] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000001). Looking for new ledger configurations from this offset.
2025-08-24 13:04:04,628 [canton-env-ec-36] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - New ledger configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) found at Absolute(000000000000000002)
2025-08-24 13:04:04,725 [canton-env-ec-69] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 13:04:04,726 [canton-env-ec-69] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 13:04:04,726 [canton-env-ec-69] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 13:04:04,842 [canton-env-ec-69] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 13:04:04,843 [canton-env-ec-69] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 13:04:04,891 [canton-env-ec-39] WARN  c.d.c.p.l.a.c.LedgerConnection$$anon$1:participant=participant1 - Creation of the ledger client
io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
	at io.grpc.Status.asRuntimeException(Status.java:532)
	at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:538)
	at io.opentelemetry.instrumentation.grpc.v1_6.TracingClientInterceptor$TracingClientCall$TracingClientCallListener.onClose(TracingClientInterceptor.java:161)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Caused by: javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.handshakeException(ReferenceCountedOpenSslEngine.java:1927)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.wrap(ReferenceCountedOpenSslEngine.java:848)
	at java.base/javax.net.ssl.SSLEngine.wrap(SSLEngine.java:564)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1131)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:975)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.access$1900(SslHandler.java:171)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1832)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1711)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1872)
	at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
	at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:405)
	at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)
	at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.security.cert.CertificateException: No subject alternative names matching IP address 0.0.0.0 found
	at java.base/sun.security.util.HostnameChecker.matchIP(HostnameChecker.java:160)
	at java.base/sun.security.util.HostnameChecker.match(HostnameChecker.java:101)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:466)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:432)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:291)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144)
	at io.grpc.netty.shaded.io.netty.handler.ssl.EnhancingX509ExtendedTrustManager.checkServerTrusted(EnhancingX509ExtendedTrustManager.java:69)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:235)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:797)
	at io.grpc.netty.shaded.io.netty.internal.tcnative.CertificateVerifierTask.runTask(CertificateVerifierTask.java:36)
	at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:48)
	at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:42)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.runAndResetNeedTask(ReferenceCountedOpenSslEngine.java:1533)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.access$700(ReferenceCountedOpenSslEngine.java:94)
	at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine$TaskDecorator.run(ReferenceCountedOpenSslEngine.java:1505)
	at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.run(SslHandler.java:1889)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 common frames omitted
2025-08-24 13:04:08,706 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - Now retrying operation 'fetch handshake'. 
2025-08-24 13:04:08,708 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 13:04:13,709 [canton-env-ec-64] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - Now retrying operation 'fetch handshake'. 
2025-08-24 13:04:13,713 [canton-env-ec-38] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 13:04:18,714 [canton-env-ec-69] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - Now retrying operation 'fetch handshake'. 
2025-08-24 13:04:18,717 [canton-env-ec-38] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 13:04:23,719 [canton-env-ec-38] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - Now retrying operation 'fetch handshake'. 
2025-08-24 13:04:23,722 [canton-env-ec-38] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 13:04:28,723 [canton-env-ec-38] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - Now retrying operation 'fetch handshake'. 
2025-08-24 13:04:28,726 [canton-env-ec-39] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 13:04:29,475 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 13:04:29,479 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 13:04:33,727 [canton-env-ec-37] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - Now retrying operation 'fetch handshake'. 
2025-08-24 13:04:33,730 [canton-env-ec-39] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 13:04:34,488 [Thread-0] INFO  c.d.canton.environment.DomainNodes - Task closing node-mydomain still not completed after 5 seconds. Continue waiting...
2025-08-24 13:04:36,990 [Thread-0] INFO  c.d.canton.environment.DomainNodes - Task closing node-mydomain still not completed after 7500 milliseconds. Continue waiting...
2025-08-24 13:04:38,731 [canton-env-ec-39] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - Now retrying operation 'fetch handshake'. 
2025-08-24 13:04:38,735 [canton-env-ec-38] INFO  c.d.c.s.h.SequencerHandshake:domain=mydomain/client=topology-manager tid:32498ddf2e303185ac4b877ab2042c20 - The operation 'fetch handshake' was not successful. Retrying after 5s. Result: Left(HandshakeRequestError(Request failed for sequencer. Is the server running? Did you configure the server address as 0.0.0.0? Are you using the right TLS settings? (details logged as DEBUG)
  GrpcServiceUnavailable: UNAVAILABLE/io exception
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
  Request: handshake
  Causes: General OpenSslEngine problem
    No subject alternative names matching IP address 0.0.0.0 found,true)). 
2025-08-24 13:04:39,488 [Thread-0] INFO  c.d.canton.environment.DomainNodes - Task closing node-mydomain still not completed after 10 seconds. Continue waiting...
2025-08-24 13:04:39,497 [Thread-0] WARN  c.d.canton.environment.DomainNodes - Task closing node-mydomain did not complete within 10 seconds. Stack traces:
  Thread[#63,participant1-wallclock-0,5,main] is-daemon=false state=TIMED_WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
    java.base@21.0.8/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    java.base@21.0.8/java.lang.Thread.runWith(Thread.java:1596)
    java.base@21.0.8/java.lang.Thread.run(Thread.java:1583)

  Thread[#58,mydomain-wallclock-0,5,main] is-daemon=false state=TIMED_WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
    java.base@21.0.8/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    java.base@21.0.8/java.lang.Thread.runWith(Thread.java:1596)
    java.base@21.0.8/java.lang.Thread.run(Thread.java:1583)

  Thread[#100,delay-util-0,5,main] is-daemon=true state=TIMED_WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
    java.base@21.0.8/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
    java.base@21.0.8/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.base@21.0.8/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    java.base@21.0.8/java.lang.Thread.runWith(Thread.java:1596)
    java.base@21.0.8/java.lang.Thread.run(Thread.java:1583)

2025-08-24 13:04:39,498 [Thread-0] WARN  c.d.canton.environment.DomainNodes - Closing 'AsyncCloseable(name=node-mydomain)' failed! Reason:
java.util.concurrent.TimeoutException: Task closing node-mydomain did not complete within 10 seconds.
	at com.digitalasset.canton.util.FutureUtil$.retry$1(FutureUtil.scala:300)
	at com.digitalasset.canton.util.FutureUtil$.noisyAwaitResultForTesting(FutureUtil.scala:304)
	at com.digitalasset.canton.util.FutureUtil$.noisyAwaitResult(FutureUtil.scala:205)
	at com.digitalasset.canton.lifecycle.AsyncCloseable.close(FlagCloseableAsync.scala:36)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$1(Lifecycle.scala:47)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at com.digitalasset.canton.lifecycle.Lifecycle$.stopSingle$1(Lifecycle.scala:47)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$7(Lifecycle.scala:62)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:61)
	at com.digitalasset.canton.lifecycle.FlagCloseableAsync.onClosed(FlagCloseableAsync.scala:22)
	at com.digitalasset.canton.lifecycle.FlagCloseableAsync.onClosed$(FlagCloseableAsync.scala:22)
	at com.digitalasset.canton.environment.ManagedNodes.onClosed(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.onFirstClose(PerformUnlessClosing.scala:227)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.onFirstClose$(PerformUnlessClosing.scala:192)
	at com.digitalasset.canton.environment.ManagedNodes.onFirstClose(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.OnShutdownRunner.close(OnShutdownRunner.scala:93)
	at com.digitalasset.canton.lifecycle.OnShutdownRunner.close$(OnShutdownRunner.scala:83)
	at com.digitalasset.canton.environment.ManagedNodes.com$digitalasset$canton$lifecycle$FlagCloseable$$super$close(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.FlagCloseable.close(FlagCloseable.scala:27)
	at com.digitalasset.canton.lifecycle.FlagCloseable.close$(FlagCloseable.scala:27)
	at com.digitalasset.canton.environment.ManagedNodes.close(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$1(Lifecycle.scala:47)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at com.digitalasset.canton.lifecycle.Lifecycle$.stopSingle$1(Lifecycle.scala:47)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$7(Lifecycle.scala:62)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:61)
	at com.digitalasset.canton.environment.Environment.$anonfun$close$1(Environment.scala:559)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
	at scala.concurrent.package$.blocking(package.scala:124)
	at com.digitalasset.canton.environment.Environment.close(Environment.scala:540)
	at com.digitalasset.canton.environment.Environment.close$(Environment.scala:540)
	at com.digitalasset.canton.environment.CommunityEnvironment.close(CommunityEnvironment.scala:33)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3(CantonAppDriver.scala:109)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3$adapted(CantonAppDriver.scala:109)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$2(CantonAppDriver.scala:109)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-08-24 13:04:39,501 [Thread-0] WARN  c.d.c.e.CommunityEnvironment - Closing 'com.digitalasset.canton.environment.DomainNodes@7013ba3b' failed! Reason:
com.digitalasset.canton.lifecycle.ShutdownFailedException: Unable to close 'AsyncCloseable(name=node-mydomain)'.
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$8(Lifecycle.scala:65)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:65)
	at com.digitalasset.canton.lifecycle.FlagCloseableAsync.onClosed(FlagCloseableAsync.scala:22)
	at com.digitalasset.canton.lifecycle.FlagCloseableAsync.onClosed$(FlagCloseableAsync.scala:22)
	at com.digitalasset.canton.environment.ManagedNodes.onClosed(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.onFirstClose(PerformUnlessClosing.scala:227)
	at com.digitalasset.canton.lifecycle.PerformUnlessClosing.onFirstClose$(PerformUnlessClosing.scala:192)
	at com.digitalasset.canton.environment.ManagedNodes.onFirstClose(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.OnShutdownRunner.close(OnShutdownRunner.scala:93)
	at com.digitalasset.canton.lifecycle.OnShutdownRunner.close$(OnShutdownRunner.scala:83)
	at com.digitalasset.canton.environment.ManagedNodes.com$digitalasset$canton$lifecycle$FlagCloseable$$super$close(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.FlagCloseable.close(FlagCloseable.scala:27)
	at com.digitalasset.canton.lifecycle.FlagCloseable.close$(FlagCloseable.scala:27)
	at com.digitalasset.canton.environment.ManagedNodes.close(Nodes.scala:104)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$1(Lifecycle.scala:47)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:210)
	at com.digitalasset.canton.lifecycle.Lifecycle$.stopSingle$1(Lifecycle.scala:47)
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$7(Lifecycle.scala:62)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:61)
	at com.digitalasset.canton.environment.Environment.$anonfun$close$1(Environment.scala:559)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
	at scala.concurrent.package$.blocking(package.scala:124)
	at com.digitalasset.canton.environment.Environment.close(Environment.scala:540)
	at com.digitalasset.canton.environment.Environment.close$(Environment.scala:540)
	at com.digitalasset.canton.environment.CommunityEnvironment.close(CommunityEnvironment.scala:33)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3(CantonAppDriver.scala:109)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3$adapted(CantonAppDriver.scala:109)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$2(CantonAppDriver.scala:109)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-08-24 13:04:39,536 [canton-env-ec-38] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 13:04:39,560 [canton-env-ec-35] WARN  c.d.c.d.s.s.SequencerWriter:domain=mydomain tid:7cb82a5464f3157fd65bf9cc28a2d590 - Sequencer writer has completed with an unrecoverable error
org.apache.pekko.stream.AbruptTerminationException: Processor actor [Actor[pekko://canton-actor-system/system/Materializers/StreamSupervisor-0/flow-2-0-ignoreSink#739733304]] terminated abruptly
2025-08-24 13:04:39,567 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 13:04:39,582 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 13:04:39,583 [canton-env-ec-37] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished with failure: Processor actor [Actor[pekko://canton-actor-system/system/Materializers/StreamSupervisor-0/flow-9-0-ignoreSink#1756848078]] terminated abruptly
2025-08-24 13:04:39,584 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 13:04:39,585 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 13:04:39,587 [canton-env-ec-37] ERROR c.d.c.p.i.RecoveringIndexer:participant=participant1 - Error while running indexer, restart scheduled after 10 seconds
org.apache.pekko.stream.AbruptTerminationException: Processor actor [Actor[pekko://canton-actor-system/system/Materializers/StreamSupervisor-0/flow-9-0-ignoreSink#1756848078]] terminated abruptly
2025-08-24 13:04:39,590 [canton-env-ec-37] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 13:04:39,596 [Thread-0] ERROR c.d.canton.CantonCommunityApp$ - Failed to shut down successfully.
com.digitalasset.canton.lifecycle.ShutdownFailedException: Unable to close 'com.digitalasset.canton.environment.DomainNodes@7013ba3b'.
	at com.digitalasset.canton.lifecycle.Lifecycle$.$anonfun$close$8(Lifecycle.scala:65)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.lifecycle.Lifecycle$.close(Lifecycle.scala:65)
	at com.digitalasset.canton.environment.Environment.$anonfun$close$1(Environment.scala:559)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
	at scala.concurrent.package$.blocking(package.scala:124)
	at com.digitalasset.canton.environment.Environment.close(Environment.scala:540)
	at com.digitalasset.canton.environment.Environment.close$(Environment.scala:540)
	at com.digitalasset.canton.environment.CommunityEnvironment.close(CommunityEnvironment.scala:33)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3(CantonAppDriver.scala:109)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$3$adapted(CantonAppDriver.scala:109)
	at scala.Option.foreach(Option.scala:437)
	at com.digitalasset.canton.CantonAppDriver.$anonfun$new$2(CantonAppDriver.scala:109)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-08-24 13:04:39,597 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 13:33:08,510 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 13:33:09,643 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 13:33:09,670 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 13:33:09,904 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 13:33:09,989 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 13:33:10,057 [main] INFO  c.d.c.e.CommunityEnvironment tid:11649a46944079341b70dc34f47fbf10 - Automatically starting all instances
2025-08-24 13:33:10,117 [canton-env-ec-35] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 13:33:10,118 [canton-env-ec-39] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 13:33:10,144 [canton-env-ec-39] INFO  c.d.c.r.DbStorage:participant1 tid:356cc512663de91f6a726674ace281a2 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:33:10,144 [canton-env-ec-35] INFO  c.d.c.resource.DbStorage:mydomain tid:285fd9940a6503fcd7d0245f80aa986e - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:33:10,176 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 13:33:10,176 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 13:33:10,736 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Start completed.
2025-08-24 13:33:10,736 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Start completed.
2025-08-24 13:33:10,914 [canton-env-ec-35] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:33:10,914 [canton-env-ec-39] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:33:10,926 [canton-env-ec-39] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:33:10,926 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:33:10,926 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:33:10,926 [canton-env-ec-39] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:33:10,926 [canton-env-ec-39] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:33:10,926 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:33:11,079 [canton-env-ec-39] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:33:11,079 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 13:33:11,089 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:33:11,089 [canton-env-ec-39] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:33:11,139 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown initiated...
2025-08-24 13:33:11,139 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown initiated...
2025-08-24 13:33:11,145 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown completed.
2025-08-24 13:33:11,146 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown completed.
2025-08-24 13:33:11,203 [canton-env-ec-35] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 13:33:11,207 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 13:33:11,242 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 13:33:11,332 [canton-env-ec-39] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 13:33:11,335 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 13:33:11,362 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 13:33:11,420 [canton-env-ec-36] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:11,420 [canton-env-ec-37] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:11,429 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:33:11,429 [canton-env-ec-39] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:33:12,004 [canton-env-ec-39] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:33:12,005 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:33:12,081 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Node is not initialized yet. Performing automated default initialization.
2025-08-24 13:33:12,117 [canton-env-ec-65] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 13:33:12,649 [canton-env-ec-39] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 13:33:12,657 [canton-env-ec-36] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (7 ms)
2025-08-24 13:33:12,770 [canton-env-ec-35] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:9f75b726ea71215e48759cf529f4b82a - Recovering published timely rejections
2025-08-24 13:33:12,792 [canton-env-ec-68] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:9f75b726ea71215e48759cf529f4b82a - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 13:33:12,795 [canton-env-ec-39] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 13:33:12,797 [canton-env-ec-68] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:9f75b726ea71215e48759cf529f4b82a - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 1)) (exclusive) up to None (inclusive)
2025-08-24 13:33:12,846 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 13:33:12,849 [canton-env-ec-39] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:33:12,940 [canton-env-ec-37] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 13:33:13,316 [canton-env-ec-68] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 13:33:13,431 [canton-env-ec-39] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 13:33:13,529 [canton-env-ec-39] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:33:13,532 [canton-env-ec-39] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:33:13,548 [canton-env-ec-65] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - No event found up to 0001-01-01T00:00:00Z. Resubscribing from the beginning.
2025-08-24 13:33:13,549 [canton-env-ec-65] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 0001-01-01T00:00:00.000001Z on
2025-08-24 13:33:13,620 [canton-env-ec-38] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp None; next counter 0
2025-08-24 13:33:13,632 [canton-env-ec-38] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:13,649 [canton-env-ec-39] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:33:13,693 [canton-env-ec-39] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.038s)
2025-08-24 13:33:13,696 [canton-env-ec-68] INFO  c.d.c.d.i.TopologyManagementInitialization$:domain=mydomain - Sending initial topology transactions to domain members Set(DOM::mydomain::122060f2dea2..., MED::mydomain::122060f2dea2...) as DOM::mydomain::122060f2dea2...
2025-08-24 13:33:13,720 [canton-env-ec-39] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 13:33:13,721 [canton-env-ec-39] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 13:33:13,725 [canton-env-ec-39] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 13:33:13,770 [canton-env-ec-37] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 13:33:13,777 [canton-env-ec-66] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 13:33:13,780 [canton-env-ec-66] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 13:33:13,789 [canton-env-ec-65] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 13:33:13,806 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:33:13,806 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 13:33:13,813 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 13:33:13,813 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 13:33:13,814 [canton-env-ec-65] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 13:33:13,816 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 13:33:13,822 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:33:13,825 [canton-env-ec-66] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 13:33:13,846 [canton-env-ec-66] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 13:33:13,875 [canton-env-ec-37] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:33:13.867980Z
2025-08-24 13:33:13,885 [canton-env-ec-39] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 13:33:13,889 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 13:33:13,915 [canton-env-ec-102] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000002)),0,0)
2025-08-24 13:33:13,925 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:33:13,927 [canton-env-ec-39] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 13:33:13,932 [canton-env-ec-68] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 13:33:13,939 [canton-env-ec-38] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (18 ms)
2025-08-24 13:33:13,979 [canton-env-ec-37] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (39 ms)
2025-08-24 13:33:13,982 [canton-env-ec-68] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000002)).
2025-08-24 13:33:14,005 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - 'DOM::mydomain::122060f2dea2...' sends request with id '39517630-8bab-4ccb-bf1c-a99c999b2365' of size 1801 bytes with 2 envelopes.
2025-08-24 13:33:14,064 [canton-env-ec-68] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 13:33:14,065 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 13:33:14,093 [canton-env-ec-35] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 13:33:14,128 [canton-env-ec-38] INFO  c.d.c.d.s.s.SequencerReader$EventsReader:domain=mydomain/subscriber=DOM::mydomain::122060f2dea2 tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Using approximate topology snapshot at 0001-01-01T00:00:00.000001Z for desired timestamp 2025-08-24T13:33:14.056514Z
2025-08-24 13:33:14,139 [canton-env-ec-102] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 13:33:14,167 [canton-env-ec-39] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 13:33:14,168 [canton-env-ec-35] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:33:14,242 [canton-env-ec-36] INFO  c.d.c.t.p.DomainTopologyTransactionMessageValidator$Impl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Using approximate topology snapshot at 0001-01-01T00:00:00.000001Z for desired timestamp 2025-08-24T13:33:14.056514Z
2025-08-24 13:33:14,302 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Updated topology change delay from=0s to 0.25s
2025-08-24 13:33:14,307 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 1/6 Add NamespaceDelegation(122060f2dea2..., SigningPublicKey(id = 122060f2dea2..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,312 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 2/6 Replace DomainParametersChange(
  mydomain::122060f2dea2...,
  DynamicDomainParameters(
    participant response timeout = 30s,
    mediator reaction timeout = 30s,
    transfer exclusivity timeout = 1m,
    topology change delay = 0.25s,
    ledger time record time tolerance = 1m,
    mediator deduplication timeout = 2m,
    reconciliation interval = 1m,
    max rate per participant = 1000000,
    max request size = 10485760,
    catchup config = CatchUpConfig(catchUpIntervalSkip = 5, nrIntervalsToTriggerCatchUp = 2)
  )
) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,314 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 3/6 Add OwnerToKeyMapping(DOM::mydomain::122060f2dea2..., SigningPublicKey(id = 1220289626ce..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,315 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 4/6 Add OwnerToKeyMapping(MED::mydomain::122060f2dea2..., SigningPublicKey(id = 1220120ea179..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,316 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 5/6 Add MediatorDomainState(Both, mydomain::122060f2dea2..., MED::mydomain::122060f2dea2...) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,317 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 6/6 Add OwnerToKeyMapping(SEQ::mydomain::122060f2dea2..., SigningPublicKey(id = 122096c87f5e..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,329 [canton-env-ec-66] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 13:33:14,329 [canton-env-ec-66] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 13:33:14,330 [canton-env-ec-66] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 13:33:14,430 [canton-env-ec-68] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:14,500 [canton-env-ec-66] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 13:33:14,503 [canton-env-ec-37] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 13:33:14,541 [canton-env-ec-68] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - No event found up to 0001-01-01T00:00:00Z. Resubscribing from the beginning.
2025-08-24 13:33:14,542 [canton-env-ec-68] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 0001-01-01T00:00:00.000001Z on
2025-08-24 13:33:14,552 [canton-env-ec-102] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp None; next counter 0
2025-08-24 13:33:14,553 [canton-env-ec-102] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:14,561 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:13c3b9cf161d62785e8418c8d17fede5 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:33:14,618 [canton-env-ec-37] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:33:14.615093Z
2025-08-24 13:33:14,631 [canton-env-ec-68] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:33:14,664 [canton-env-ec-65] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:33:14,678 [canton-env-ec-66] INFO  c.d.c.d.m.Mediator:domain=mydomain/node=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Caught up with batch with counter=0 with sequencer with 621 ms delay
2025-08-24 13:33:14,680 [canton-env-ec-66] INFO  c.d.c.t.p.DomainTopologyTransactionMessageValidator$Impl:domain=mydomain/node=mediator/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Using approximate topology snapshot at 0001-01-01T00:00:00.000001Z for desired timestamp 2025-08-24T13:33:14.056514Z
2025-08-24 13:33:14,702 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Updated topology change delay from=0s to 0.25s
2025-08-24 13:33:14,704 [canton-env-ec-65] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 1/6 Add NamespaceDelegation(122060f2dea2..., SigningPublicKey(id = 122060f2dea2..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,706 [canton-env-ec-65] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 2/6 Replace DomainParametersChange(
  mydomain::122060f2dea2...,
  DynamicDomainParameters(
    participant response timeout = 30s,
    mediator reaction timeout = 30s,
    transfer exclusivity timeout = 1m,
    topology change delay = 0.25s,
    ledger time record time tolerance = 1m,
    mediator deduplication timeout = 2m,
    reconciliation interval = 1m,
    max rate per participant = 1000000,
    max request size = 10485760,
    catchup config = CatchUpConfig(catchUpIntervalSkip = 5, nrIntervalsToTriggerCatchUp = 2)
  )
) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,707 [canton-env-ec-65] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 3/6 Add OwnerToKeyMapping(DOM::mydomain::122060f2dea2..., SigningPublicKey(id = 1220289626ce..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,708 [canton-env-ec-65] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 4/6 Add OwnerToKeyMapping(MED::mydomain::122060f2dea2..., SigningPublicKey(id = 1220120ea179..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,709 [canton-env-ec-65] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 5/6 Add MediatorDomainState(Both, mydomain::122060f2dea2..., MED::mydomain::122060f2dea2...) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:14,710 [canton-env-ec-65] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Storing topology transaction 6/6 Add OwnerToKeyMapping(SEQ::mydomain::122060f2dea2..., SigningPublicKey(id = 122096c87f5e..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T13:33:14.056514Z) (epsilon=0 ms)
2025-08-24 13:33:15,934 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f61feb79bb9a29cb263a8d0f47481700 - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 13:33:15,934 [canton-env-ec-145] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:83ff8c8d5210faa00759268c68e45b10 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 13:33:15,935 [canton-env-ec-143] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9c6e1d6f9abfc5fb1e1d032202d57a83 - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 13:33:15,934 [canton-env-ec-144] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:3ec2ac97f04f9855b225010436720abb - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 13:33:15,936 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:858e41522d9d9317cc7929505246d862 - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 13:33:15,934 [canton-env-ec-148] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d800062202b0cfb993716b04580a8aac - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 13:33:15,937 [canton-env-ec-149] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:79ab8bb75f26c9f90af78946c88f82df - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 13:33:15,938 [canton-env-ec-68] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ca9d1bd178c6ca8468891e39c898f755 - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 13:33:15,937 [canton-env-ec-146] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:44fe8cabf4838dca9d697a4a1480cefb - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 13:33:15,940 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b105aa21b3dc17c7ae3d37891f49ffdd - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 13:33:15,941 [canton-env-ec-155] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7f34e739016f18d9207f408e7c10eaef - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 13:33:15,942 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e6777d7651a1a610bf3228a6d41f6aa8 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 13:33:15,940 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a2ef2262b6e02c018258bdf35a700f52 - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 13:33:15,945 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:af3d2cc1f52a73e99757a15c2f4b87f6 - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 13:33:15,945 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:abb7da992ade7e749b6fb189626fe8d5 - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 13:33:15,946 [canton-env-ec-147] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:562d025467bbe04def480376f889d6c7 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:33:15,947 [canton-env-ec-145] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:3b23406de91d42925a3dbd4938f86b0d - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 13:33:15,948 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:96b13032dcc41e148bb75349a256c978 - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 13:33:15,946 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f975cab86217a069fdb02b44baf41896 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 13:33:15,945 [canton-env-ec-152] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d66942026f89a59520d40284defa650b - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 13:33:15,944 [canton-env-ec-155] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:db2944157678894b9185ee375636a075 - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 13:33:15,949 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:76a1c7159ba0a00d1565fe90eaf2673c - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 13:33:15,944 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:405299e3484661a61fb4218ee652935b - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 13:33:15,950 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:bce430c28eeab408cb2dff237935dbda - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 13:33:15,956 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d24a4bca070ba5898af0ca2819fdbe81 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 13:33:15,959 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:75503a03f02dec87acd52457a6bcb05b - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 13:33:15,960 [canton-env-ec-148] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6d062d6dc5177f9baa5fe06bc41d6314 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 13:33:15,941 [canton-env-ec-150] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:56d741d940d858166f45604b101e3d48 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 13:33:15,961 [canton-env-ec-154] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b721bed8ee9c10f1d10bbd28460c1922 - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 13:33:15,959 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2f4a3157ce86ea231ea284a0a13732d5 - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 13:33:15,956 [canton-env-ec-156] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:15afcbeeb4ead890e3a8c44a5fe57b2c - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 13:33:15,948 [canton-env-ec-151] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5e7709e5d20942deacb5fe694ed2b1a4 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 13:33:18,147 [input-mapping-pool-0] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:273f3471293b723c77faa8561c324816 - Storing at offset=000000000000000003 PublicPackageUpload(
  recordTime = 2025-08-24T13:33:17.724572Z,
  archives = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  ),
  sourceDescription = 'AdminWorkflowsWithVacuuming'
)
2025-08-24 13:33:18,357 [canton-env-ec-37] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:273f3471293b723c77faa8561c324816 - Applied topology transaction Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) at 2025-08-24T13:33:18.356323Z
2025-08-24 13:33:18,414 [canton-env-ec-148] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:42bcbf1f6f0c41ebf8c557d75ef44d60 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:33:18,433 [canton-env-ec-148] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:41321990b0b61ccd93e5a50a0e764fe2 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:33:18,440 [canton-env-ec-148] INFO  c.d.c.p.a.PingService:participant=participant1 - The operation 'wait-for-admin-workflows-to-appear-on-ledger-api' was not successful. New kind of error: no success error (request infinite retries). Retrying after 0.1s. Result: Outcome(false). 
2025-08-24 13:33:18,510 [canton-env-ec-68] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:8601f0ccfcc158ad1eec42c15aca4361 - Received request for transactions, startExclusive -> '000000000000000002', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 13:33:18,543 [canton-env-ec-68] INFO  c.d.c.p.a.PingService:participant=participant1 - Now retrying operation 'wait-for-admin-workflows-to-appear-on-ledger-api'. 
2025-08-24 13:33:18,548 [canton-env-ec-150] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6568c655b747e4dfccaecc1888bbd084 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:33:18,613 [canton-env-ec-68] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:9267c6bc1ab2a29b453760df2912c856 - Applied topology transaction Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., SigningPublicKey(id = 12205c0df38e..., format = Tink, scheme = Ed25519)) at 2025-08-24T13:33:18.613247Z
2025-08-24 13:33:18,632 [canton-env-ec-150] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:9267c6bc1ab2a29b453760df2912c856 - Applied topology transaction Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., EncryptionPublicKey(id = 12204d828285..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) at 2025-08-24T13:33:18.631325Z
2025-08-24 13:33:18,638 [canton-env-ec-102] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Initializing node with id NodeId(participant1::1220b845dcf0...)
2025-08-24 13:33:18,650 [canton-env-ec-102] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:11649a46944079341b70dc34f47fbf10 - Reconnecting to domains List(). Already connected: Set()
2025-08-24 13:33:18,652 [canton-env-ec-39] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:11649a46944079341b70dc34f47fbf10 - Successfully re-connected to domains List()
2025-08-24 13:33:18,654 [main] INFO  c.d.c.e.CommunityEnvironment tid:11649a46944079341b70dc34f47fbf10 - Successfully started all nodes
2025-08-24 13:33:29,154 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 13:33:29,157 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 13:33:29,165 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:29,167 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:29,169 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:29,170 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:29,183 [canton-env-ec-39] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 13:33:29,193 [canton-env-ec-150] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:8601f0ccfcc158ad1eec42c15aca4361 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:54384: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:33:29,204 [canton-env-ec-154] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 13:33:29,215 [canton-env-ec-150] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 13:33:29,219 [canton-env-ec-150] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 13:33:29,219 [canton-env-ec-102] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 13:33:29,220 [canton-env-ec-102] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 13:33:29,221 [canton-env-ec-102] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 13:33:29,223 [canton-env-ec-148] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 13:33:29,239 [canton-env-ec-148] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 13:33:29,240 [canton-env-ec-148] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 13:33:29,241 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 13:33:29,243 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 13:33:29,243 [canton-env-ec-102] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 13:33:29,243 [canton-env-ec-102] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 13:33:29,246 [canton-env-ec-102] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 13:33:29,249 [canton-env-ec-102] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 13:33:29,263 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:29,264 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:29,265 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:29,266 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:29,267 [canton-env-ec-39] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:29,268 [canton-env-ec-39] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:33:29,269 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 13:33:29,274 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 13:33:29,275 [canton-env-ec-39] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:29,276 [canton-env-ec-39] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:29,278 [canton-env-ec-39] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 13:33:29,281 [canton-env-ec-39] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:29,281 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:29,286 [canton-env-ec-39] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:33:29,287 [canton-env-ec-39] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:33:29,289 [canton-env-ec-148] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 13:33:29,291 [canton-env-ec-37] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:43791d9872bc38c9559263004f5e69cc - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:34056: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:33:29,302 [canton-env-ec-39] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:33:29,303 [canton-env-ec-39] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:33:29,304 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 13:33:29,305 [canton-env-ec-37] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:e77ec89f76e34a7b78b91bcca2100eb1 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:34074: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:33:29,315 [canton-env-ec-39] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 13:33:29,316 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 13:33:29,317 [canton-env-ec-39] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:33:29,331 [canton-env-ec-39] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:29,332 [canton-env-ec-39] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:33:29,333 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 13:33:29,339 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 13:33:29,341 [canton-env-ec-39] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:29,342 [canton-env-ec-39] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:29,365 [canton-env-ec-68] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 13:33:29,388 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 13:33:29,388 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 13:33:34,228 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 13:33:35,367 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 13:33:35,413 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 13:33:35,656 [canton-env-ec-34] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 13:33:35,776 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 13:33:35,835 [main] INFO  c.d.c.e.CommunityEnvironment tid:8a88d5e61923c7eabb255e262ec6cf68 - Automatically starting all instances
2025-08-24 13:33:35,888 [canton-env-ec-35] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 13:33:35,888 [canton-env-ec-34] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 13:33:35,913 [canton-env-ec-34] INFO  c.d.c.r.DbStorage:participant1 tid:011b3b8be543b580757e1eacf6ffbd0a - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:33:35,914 [canton-env-ec-35] INFO  c.d.c.resource.DbStorage:mydomain tid:c6bb33e1564032aef1c26e1d283559cc - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:33:35,944 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 13:33:35,944 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 13:33:36,482 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 13:33:36,482 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 13:33:36,659 [canton-env-ec-34] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:33:36,659 [canton-env-ec-35] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:33:36,667 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:33:36,667 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:33:36,668 [canton-env-ec-34] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:33:36,667 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:33:36,668 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:33:36,668 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:33:36,730 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 13:33:36,730 [canton-env-ec-34] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:33:36,739 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:33:36,739 [canton-env-ec-34] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:33:36,816 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 13:33:36,816 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 13:33:36,821 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 13:33:36,821 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 13:33:36,887 [canton-env-ec-35] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 13:33:36,890 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 13:33:36,914 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 13:33:36,988 [canton-env-ec-34] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 13:33:36,990 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 13:33:37,017 [canton-env-ec-34] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 13:33:37,124 [canton-env-ec-62] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:37,124 [canton-env-ec-36] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:37,133 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:33:37,133 [canton-env-ec-34] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:33:37,793 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:33:37,800 [canton-env-ec-34] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:33:37,890 [canton-env-ec-34] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 13:33:37,890 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 13:33:38,165 [canton-env-ec-63] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 13:33:38,176 [canton-env-ec-34] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (10 ms)
2025-08-24 13:33:38,266 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:e333c5b4b2142ee97e641222f3428cde - Recovering published timely rejections
2025-08-24 13:33:38,309 [canton-env-ec-62] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:e333c5b4b2142ee97e641222f3428cde - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 13:33:38,313 [canton-env-ec-62] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:e333c5b4b2142ee97e641222f3428cde - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 2)) (exclusive) up to None (inclusive)
2025-08-24 13:33:38,424 [canton-env-ec-34] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 13:33:38,539 [canton-env-ec-35] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 13:33:38,572 [canton-env-ec-65] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 13:33:38,574 [canton-env-ec-65] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:33:38,748 [canton-env-ec-35] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 13:33:38,834 [canton-env-ec-63] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 13:33:38,877 [canton-env-ec-63] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:33:38,880 [canton-env-ec-63] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:33:39,002 [canton-env-ec-63] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:33:39,044 [canton-env-ec-63] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.036s)
2025-08-24 13:33:39,074 [canton-env-ec-63] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 13:33:39,075 [canton-env-ec-63] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 13:33:39,078 [canton-env-ec-63] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 13:33:39,124 [canton-env-ec-34] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 13:33:39,131 [canton-env-ec-65] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 13:33:39,134 [canton-env-ec-65] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 13:33:39,141 [canton-env-ec-65] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 13:33:39,154 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:33:39,154 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 13:33:39,160 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 13:33:39,160 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 13:33:39,163 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 13:33:39,166 [canton-env-ec-65] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 13:33:39,169 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:33:39,173 [canton-env-ec-64] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 13:33:39,197 [canton-env-ec-64] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 13:33:39,227 [canton-env-ec-62] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 13:33:39,232 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 13:33:39,259 [canton-env-ec-63] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000003)),0,0)
2025-08-24 13:33:39,263 [canton-env-ec-63] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 13:33:39,265 [canton-env-ec-63] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 13:33:39,309 [canton-env-ec-34] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 13:33:39,372 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 13:33:39,809 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 13:33:39,823 [canton-env-ec-63] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 13:33:39,839 [canton-env-ec-63] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:39,911 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 13:33:39,980 [canton-env-ec-153] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (714 ms)
2025-08-24 13:33:40,007 [canton-env-ec-65] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (26 ms)
2025-08-24 13:33:40,011 [canton-env-ec-154] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000003)).
2025-08-24 13:33:40,019 [canton-env-ec-153] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:40,083 [canton-env-ec-35] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 13:33:40,113 [canton-env-ec-154] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 13:33:40,114 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 13:33:40,133 [canton-env-ec-152] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 13:33:40,148 [canton-env-ec-64] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:33:40.137305Z
2025-08-24 13:33:40,158 [canton-env-ec-152] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 13:33:40,158 [canton-env-ec-154] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 13:33:40,189 [canton-env-ec-154] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 13:33:40,190 [canton-env-ec-154] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:33:40,195 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:33:40,253 [canton-env-ec-34] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:33:40.250454Z
2025-08-24 13:33:40,292 [canton-env-ec-152] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:33:40,331 [canton-env-ec-62] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 13:33:40,332 [canton-env-ec-62] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 13:33:40,332 [canton-env-ec-62] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 13:33:40,370 [canton-env-ec-135] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:33:40,370 [canton-env-ec-34] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:33:40,476 [canton-env-ec-36] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 13:33:40,478 [canton-env-ec-35] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 13:33:40,551 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:3f3ffe8c8ac40a7fa41f1fae0d2b3322 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:33:41,328 [canton-env-ec-152] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:de0e222af84f0762fa7d49739a081ce2 - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 13:33:41,328 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ddffa52240269cf443c742ce9c600448 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 13:33:41,328 [canton-env-ec-34] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:06d68d96d11139609e72f93700cb2706 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 13:33:41,331 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:960002caa2fefd6d921c503c0b6bdc6e - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 13:33:41,337 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:04ac340d785ce70aa00cdf2dce8c8337 - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 13:33:41,332 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5b5424a467f2ab97fd14f807f0e91e4f - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 13:33:41,330 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:14c2b3339b6d4d238235d0bb0112d9da - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 13:33:41,343 [canton-env-ec-135] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0af6bfec41f603fd91cd1b55ca1b6868 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:33:41,344 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:df2d3dee9f5c8fbf8d98c980ba63b4b4 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 13:33:41,330 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0c424e92b4abd87c6a03e736d16c2f39 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 13:33:41,345 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7b1dfc03ee3484766d3cb4d43ab72e02 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 13:33:41,348 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c583eb50562030d2623036b279b10033 - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 13:33:41,345 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ea9d2d041b7d8ad1e84d1d6bdc6fc986 - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 13:33:41,345 [canton-env-ec-135] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5376e854ba95865cb168440728c12ef2 - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 13:33:41,350 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1beed8d682dc1c95c48d99bf8eb51837 - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 13:33:41,342 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c67eaf8d223b74dfd027bd7b47536b9b - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 13:33:41,351 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9b230fd44e04353b49262ba6d0bb222f - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 13:33:41,351 [canton-env-ec-154] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:00e9ab15a960a9d8ca4499a2ac816583 - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 13:33:41,342 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d81f7b167c39a658b1f03fb833fbdcff - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 13:33:41,353 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:13aa6cfc81e793638dece53a3c8b07cc - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 13:33:41,337 [canton-env-ec-34] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c58aafa53d33b6100651e562e23b5930 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 13:33:41,337 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b427db862546bf4ffad035c1dc568869 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 13:33:41,355 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:016ba63dc79b37f3a72b7073a0ef9f3c - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 13:33:41,358 [canton-env-ec-152] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:37b4cfb24c14dabe2bb21ba6fd2f6793 - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 13:33:41,335 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:3665550f4caeb1863a902e7abe3a3f90 - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 13:33:41,335 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f510472240f32ae71213130fd8e7328c - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 13:33:41,358 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d8321fa87545748515a169f915c04a14 - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 13:33:41,373 [canton-env-ec-166] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:755ed7cc7c74c5586d23ec5eea77b41b - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 13:33:41,376 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fb9f37636cc137f2c741c70a99e4cc53 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 13:33:41,378 [canton-env-ec-135] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8cf7338c6050abba6f96f19385bb524a - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 13:33:41,349 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:63048de6d27153122fde394b483d44f1 - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 13:33:41,383 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:46e49aab1aa5113b70d5e080a4418e08 - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 13:33:41,464 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:34fbb71703a97e6f6f057c1a237e5897 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:33:41,488 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5bb2d10a6965157363c5469f99168213 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:33:41,601 [canton-env-ec-35] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:dccfba0389883243203786d32b9d6655 - Received request for transactions, startExclusive -> '000000000000000003', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 13:33:41,658 [canton-env-ec-62] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:8a88d5e61923c7eabb255e262ec6cf68 - Reconnecting to domains List(). Already connected: Set()
2025-08-24 13:33:41,663 [canton-env-ec-62] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:8a88d5e61923c7eabb255e262ec6cf68 - Successfully re-connected to domains List()
2025-08-24 13:33:41,665 [main] INFO  c.d.c.e.CommunityEnvironment tid:8a88d5e61923c7eabb255e262ec6cf68 - Successfully started all nodes
2025-08-24 13:33:42,810 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 13:33:42,813 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 13:33:42,823 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:42,826 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:42,828 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:42,829 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:42,842 [canton-env-ec-161] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 13:33:42,853 [canton-env-ec-35] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:dccfba0389883243203786d32b9d6655 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:41358: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:33:42,866 [canton-env-ec-158] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 13:33:42,884 [canton-env-ec-158] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 13:33:42,888 [canton-env-ec-158] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 13:33:42,889 [canton-env-ec-159] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 13:33:42,891 [canton-env-ec-159] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 13:33:42,891 [canton-env-ec-159] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 13:33:42,896 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 13:33:42,912 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 13:33:42,913 [canton-env-ec-36] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 13:33:42,915 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 13:33:42,916 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 13:33:42,917 [canton-env-ec-159] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 13:33:42,918 [canton-env-ec-159] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 13:33:42,922 [canton-env-ec-35] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 13:33:42,929 [canton-env-ec-35] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 13:33:42,945 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:42,946 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:42,947 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:42,948 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:33:42,949 [canton-env-ec-161] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:42,950 [canton-env-ec-161] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:33:42,952 [canton-env-ec-161] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 13:33:42,958 [canton-env-ec-161] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 13:33:42,959 [canton-env-ec-161] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:42,960 [canton-env-ec-161] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:42,964 [canton-env-ec-161] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 13:33:42,968 [canton-env-ec-161] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:42,969 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:42,974 [canton-env-ec-161] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:33:42,975 [canton-env-ec-161] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:33:42,978 [canton-env-ec-165] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 13:33:42,979 [canton-env-ec-158] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:25d7a499f5faa9a1fcb7fb100c738887 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:40244: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:33:42,988 [canton-env-ec-161] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:33:42,989 [canton-env-ec-161] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:33:42,990 [canton-env-ec-62] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 13:33:42,991 [canton-env-ec-158] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:8710433ab91722f8f848c2814fb37a67 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:40260: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:33:42,998 [canton-env-ec-161] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 13:33:43,000 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 13:33:43,000 [canton-env-ec-161] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:33:43,014 [canton-env-ec-161] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:43,015 [canton-env-ec-161] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:33:43,016 [canton-env-ec-161] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 13:33:43,022 [canton-env-ec-161] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 13:33:43,024 [canton-env-ec-161] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:43,024 [canton-env-ec-161] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:33:43,050 [canton-env-ec-35] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 13:33:43,082 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 13:33:43,082 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 13:34:22,954 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 13:34:24,058 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 13:34:24,079 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 13:34:24,282 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 13:34:24,381 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 13:34:24,456 [main] INFO  c.d.c.e.CommunityEnvironment tid:0b7c5b2d8879dcaaefd74a7229b1aac0 - Automatically starting all instances
2025-08-24 13:34:24,512 [canton-env-ec-41] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 13:34:24,513 [canton-env-ec-36] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 13:34:24,531 [canton-env-ec-41] INFO  c.d.c.r.DbStorage:participant1 tid:07431020ae251b0f80e0ae4fc5f1705b - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:34:24,532 [canton-env-ec-36] INFO  c.d.c.resource.DbStorage:mydomain tid:c794f09bbc21a308726b679f9edba400 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:34:24,568 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 13:34:24,568 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 13:34:25,183 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 13:34:25,183 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 13:34:25,346 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:34:25,346 [canton-env-ec-41] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:34:25,356 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:34:25,356 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:34:25,357 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:34:25,357 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:34:25,357 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:34:25,358 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:34:25,457 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 13:34:25,457 [canton-env-ec-41] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:34:25,466 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:34:25,466 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:34:25,517 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 13:34:25,517 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 13:34:25,526 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 13:34:25,529 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 13:34:25,575 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 13:34:25,578 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 13:34:25,619 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 13:34:25,679 [canton-env-ec-41] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 13:34:25,681 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 13:34:25,707 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 13:34:25,796 [canton-env-ec-35] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:25,796 [canton-env-ec-64] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:25,804 [canton-env-ec-41] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:34:25,805 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:34:26,462 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:34:26,473 [canton-env-ec-41] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:34:26,557 [canton-env-ec-64] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 13:34:26,564 [canton-env-ec-59] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 13:34:26,892 [canton-env-ec-37] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 13:34:26,899 [canton-env-ec-63] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (6 ms)
2025-08-24 13:34:27,021 [canton-env-ec-63] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:89341c17cce76c5bfb97264e55afbb90 - Recovering published timely rejections
2025-08-24 13:34:27,043 [canton-env-ec-64] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:89341c17cce76c5bfb97264e55afbb90 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 13:34:27,047 [canton-env-ec-64] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:89341c17cce76c5bfb97264e55afbb90 - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 2)) (exclusive) up to None (inclusive)
2025-08-24 13:34:27,194 [canton-env-ec-41] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 13:34:27,214 [canton-env-ec-35] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 13:34:27,269 [canton-env-ec-59] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 13:34:27,273 [canton-env-ec-59] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:34:27,613 [canton-env-ec-37] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 13:34:27,710 [canton-env-ec-62] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 13:34:27,795 [canton-env-ec-62] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:34:27,799 [canton-env-ec-62] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:34:27,906 [canton-env-ec-62] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:34:27,954 [canton-env-ec-62] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.043s)
2025-08-24 13:34:27,991 [canton-env-ec-62] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 13:34:27,992 [canton-env-ec-62] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 13:34:27,996 [canton-env-ec-62] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 13:34:28,035 [canton-env-ec-36] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 13:34:28,039 [canton-env-ec-62] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 13:34:28,042 [canton-env-ec-63] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 13:34:28,046 [canton-env-ec-63] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 13:34:28,053 [canton-env-ec-63] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 13:34:28,069 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:34:28,070 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 13:34:28,077 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 13:34:28,077 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 13:34:28,079 [canton-env-ec-63] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 13:34:28,080 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 13:34:28,087 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:34:28,090 [canton-env-ec-62] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 13:34:28,115 [canton-env-ec-62] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 13:34:28,119 [canton-env-ec-64] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 13:34:28,128 [canton-env-ec-64] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:28,149 [canton-env-ec-63] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 13:34:28,155 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 13:34:28,158 [canton-env-ec-41] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 13:34:28,159 [canton-env-ec-41] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:28,190 [canton-env-ec-62] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000003)),0,0)
2025-08-24 13:34:28,266 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:28,367 [canton-env-ec-36] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:28,373 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:28,500 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:28,701 [canton-env-ec-63] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:34:28.689408Z
2025-08-24 13:34:28,731 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 5/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:28,795 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:34:28,833 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 6/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:28,839 [canton-env-ec-41] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 13:34:28,879 [canton-env-ec-106] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (684 ms)
2025-08-24 13:34:28,895 [canton-env-ec-121] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 13:34:28,897 [canton-env-ec-121] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:28,903 [canton-env-ec-59] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (22 ms)
2025-08-24 13:34:28,905 [canton-env-ec-106] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000003)).
2025-08-24 13:34:28,961 [canton-env-ec-41] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:34:28.957337Z
2025-08-24 13:34:28,980 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:34:29,000 [canton-env-ec-106] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 13:34:29,001 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 13:34:29,004 [canton-env-ec-63] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 13:34:29,008 [canton-env-ec-59] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:34:29,011 [canton-env-ec-64] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:34:29,045 [canton-env-ec-121] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 13:34:29,074 [canton-env-ec-121] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 13:34:29,217 [canton-env-ec-63] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 13:34:29,218 [canton-env-ec-63] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 13:34:29,218 [canton-env-ec-63] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 13:34:29,371 [canton-env-ec-154] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 13:34:29,373 [canton-env-ec-63] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 13:34:29,471 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:c4b3a3abbe2390a9721a606f4dbb571d - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:34:30,398 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:697b833d285b5c52d1c5f3eb7840c8dc - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 13:34:30,399 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:801db21a3c7bec9900d426d0bf3fc4e3 - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 13:34:30,399 [canton-env-ec-59] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:28c254cc2c028da126c46fa0af1b405f - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 13:34:30,399 [canton-env-ec-106] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:95fa9309dc22014b457ae8bb546a6465 - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 13:34:30,398 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b285967acf89f48f8255897cc521badf - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 13:34:30,398 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:951e34254d455cc7caf4e07c2febc722 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 13:34:30,398 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:cd5f18e013a9438dab45efa14406d2cf - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 13:34:30,403 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:814f84be64e08552b677e87eaf7697c5 - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 13:34:30,405 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e265ffe7819915417a56cfd6a7f59987 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 13:34:30,407 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8dc24b652dbbef31ea7340167a088505 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 13:34:30,408 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:92c2e0472c2c2aa8b9ec16469a5684a9 - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 13:34:30,398 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:71838f616ceee63d1c559b97ee9a63fb - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 13:34:30,411 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c332db941e8a950fd0cf6e6b094e500a - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 13:34:30,412 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8b27eb7e77771f50400a022d8258da31 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 13:34:30,398 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a96cb8f2c30a12389506160df8f21942 - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 13:34:30,414 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a6cf46f73adba3d6680a721c11b932ad - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 13:34:30,419 [canton-env-ec-121] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:563128d126c99513e18bc9172d0ef3ab - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 13:34:30,420 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fa72142afdb6154721034092b25745c0 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 13:34:30,421 [canton-env-ec-41] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:23f8dfa1b07c25fbc64fd7505b357c81 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 13:34:30,421 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2af92c08a6622c42d08f41fa627559ee - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 13:34:30,420 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d09197b8f156daae9e14217589158523 - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 13:34:30,413 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ed859a21522fc76592446e40083d3a6a - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 13:34:30,407 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:faf9f369f2a5edf8929caf5797823a94 - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 13:34:30,405 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c28ec9108132fd0302d3e5641fecc604 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 13:34:30,436 [canton-env-ec-59] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c7613fde53af48da3008d6594ff1003c - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 13:34:30,441 [canton-env-ec-106] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2c379228acfa30a8fdcee0159a17e1c8 - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 13:34:30,403 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:019e8ccbfd0f74034655b799f6f1ad69 - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 13:34:30,403 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:4ed256e60b4160226dbfe397fd1f017d - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 13:34:30,445 [canton-env-ec-166] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:81c697cfcbe11507e2a8c2e38af20936 - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 13:34:30,448 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fc01f69ade57bd4fc437f2c749f1d3e9 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 13:34:30,449 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2c248cb6547a7dd17b8bbdda4ed1ff75 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:34:30,436 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8a0b5f604ffdf54ab34b0ff497397393 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 13:34:30,532 [canton-env-ec-166] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:cc5744a73ec3d28118570cee5b560cd0 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:34:30,556 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:459ac197c8d88b72e9646142f80ebf4b - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:34:30,678 [canton-env-ec-106] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:0afb307c3c30945dc898d375718606d4 - Received request for transactions, startExclusive -> '000000000000000003', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 13:34:30,750 [canton-env-ec-154] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:0b7c5b2d8879dcaaefd74a7229b1aac0 - Reconnecting to domains List(). Already connected: Set()
2025-08-24 13:34:30,753 [canton-env-ec-106] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:0b7c5b2d8879dcaaefd74a7229b1aac0 - Successfully re-connected to domains List()
2025-08-24 13:34:30,756 [main] INFO  c.d.c.e.CommunityEnvironment tid:0b7c5b2d8879dcaaefd74a7229b1aac0 - Successfully started all nodes
2025-08-24 13:34:31,974 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 13:34:31,977 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 13:34:31,986 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:34:31,988 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:34:31,990 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:34:31,991 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:34:32,000 [canton-env-ec-163] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 13:34:32,009 [canton-env-ec-106] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:0afb307c3c30945dc898d375718606d4 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:46756: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:34:32,021 [canton-env-ec-154] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 13:34:32,039 [canton-env-ec-59] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 13:34:32,046 [canton-env-ec-59] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 13:34:32,047 [canton-env-ec-166] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 13:34:32,048 [canton-env-ec-166] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 13:34:32,048 [canton-env-ec-166] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 13:34:32,052 [canton-env-ec-154] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 13:34:32,065 [canton-env-ec-154] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 13:34:32,066 [canton-env-ec-154] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 13:34:32,067 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 13:34:32,068 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 13:34:32,069 [canton-env-ec-106] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 13:34:32,069 [canton-env-ec-106] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 13:34:32,072 [canton-env-ec-158] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 13:34:32,078 [canton-env-ec-158] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 13:34:32,089 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:34:32,089 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:34:32,090 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:34:32,091 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 13:34:32,092 [canton-env-ec-163] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:34:32,093 [canton-env-ec-163] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:34:32,094 [canton-env-ec-163] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 13:34:32,096 [canton-env-ec-163] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 13:34:32,098 [canton-env-ec-163] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:34:32,100 [canton-env-ec-163] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:34:32,104 [canton-env-ec-163] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 13:34:32,109 [canton-env-ec-163] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:34:32,110 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:34:32,117 [canton-env-ec-163] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:34:32,118 [canton-env-ec-163] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:34:32,121 [canton-env-ec-106] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 13:34:32,123 [canton-env-ec-164] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:7ee15e41e97ac39beb60febccdfee93c - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:41968: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:34:32,133 [canton-env-ec-163] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:34:32,134 [canton-env-ec-163] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 13:34:32,135 [canton-env-ec-106] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 13:34:32,136 [canton-env-ec-106] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:c5fd07a540b0e15031defc5148aded5c - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:41984: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 13:34:32,147 [canton-env-ec-163] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 13:34:32,148 [canton-env-ec-163] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 13:34:32,149 [canton-env-ec-163] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:34:32,165 [canton-env-ec-163] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:34:32,166 [canton-env-ec-163] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 13:34:32,167 [canton-env-ec-163] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 13:34:32,174 [canton-env-ec-163] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 13:34:32,177 [canton-env-ec-163] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:34:32,179 [canton-env-ec-163] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 13:34:32,209 [canton-env-ec-163] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 13:34:32,237 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 13:34:32,238 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 13:34:53,739 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 13:34:54,848 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 13:34:54,868 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 13:34:55,121 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 13:34:55,211 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 13:34:55,274 [main] INFO  c.d.c.e.CommunityEnvironment tid:02c5010830c505c85ca242eb2ed39d65 - Automatically starting all instances
2025-08-24 13:34:55,336 [canton-env-ec-41] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 13:34:55,336 [canton-env-ec-36] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 13:34:55,361 [canton-env-ec-41] INFO  c.d.c.resource.DbStorage:mydomain tid:6b64b34e31a8809a3eb014d63f2c75cd - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:34:55,361 [canton-env-ec-36] INFO  c.d.c.r.DbStorage:participant1 tid:ab8770a443bb45f531bb89306c560dbd - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:34:55,392 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 13:34:55,393 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 13:34:55,954 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Start completed.
2025-08-24 13:34:55,954 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Start completed.
2025-08-24 13:34:56,105 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:34:56,105 [canton-env-ec-41] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:34:56,113 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:34:56,113 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:34:56,113 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:34:56,113 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:34:56,114 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:34:56,115 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:34:56,224 [canton-env-ec-41] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 13:34:56,224 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:34:56,235 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:34:56,235 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:34:56,298 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown initiated...
2025-08-24 13:34:56,299 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown initiated...
2025-08-24 13:34:56,305 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown completed.
2025-08-24 13:34:56,307 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown completed.
2025-08-24 13:34:56,367 [canton-env-ec-41] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 13:34:56,372 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 13:34:56,401 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 13:34:56,535 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 13:34:56,538 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 13:34:56,568 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 13:34:56,621 [canton-env-ec-66] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:56,621 [canton-env-ec-67] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:56,630 [canton-env-ec-41] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:34:56,630 [canton-env-ec-36] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:34:57,247 [canton-env-ec-41] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:34:57,248 [canton-env-ec-36] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:34:57,347 [canton-env-ec-58] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 13:34:57,347 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 13:34:57,624 [canton-env-ec-41] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 13:34:57,635 [canton-env-ec-35] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (10 ms)
2025-08-24 13:34:57,742 [canton-env-ec-67] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:d71e623f57b4b015448e531ab6625145 - Recovering published timely rejections
2025-08-24 13:34:57,763 [canton-env-ec-41] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:d71e623f57b4b015448e531ab6625145 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 13:34:57,766 [canton-env-ec-41] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:d71e623f57b4b015448e531ab6625145 - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 2)) (exclusive) up to None (inclusive)
2025-08-24 13:34:57,929 [canton-env-ec-37] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 13:34:57,999 [canton-env-ec-36] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 13:34:58,042 [canton-env-ec-66] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 13:34:58,044 [canton-env-ec-66] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:34:58,289 [canton-env-ec-41] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 13:34:58,381 [canton-env-ec-41] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 13:34:58,422 [canton-env-ec-41] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:34:58,425 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:34:58,534 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:34:58,587 [canton-env-ec-41] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.046s)
2025-08-24 13:34:58,620 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 13:34:58,621 [canton-env-ec-41] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 13:34:58,627 [canton-env-ec-41] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 13:34:58,670 [canton-env-ec-36] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 13:34:58,680 [canton-env-ec-58] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 13:34:58,684 [canton-env-ec-58] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 13:34:58,691 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 13:34:58,706 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:34:58,707 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 13:34:58,715 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 13:34:58,715 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 13:34:58,718 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 13:34:58,722 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 13:34:58,724 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:34:58,727 [canton-env-ec-66] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 13:34:58,756 [canton-env-ec-66] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 13:34:58,796 [canton-env-ec-66] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 13:34:58,796 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 13:34:58,802 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 13:34:58,828 [canton-env-ec-67] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000003)),0,0)
2025-08-24 13:34:58,833 [canton-env-ec-41] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 13:34:58,834 [canton-env-ec-66] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:58,892 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 13:34:58,903 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:58,940 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:59,149 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:59,404 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:59,509 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 5/3000. Retrying again in 100 milliseconds.
2025-08-24 13:34:59,513 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:59,560 [canton-env-ec-77] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (725 ms)
2025-08-24 13:34:59,582 [canton-env-ec-67] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (22 ms)
2025-08-24 13:34:59,585 [canton-env-ec-58] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000003)).
2025-08-24 13:34:59,590 [canton-env-ec-67] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:34:59.585550Z
2025-08-24 13:34:59,630 [canton-env-ec-77] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:34:59,651 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 13:34:59,670 [canton-env-ec-58] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 13:34:59,671 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 13:34:59,681 [canton-env-ec-67] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 13:34:59,681 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 13:34:59,682 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:34:59,728 [canton-env-ec-37] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 13:34:59,742 [canton-env-ec-58] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:34:59.739415Z
2025-08-24 13:34:59,750 [canton-env-ec-36] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 13:34:59,753 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:34:59,792 [canton-env-ec-58] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:34:59,792 [canton-env-ec-36] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:34:59,894 [canton-env-ec-58] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 13:34:59,895 [canton-env-ec-58] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 13:34:59,895 [canton-env-ec-58] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 13:35:00,041 [canton-env-ec-35] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 13:35:00,043 [canton-env-ec-41] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 13:35:00,113 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:7a2f1ebbf08cfe027498bf9f9c40160e - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:35:00,857 [canton-env-ec-135] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b66c18c236595a792484de6659a159c0 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 13:35:00,857 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:78ce2705289c410f913dfd38fa615f7c - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 13:35:00,856 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:af6a7cb25772c23f74693a961df5142d - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 13:35:00,856 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b2d371ab82da12b529cde27b487a32e0 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 13:35:00,859 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9bbea8df2346624251b1bcaec26f96b3 - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 13:35:00,857 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e153bf772ac6c6f012c3ba07290039f4 - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 13:35:00,859 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:4fdf5cf3c9663984001fd47801920a69 - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 13:35:00,857 [canton-env-ec-41] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:94ac42a117520f7cf933db6c1f1c1803 - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 13:35:00,860 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9114655a18a1fbd5dcf044cf16d61993 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 13:35:00,861 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:3aaa380ce482ff82c686eb20d98949cc - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 13:35:00,857 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:626403f874e814d44be9606e3c557b7a - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 13:35:00,857 [canton-env-ec-77] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:956f8e03cf210c8d5df8f76846b964da - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 13:35:00,857 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ee9dc458787aa0a0d407941c366bf6ff - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 13:35:00,881 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a251229c19649faa247a486aa2b5a10d - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 13:35:00,884 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:bf2697ab732c735812a73f9f219349cc - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 13:35:00,885 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:68815032776a10435f83114cb1a6999e - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 13:35:00,886 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:025510435c612612c94bdafe977554a4 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 13:35:00,887 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f4db085bdb3873c8227e6373b356494c - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 13:35:00,889 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e1c12cd78196324fb8786d1832151af0 - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 13:35:00,889 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:13d1aba3eceac798623e9e78208a6544 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 13:35:00,890 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9f28bc87c21abf1ca0942a113c328aaa - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 13:35:00,890 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7653afc4d06819bb0c6a9b21dfde3a2a - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 13:35:00,891 [canton-env-ec-77] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fa16cdfbd553c93db46aa0a8ea56407f - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 13:35:00,893 [canton-env-ec-41] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5f61e32bef904fd249a5db8e996d2f6b - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 13:35:00,892 [canton-env-ec-77] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:bb42dc89d0a9f5af5e62df1eb74bf138 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 13:35:00,891 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:50f66e0c9fcc8ceb8cdeeebf5f095615 - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 13:35:00,909 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fdc939cb81402ad5b29b41cbd0115b21 - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 13:35:00,911 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a7568a9577b704407ac114f50a306568 - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 13:35:00,914 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f30c7215d15f4110fe2bc3b1c76c4f42 - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 13:35:00,883 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d0c3b4a16fb3ae20fa46ac47af8989fe - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 13:35:00,881 [canton-env-ec-166] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:93f8322874db444ecdc0f6e5e3dc5293 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:35:00,908 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d51cd294910ff681ac5a7d2ff68b7f36 - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 13:35:00,987 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:f22ba554776236f4454f08aa832eef53 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:35:01,012 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:73386ac501c214ebdff7f55d5b06108a - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:35:01,085 [canton-env-ec-157] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:f6e35c0941477afec1b878950a74f8b7 - Received request for transactions, startExclusive -> '000000000000000003', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 13:35:01,179 [canton-env-ec-157] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:02c5010830c505c85ca242eb2ed39d65 - Reconnecting to domains List(). Already connected: Set()
2025-08-24 13:35:01,182 [canton-env-ec-166] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:02c5010830c505c85ca242eb2ed39d65 - Successfully re-connected to domains List()
2025-08-24 13:35:01,184 [main] INFO  c.d.c.e.CommunityEnvironment tid:02c5010830c505c85ca242eb2ed39d65 - Successfully started all nodes
2025-08-24 13:35:01,186 [main] INFO  c.digitalasset.canton.ServerRunner - Canton started
2025-08-24 13:40:58,806 [daml.index.db.threadpool.connection.indexer-4] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes()),2025-08-24T12:00:00Z)
2025-08-24 13:40:58,809 [daml.index.db.threadpool.connection.indexer-4] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes()),2025-08-24T12:00:00Z)
2025-08-24 13:46:58,798 [daml.index.db.threadpool.connection.indexer-6] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes()),2025-08-24T13:00:00Z)
2025-08-24 13:46:58,801 [daml.index.db.threadpool.connection.indexer-6] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes()),2025-08-24T13:00:00Z)
2025-08-24 13:47:19,661 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 13:47:20,762 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 13:47:20,791 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 13:47:20,995 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 13:47:21,091 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 13:47:21,167 [main] INFO  c.d.c.e.CommunityEnvironment tid:21eae2d3c7a10e4bc500ff40e4393f60 - Automatically starting all instances
2025-08-24 13:47:21,220 [canton-env-ec-36] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 13:47:21,220 [canton-env-ec-37] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 13:47:21,245 [canton-env-ec-37] INFO  c.d.c.resource.DbStorage:mydomain tid:cf42a2fc7e8ebc7112a931ed9f117a33 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:47:21,245 [canton-env-ec-36] INFO  c.d.c.r.DbStorage:participant1 tid:e256c77a54c4512292b61e22162eac53 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:47:21,273 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 13:47:21,273 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 13:47:21,800 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 13:47:21,800 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 13:47:21,962 [canton-env-ec-37] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:47:21,963 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:47:21,973 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:47:21,974 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:47:21,974 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:47:21,973 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:47:21,974 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:47:21,975 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:47:22,091 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:47:22,091 [canton-env-ec-37] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 13:47:22,101 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:47:22,101 [canton-env-ec-37] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:47:22,166 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 13:47:22,167 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 13:47:22,173 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 13:47:22,174 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 13:47:22,208 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 13:47:22,211 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 13:47:22,242 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 13:47:22,338 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 13:47:22,340 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 13:47:22,370 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 13:47:22,454 [canton-env-ec-66] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:47:22,454 [canton-env-ec-61] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:47:22,462 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:47:22,463 [canton-env-ec-36] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:47:22,909 [main] ERROR c.d.c.e.CommunityEnvironment tid:21eae2d3c7a10e4bc500ff40e4393f60 - Failed to start mydomain: Failed to bind to address /0.0.0.0:5019
2025-08-24 13:47:22,912 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 13:47:22,917 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 13:47:22,967 [canton-env-ec-38] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 13:47:22,995 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 13:47:22,996 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 13:51:53,175 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 13:51:54,319 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 13:51:54,340 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 13:51:54,553 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 13:51:54,625 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 13:51:54,686 [main] INFO  c.d.c.e.CommunityEnvironment tid:2a7a4ec5c6f48f52cb7058142d696609 - Automatically starting all instances
2025-08-24 13:51:54,738 [canton-env-ec-36] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 13:51:54,738 [canton-env-ec-37] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 13:51:54,760 [canton-env-ec-37] INFO  c.d.c.r.DbStorage:participant1 tid:d4cba0462280b6675fe9328940a7ba5b - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:51:54,760 [canton-env-ec-36] INFO  c.d.c.resource.DbStorage:mydomain tid:108c6843a43479641e4a77cf535fc980 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 13:51:54,788 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 13:51:54,788 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 13:51:55,269 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 13:51:55,269 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 13:51:55,420 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:51:55,420 [canton-env-ec-37] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 13:51:55,429 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:51:55,429 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:51:55,429 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 13:51:55,429 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 13:51:55,429 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:51:55,430 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 13:51:55,533 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 13:51:55,533 [canton-env-ec-37] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:51:55,543 [canton-env-ec-37] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:51:55,543 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:51:55,605 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 13:51:55,605 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 13:51:55,610 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 13:51:55,614 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 13:51:55,669 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 13:51:55,672 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 13:51:55,697 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 13:51:55,822 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 13:51:55,824 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 13:51:55,847 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 13:51:55,913 [canton-env-ec-35] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:51:55,913 [canton-env-ec-65] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:51:55,920 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:51:55,920 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 13:51:56,496 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:51:56,498 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:51:56,611 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 13:51:56,611 [canton-env-ec-67] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 13:51:56,860 [canton-env-ec-35] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 13:51:56,869 [canton-env-ec-35] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (8 ms)
2025-08-24 13:51:56,935 [canton-env-ec-37] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:6d271a4a5edb49c134ff2f519e33aa75 - Recovering published timely rejections
2025-08-24 13:51:56,964 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:6d271a4a5edb49c134ff2f519e33aa75 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 13:51:56,971 [canton-env-ec-36] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:6d271a4a5edb49c134ff2f519e33aa75 - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 2)) (exclusive) up to None (inclusive)
2025-08-24 13:51:57,135 [canton-env-ec-90] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 13:51:57,354 [canton-env-ec-39] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 13:51:57,405 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 13:51:57,409 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 13:51:57,508 [canton-env-ec-36] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 13:51:57,605 [canton-env-ec-38] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 13:51:57,654 [canton-env-ec-38] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 13:51:57,658 [canton-env-ec-38] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:51:57,781 [canton-env-ec-38] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 13:51:57,816 [canton-env-ec-38] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.030s)
2025-08-24 13:51:57,850 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 13:51:57,850 [canton-env-ec-38] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 13:51:57,855 [canton-env-ec-38] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 13:51:57,900 [canton-env-ec-37] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 13:51:57,906 [canton-env-ec-67] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 13:51:57,909 [canton-env-ec-67] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 13:51:57,916 [canton-env-ec-67] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 13:51:57,930 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:51:57,930 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 13:51:57,937 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 13:51:57,937 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 13:51:57,940 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 13:51:57,941 [canton-env-ec-67] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 13:51:57,945 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 13:51:57,947 [canton-env-ec-66] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 13:51:57,977 [canton-env-ec-66] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 13:51:58,009 [canton-env-ec-66] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 13:51:58,017 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 13:51:58,039 [canton-env-ec-39] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 13:51:58,045 [canton-env-ec-36] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 13:51:58,048 [canton-env-ec-66] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000003)),0,0)
2025-08-24 13:51:58,148 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 13:51:58,319 [canton-env-ec-65] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 13:51:58,558 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 13:51:58,652 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 13:51:58,667 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 13:51:58,691 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:51:58,767 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 5/3000. Retrying again in 100 milliseconds.
2025-08-24 13:51:58,776 [canton-env-ec-37] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (724 ms)
2025-08-24 13:51:58,797 [canton-env-ec-36] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (20 ms)
2025-08-24 13:51:58,801 [canton-env-ec-153] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000003)).
2025-08-24 13:51:58,824 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:51:58,892 [canton-env-ec-153] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 13:51:58,893 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 13:51:58,920 [canton-env-ec-67] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 13:51:58,947 [canton-env-ec-36] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:51:58.941113Z
2025-08-24 13:51:58,947 [canton-env-ec-39] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 13:51:58,969 [canton-env-ec-90] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 13:51:58,982 [canton-env-ec-157] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 13:51:58,984 [canton-env-ec-157] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 13:51:58,995 [canton-env-ec-35] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:51:59,002 [canton-env-ec-65] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 13:51:59,051 [canton-env-ec-36] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T14:51:59.048404Z
2025-08-24 13:51:59,091 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 13:51:59,166 [canton-env-ec-67] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 13:51:59,167 [canton-env-ec-67] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 13:51:59,167 [canton-env-ec-67] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 13:51:59,171 [canton-env-ec-66] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:51:59,171 [canton-env-ec-67] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 13:51:59,301 [canton-env-ec-35] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 13:51:59,304 [canton-env-ec-38] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 13:51:59,363 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:d99b129aec9c7f124f3651a4944e5d66 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:52:00,206 [canton-env-ec-90] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1a18740bdd619e83d3df770e8cba2737 - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 13:52:00,207 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:44d845cbd44825e8ca55fb901848f4bf - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 13:52:00,208 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:12412baad18e0400c0e42607ae609ce4 - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 13:52:00,206 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:57c113a95f1657ccfcd2b474b49bba11 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 13:52:00,210 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fa9adfd934f479ddfb6abb1e9b1ae326 - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 13:52:00,213 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:74939cf507eef1daf2a5279d2fb2c02a - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 13:52:00,213 [canton-env-ec-90] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:712b74b710b4a0e66fd190981ffc7544 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:52:00,213 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c39036c21720b4656160c982919c1c6c - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 13:52:00,215 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b4c91f7af031ba67e4d26a51eaedc29a - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 13:52:00,216 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:451e03de0cdf127bc6d7f2dfa8cd23d7 - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 13:52:00,216 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d52a58f96757cc4d1bdf2812cf7b2be5 - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 13:52:00,217 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f2e058a1c8b6eea4fa3c1de1f57c9dbc - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 13:52:00,218 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:22c02e54966cda8d9bf1617abc996e71 - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 13:52:00,218 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9a3f27f377866977c38bc872e11ee9a9 - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 13:52:00,219 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:512e6b23f8ae2ec412ff06a753c3be39 - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 13:52:00,219 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5d9682942275d49cb5e7c0b8d531c16f - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 13:52:00,219 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6ff98e88e444cdcd017699ba46ba3777 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 13:52:00,220 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:69a4e2f98acbe5a79991de9f366e6e00 - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 13:52:00,220 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6aac4e0f50eed207026bc141dc1dfa83 - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 13:52:00,218 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8318be0e64542b527173df93bb0900ee - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 13:52:00,221 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:673009f1ef3875f9e5b5a369c2c74170 - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 13:52:00,221 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a424dc3b0bdc9d0d6b0e397854951298 - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 13:52:00,217 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:56a8a81049666db564d3a10e1666d6c8 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 13:52:00,222 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6b24d2cf999820adb4a947416e59e703 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 13:52:00,222 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e2207cf9af9e9e43680bb7c4ca92d014 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 13:52:00,223 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7e3713cdcf1ef1ef69865402aa9432b1 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 13:52:00,223 [canton-env-ec-90] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:3d293ca719d3ca458ff6910c18e4e2fb - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 13:52:00,222 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b51b4dba70a05984b9d5d67202246ba6 - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 13:52:00,224 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7b16d60748edd44033964ae82a78f646 - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 13:52:00,225 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:dee0376a3b3ef26b2897c4bd7d52646a - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 13:52:00,227 [canton-env-ec-39] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b31c97b6f01e98ab57a628f8e7d4217e - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 13:52:00,236 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:62dd0de1e1e7614932bcbc503f7a41d6 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 13:52:00,344 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:045f397f84a514b529f70a9f0570bd3e - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 13:52:00,363 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:04a84706baa7339c4a5eac383e1a55c7 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 13:52:00,460 [canton-env-ec-153] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:3105e96b575e58a9043295bc03b5e9d1 - Received request for transactions, startExclusive -> '000000000000000003', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 13:52:00,587 [canton-env-ec-167] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:2a7a4ec5c6f48f52cb7058142d696609 - Reconnecting to domains List(). Already connected: Set()
2025-08-24 13:52:00,590 [canton-env-ec-167] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:2a7a4ec5c6f48f52cb7058142d696609 - Successfully re-connected to domains List()
2025-08-24 13:52:00,593 [main] INFO  c.d.c.e.CommunityEnvironment tid:2a7a4ec5c6f48f52cb7058142d696609 - Successfully started all nodes
2025-08-24 13:52:00,596 [main] INFO  c.digitalasset.canton.ServerRunner - Canton started
2025-08-24 14:01:30,820 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 14:01:30,826 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 14:01:30,837 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:01:30,839 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:01:30,840 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:01:30,842 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:01:30,853 [canton-env-ec-39] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 14:01:30,864 [canton-env-ec-167] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:3105e96b575e58a9043295bc03b5e9d1 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:45684: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:01:30,879 [canton-env-ec-167] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 14:01:30,899 [canton-env-ec-163] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 14:01:30,902 [canton-env-ec-163] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 14:01:30,903 [canton-env-ec-163] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 14:01:30,904 [canton-env-ec-163] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 14:01:30,905 [canton-env-ec-163] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 14:01:30,908 [canton-env-ec-162] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 14:01:30,915 [canton-env-ec-162] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 14:01:30,916 [canton-env-ec-162] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 14:01:30,918 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 14:01:30,920 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 14:01:30,920 [canton-env-ec-161] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 14:01:30,921 [canton-env-ec-161] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 14:01:30,923 [canton-env-ec-35] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 14:01:30,926 [canton-env-ec-35] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 14:01:30,933 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:01:30,933 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:01:30,934 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:01:30,934 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:01:30,935 [canton-env-ec-39] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:01:30,935 [canton-env-ec-39] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:01:30,937 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 14:01:30,940 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 14:01:30,941 [canton-env-ec-39] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:01:30,941 [canton-env-ec-39] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:01:30,945 [canton-env-ec-39] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 14:01:30,950 [canton-env-ec-39] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:01:30,951 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:01:30,958 [canton-env-ec-39] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:01:30,959 [canton-env-ec-39] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:01:30,963 [canton-env-ec-163] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:01:30,964 [canton-env-ec-167] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:bd48ad7de928da1e8a96f318c8bedaf5 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:51936: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:01:30,966 [canton-env-ec-37] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@2d9a5524 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 14:01:30,971 [canton-env-ec-37] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@6ab30bb5 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 14:01:30,972 [canton-env-ec-37] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@157f50b2 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 14:01:30,973 [canton-env-ec-37] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@578504af (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 14:01:30,974 [canton-env-ec-37] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@2e316728 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 14:01:30,975 [canton-env-ec-37] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@5cad3ff8 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 14:01:30,976 [canton-env-ec-37] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@33b81e1f (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 14:01:30,977 [canton-env-ec-37] INFO  com.zaxxer.hikari.pool.PoolBase - slick-mydomain-3 - Failed to validate connection org.postgresql.jdbc.PgConnection@4ce7c4bb (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
2025-08-24 14:01:30,977 [canton-env-ec-39] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:01:30,978 [canton-env-ec-39] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:01:30,979 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:01:30,980 [canton-env-ec-162] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:978b2ccff779af73ec45921fde23fad7 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:51954: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:01:30,989 [canton-env-ec-39] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:01:30,990 [canton-env-ec-39] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:01:30,991 [canton-env-ec-39] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:01:31,009 [canton-env-ec-39] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:01:31,009 [canton-env-ec-39] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:01:31,011 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 14:01:31,016 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 14:01:31,017 [canton-env-ec-39] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:01:31,019 [canton-env-ec-39] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:01:31,042 [canton-env-ec-37] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 14:01:31,077 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 14:01:31,077 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 14:18:57,008 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 14:18:58,122 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 14:18:58,143 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 14:18:58,347 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 14:18:58,441 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 14:18:58,504 [main] INFO  c.d.c.e.CommunityEnvironment tid:da624e4d7825b05b45b41175aaf11122 - Automatically starting all instances
2025-08-24 14:18:58,562 [canton-env-ec-36] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 14:18:58,562 [canton-env-ec-35] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 14:18:58,584 [canton-env-ec-36] INFO  c.d.c.r.DbStorage:participant1 tid:3ca8d29f24c8fb422222229e94504048 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:18:58,584 [canton-env-ec-35] INFO  c.d.c.resource.DbStorage:mydomain tid:547754df78a64ecce1a9a7f24554d908 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:18:58,611 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 14:18:58,611 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 14:18:59,097 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 14:18:59,097 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 14:18:59,255 [canton-env-ec-35] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:18:59,255 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:18:59,264 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:18:59,265 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:18:59,265 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:18:59,264 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:18:59,265 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:18:59,266 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:18:59,417 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 14:18:59,418 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:18:59,426 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:18:59,428 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:18:59,479 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 14:18:59,479 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 14:18:59,494 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 14:18:59,497 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 14:18:59,531 [canton-env-ec-35] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 14:18:59,534 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 14:18:59,567 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 14:18:59,663 [canton-env-ec-38] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 14:18:59,666 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 14:18:59,696 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 14:18:59,774 [canton-env-ec-66] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:18:59,774 [canton-env-ec-67] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:18:59,785 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:18:59,785 [canton-env-ec-38] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:19:00,370 [canton-env-ec-38] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:19:00,371 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:19:00,473 [canton-env-ec-35] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 14:19:00,473 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 14:19:00,752 [canton-env-ec-35] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 14:19:00,760 [canton-env-ec-66] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (7 ms)
2025-08-24 14:19:00,858 [canton-env-ec-35] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:699a983fa466e65004d4f27b26aeb60d - Recovering published timely rejections
2025-08-24 14:19:00,878 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:699a983fa466e65004d4f27b26aeb60d - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 14:19:00,883 [canton-env-ec-36] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:699a983fa466e65004d4f27b26aeb60d - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 2)) (exclusive) up to None (inclusive)
2025-08-24 14:19:01,010 [canton-env-ec-36] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 14:19:01,084 [canton-env-ec-58] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 14:19:01,130 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 14:19:01,133 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:19:01,381 [canton-env-ec-58] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 14:19:01,476 [canton-env-ec-35] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 14:19:01,545 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:19:01,548 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:19:01,681 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:19:01,735 [canton-env-ec-35] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.047s)
2025-08-24 14:19:01,776 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 14:19:01,777 [canton-env-ec-35] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 14:19:01,780 [canton-env-ec-35] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 14:19:01,824 [canton-env-ec-65] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 14:19:01,834 [canton-env-ec-35] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 14:19:01,838 [canton-env-ec-35] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 14:19:01,845 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 14:19:01,859 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:19:01,859 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 14:19:01,866 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 14:19:01,866 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 14:19:01,869 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 14:19:01,877 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:19:01,877 [canton-env-ec-65] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 14:19:01,882 [canton-env-ec-65] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 14:19:01,882 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 14:19:01,913 [canton-env-ec-65] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 14:19:01,948 [canton-env-ec-67] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 14:19:01,953 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 14:19:01,979 [canton-env-ec-65] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 14:19:01,980 [canton-env-ec-106] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 14:19:01,981 [canton-env-ec-66] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000003)),0,0)
2025-08-24 14:19:01,982 [canton-env-ec-106] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 14:19:01,989 [canton-env-ec-65] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:19:02,084 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 14:19:02,326 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 14:19:02,517 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 14:19:02,563 [canton-env-ec-139] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:19:02,600 [canton-env-ec-66] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:19:02.563417Z
2025-08-24 14:19:02,658 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 5/3000. Retrying again in 100 milliseconds.
2025-08-24 14:19:02,686 [canton-env-ec-139] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 14:19:02,710 [canton-env-ec-154] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (724 ms)
2025-08-24 14:19:02,742 [canton-env-ec-106] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (29 ms)
2025-08-24 14:19:02,744 [canton-env-ec-154] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000003)).
2025-08-24 14:19:02,778 [canton-env-ec-38] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 14:19:02,809 [canton-env-ec-58] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 14:19:02,810 [canton-env-ec-58] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:19:02,823 [canton-env-ec-35] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 14:19:02,832 [canton-env-ec-154] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 14:19:02,833 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 14:19:02,877 [canton-env-ec-106] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 14:19:02,879 [canton-env-ec-35] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 14:19:02,892 [canton-env-ec-66] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:19:02.886155Z
2025-08-24 14:19:02,905 [canton-env-ec-139] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 14:19:02,905 [canton-env-ec-65] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 14:19:02,943 [canton-env-ec-58] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 14:19:03,046 [canton-env-ec-38] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 14:19:03,046 [canton-env-ec-38] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 14:19:03,047 [canton-env-ec-38] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 14:19:03,186 [canton-env-ec-65] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 14:19:03,188 [canton-env-ec-58] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 14:19:03,257 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:65ecb292b9c72c4a16e59755ab5603a8 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:19:04,041 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ae3682b66f5a6afe2ea030481feb44c0 - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 14:19:04,047 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8a207dd519f0680a565d30facd367dda - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 14:19:04,048 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e0713f3681a85eeb26ac5cffa4148d03 - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 14:19:04,042 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6ce25d32b215da109832b74d9da8ca1a - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 14:19:04,041 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c1411b4ebf458e2da761789b89911253 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 14:19:04,052 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:42470cb781c5a83c8e5608a1473506cc - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 14:19:04,053 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:62abb4a16862874717effc2e0f603b19 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 14:19:04,054 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2a5416d720b5f6e129a06626c938d686 - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 14:19:04,055 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1927991c203e27c7c422aab5a9717791 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 14:19:04,056 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b095bff67d576ee6abec464c578ee9a8 - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 14:19:04,050 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fea58136b32338a4d0c8f6f08603e251 - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 14:19:04,064 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:dd891bb58a1763b27bffa44c0889a66e - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 14:19:04,050 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a97d4ec88bafc616336476b7d58de891 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 14:19:04,069 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:10c9589b56f07d9ebfee2791c0ab01d3 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 14:19:04,049 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:27ad2700eb298d484cf3a4a6556598ba - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 14:19:04,072 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1640697cd3f018dd2fa727b8e905f4f0 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 14:19:04,073 [canton-env-ec-166] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b997865b0aeb2abb23be7bbe7c4b0431 - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 14:19:04,046 [canton-env-ec-139] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:cfc96c5321228e38faebce6d5a7dacce - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 14:19:04,079 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a8417d52ae3d898c4fd606fd7f12adb1 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 14:19:04,081 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5925961980a1bb7b0fd7c8a92fb3d8f7 - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 14:19:04,045 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0f91bedbc44e9370dcbb857ca35d3dc4 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 14:19:04,090 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1b1ad7e5a344bd111a787bdd44f582f1 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 14:19:04,045 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b873d09f801958f95c87417de7e21a33 - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 14:19:04,094 [canton-env-ec-106] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b02d6ae7d64159d872f4d3aacb3b0048 - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 14:19:04,044 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d9de49d547ec9d1253712894c6d40400 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:19:04,044 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a999b90b42027e2ffd9a8748b3d44f3d - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 14:19:04,044 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:50f19c7b7eb5c039c5afa61bbe796735 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 14:19:04,072 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:66c957843add94a851b991d19ccbe063 - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 14:19:04,070 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e787fe171654015a0027d94a843f3a7b - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 14:19:04,064 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:4c143d9b887286531d5748d92446faef - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 14:19:04,112 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1b8201d60b5d947fc7d216231d232e3d - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 14:19:04,111 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f4bb0324da6fee3c79d8788b92f43293 - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 14:19:04,203 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:8033ad84d00e4151f1d11293177cf28d - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:19:04,232 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ad9af901df502e79770f93eaeee35729 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:19:04,331 [canton-env-ec-159] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:039616322d86177ee65af408b8630cb3 - Received request for transactions, startExclusive -> '000000000000000003', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 14:19:04,441 [canton-env-ec-159] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:da624e4d7825b05b45b41175aaf11122 - Reconnecting to domains List(). Already connected: Set()
2025-08-24 14:19:04,445 [canton-env-ec-65] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:da624e4d7825b05b45b41175aaf11122 - Successfully re-connected to domains List()
2025-08-24 14:19:04,447 [main] INFO  c.d.c.e.CommunityEnvironment tid:da624e4d7825b05b45b41175aaf11122 - Successfully started all nodes
2025-08-24 14:19:05,717 [main] INFO  c.d.canton.ConsoleInteractiveRunner - Running startup script CantonScriptFromFile(setup-production-network.canton)
2025-08-24 14:19:08,440 [main] ERROR c.d.canton.ConsoleInteractiveRunner - Running bootstrap script failed with an exception (Compilation Failed)!
2025-08-24 14:19:08,442 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 14:19:08,445 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 14:19:08,452 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:19:08,453 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:19:08,454 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:19:08,455 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:19:08,466 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 14:19:08,483 [canton-env-ec-65] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:039616322d86177ee65af408b8630cb3 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:55986: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:19:08,486 [canton-env-ec-160] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 14:19:08,502 [canton-env-ec-167] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 14:19:08,511 [canton-env-ec-167] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 14:19:08,511 [canton-env-ec-36] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 14:19:08,512 [canton-env-ec-36] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 14:19:08,513 [canton-env-ec-36] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 14:19:08,516 [canton-env-ec-65] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 14:19:08,537 [canton-env-ec-65] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 14:19:08,538 [canton-env-ec-65] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 14:19:08,539 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 14:19:08,544 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 14:19:08,545 [canton-env-ec-163] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 14:19:08,545 [canton-env-ec-163] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 14:19:08,549 [canton-env-ec-67] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 14:19:08,554 [canton-env-ec-67] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 14:19:08,568 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:19:08,569 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:19:08,570 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:19:08,571 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:19:08,572 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:19:08,573 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:19:08,575 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 14:19:08,579 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 14:19:08,581 [canton-env-ec-37] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:19:08,582 [canton-env-ec-37] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:19:08,586 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 14:19:08,592 [canton-env-ec-37] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:19:08,592 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:19:08,600 [canton-env-ec-37] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:19:08,601 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:19:08,606 [canton-env-ec-36] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:19:08,607 [canton-env-ec-159] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:492833a20d533db8e520965b2e8ee906 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:38250: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:19:08,627 [canton-env-ec-37] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:19:08,628 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:19:08,629 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:19:08,630 [canton-env-ec-163] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:1c90eed1828b16e6b134fb1245e492f4 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:38272: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:19:08,639 [canton-env-ec-37] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:19:08,640 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:19:08,640 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:19:08,653 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:19:08,654 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:19:08,655 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 14:19:08,668 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 14:19:08,670 [canton-env-ec-37] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:19:08,671 [canton-env-ec-37] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:19:08,702 [canton-env-ec-160] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 14:19:08,728 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 14:19:08,728 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 14:38:42,889 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 14:38:43,971 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 14:38:43,994 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 14:38:44,220 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 14:38:44,301 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 14:38:44,362 [main] INFO  c.d.c.e.CommunityEnvironment tid:16a99f60ee567f02839bbeb835b364be - Automatically starting all instances
2025-08-24 14:38:44,426 [canton-env-ec-37] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 14:38:44,426 [canton-env-ec-35] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 14:38:44,454 [canton-env-ec-35] INFO  c.d.c.resource.DbStorage:mydomain tid:32977ab48eb8a5011a0de196e36142fc - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:38:44,456 [canton-env-ec-37] INFO  c.d.c.r.DbStorage:participant1 tid:7c037dd2076bdc2322acb3b94d75f32c - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:38:44,487 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 14:38:44,488 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 14:38:45,026 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 14:38:45,026 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 14:38:45,197 [canton-env-ec-35] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:38:45,197 [canton-env-ec-37] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:38:45,214 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:38:45,214 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:38:45,214 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:38:45,214 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:38:45,215 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:38:45,215 [canton-env-ec-35] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:38:45,325 [canton-env-ec-35] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 14:38:45,325 [canton-env-ec-37] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:38:45,334 [canton-env-ec-35] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:38:45,334 [canton-env-ec-37] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:38:45,384 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 14:38:45,384 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 14:38:45,391 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 14:38:45,394 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 14:38:45,453 [canton-env-ec-35] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 14:38:45,458 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 14:38:45,498 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 14:38:45,574 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 14:38:45,576 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 14:38:45,610 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 14:38:45,712 [canton-env-ec-66] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:38:45,712 [canton-env-ec-36] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:38:45,721 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:38:45,721 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:38:46,280 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:38:46,282 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:38:46,409 [canton-env-ec-66] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 14:38:46,409 [canton-env-ec-35] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 14:38:46,703 [canton-env-ec-63] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 14:38:46,710 [canton-env-ec-66] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (6 ms)
2025-08-24 14:38:46,803 [canton-env-ec-38] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:e6f80c0b7731d50c3d09df6d5d7c15b4 - Recovering published timely rejections
2025-08-24 14:38:46,838 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:e6f80c0b7731d50c3d09df6d5d7c15b4 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 14:38:46,843 [canton-env-ec-36] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:e6f80c0b7731d50c3d09df6d5d7c15b4 - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 2)) (exclusive) up to None (inclusive)
2025-08-24 14:38:46,991 [canton-env-ec-38] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 14:38:47,159 [canton-env-ec-65] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 14:38:47,224 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 14:38:47,228 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:38:47,395 [canton-env-ec-66] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 14:38:47,476 [canton-env-ec-37] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 14:38:47,524 [canton-env-ec-37] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:38:47,529 [canton-env-ec-37] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:38:47,660 [canton-env-ec-37] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:38:47,698 [canton-env-ec-37] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.033s)
2025-08-24 14:38:47,728 [canton-env-ec-37] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 14:38:47,728 [canton-env-ec-37] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 14:38:47,733 [canton-env-ec-37] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 14:38:47,783 [canton-env-ec-36] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 14:38:47,791 [canton-env-ec-66] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 14:38:47,794 [canton-env-ec-66] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 14:38:47,804 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 14:38:47,827 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:38:47,828 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 14:38:47,839 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 14:38:47,840 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 14:38:47,841 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 14:38:47,844 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 14:38:47,851 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:38:47,855 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 14:38:47,888 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 14:38:47,924 [canton-env-ec-63] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 14:38:47,925 [canton-env-ec-65] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 14:38:47,932 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 14:38:47,936 [canton-env-ec-64] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 14:38:47,937 [canton-env-ec-64] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 14:38:47,969 [canton-env-ec-35] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000003)),0,0)
2025-08-24 14:38:48,016 [canton-env-ec-120] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 14:38:48,027 [canton-env-ec-120] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:38:48,041 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 14:38:48,142 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 14:38:48,263 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 14:38:48,565 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 5/3000. Retrying again in 100 milliseconds.
2025-08-24 14:38:48,646 [canton-env-ec-66] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:38:48,713 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 6/3000. Retrying again in 100 milliseconds.
2025-08-24 14:38:48,715 [canton-env-ec-38] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (741 ms)
2025-08-24 14:38:48,734 [canton-env-ec-36] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (19 ms)
2025-08-24 14:38:48,736 [canton-env-ec-144] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000003)).
2025-08-24 14:38:48,750 [canton-env-ec-36] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:38:48.746203Z
2025-08-24 14:38:48,780 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 14:38:48,786 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 14:38:48,793 [canton-env-ec-144] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 14:38:48,794 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 14:38:48,808 [canton-env-ec-38] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 14:38:48,809 [canton-env-ec-38] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:38:48,865 [canton-env-ec-35] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:38:48.862248Z
2025-08-24 14:38:48,874 [canton-env-ec-66] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 14:38:48,877 [canton-env-ec-120] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 14:38:48,923 [canton-env-ec-36] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 14:38:48,938 [canton-env-ec-64] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 14:38:48,938 [canton-env-ec-36] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 14:38:48,957 [canton-env-ec-120] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 14:38:49,096 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 14:38:49,096 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 14:38:49,097 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 14:38:49,224 [canton-env-ec-37] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 14:38:49,226 [canton-env-ec-63] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 14:38:49,299 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:acbe6ec666125fc10a4a396568bef05e - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:38:50,035 [canton-env-ec-144] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c0aa91a8de5fd93a4ef1019c4196a888 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 14:38:50,037 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8a77b564b152741c9e53f85974a62848 - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 14:38:50,039 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e713222ca60be32c591207040485b3aa - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 14:38:50,047 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e6bc0c0dfd08b4ce86c8f1278efa10fe - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 14:38:50,048 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:14ab875b1b3cbbe8b6b5df49b3def8d4 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 14:38:50,049 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:195d0737b79c5501607a0cd7f53df782 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:38:50,049 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a5db8631a724713b12ea53b167ae3e47 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 14:38:50,050 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:df6d0b3457a017165f44a8c3016a25aa - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 14:38:50,053 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7d97f126a41faa3483aed1f68c16d415 - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 14:38:50,059 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:08c5c4b7e94c67fa0d6b7cff81fbc520 - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 14:38:50,056 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:078813f93bcafadea4f811800144307d - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 14:38:50,061 [canton-env-ec-144] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8df370edfbebbf3886b63d38477abc6a - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 14:38:50,056 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:992322fc1fbd2d17b1021a0f9a2642f1 - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 14:38:50,066 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:4786d20052db2a5bcd672802456a388c - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 14:38:50,049 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:dde7a60cc35cd50b036e057ce0e5aa12 - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 14:38:50,067 [canton-env-ec-144] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d11a1c0d21a3d525a1c691fd8f942127 - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 14:38:50,068 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f01dd41df019411f60e99ee5e92e7983 - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 14:38:50,070 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:56676ae165b259b53b8dab3a3a0c87a3 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 14:38:50,071 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:dd84ae58615fe897a749157d437d918b - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 14:38:50,072 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:10bd5638f40465905bd8efdc835d5517 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 14:38:50,073 [canton-env-ec-120] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:70461fb625974313cffbebcff18d6fff - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 14:38:50,047 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:18098b78f214205974f95a4cfd21244c - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 14:38:50,084 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:833bd784194c9754b242faea32ccade8 - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 14:38:50,088 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e1fe944b1d51c70266fcebd8b8d6b286 - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 14:38:50,072 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1fdf0455c1e59be7eaeb2b7e321d3418 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 14:38:50,101 [canton-env-ec-144] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d249a6679b6f23e70fc8dd077a2cf33d - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 14:38:50,064 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:74da746f9299da505691f8cd894d701c - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 14:38:50,062 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7eb83c4625829c3684a0cac314273f40 - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 14:38:50,106 [canton-env-ec-63] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e35ebd368427d8431c8a7c1e82414e0a - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 14:38:50,060 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1b4a7a47696ac020df49032c1634ae96 - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 14:38:50,106 [canton-env-ec-162] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:50aedfa5a8a48850f89e10b952b93b51 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 14:38:50,108 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d56bb0cf4784b1bb5a7790544c6b95a7 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 14:38:50,193 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:4f12fe1ce93f5a0837153c891b39e3af - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:38:50,214 [canton-env-ec-144] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:63b5807fb2d86afa395c4e8fa8a5df1d - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:38:50,288 [canton-env-ec-144] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:e6a0256ec826635372372255e2dca56a - Received request for transactions, startExclusive -> '000000000000000003', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 14:38:50,370 [canton-env-ec-64] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:16a99f60ee567f02839bbeb835b364be - Reconnecting to domains List(). Already connected: Set()
2025-08-24 14:38:50,374 [canton-env-ec-64] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:16a99f60ee567f02839bbeb835b364be - Successfully re-connected to domains List()
2025-08-24 14:38:50,376 [main] INFO  c.d.c.e.CommunityEnvironment tid:16a99f60ee567f02839bbeb835b364be - Successfully started all nodes
2025-08-24 14:38:51,566 [main] INFO  c.d.canton.ConsoleInteractiveRunner - Running startup script CantonScriptFromFile(setup-production-network.canton)
2025-08-24 14:38:54,148 [main] ERROR c.d.canton.ConsoleInteractiveRunner - Running bootstrap script failed with an exception (Compilation Failed)!
2025-08-24 14:38:54,151 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 14:38:54,155 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 14:38:54,165 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:38:54,167 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:38:54,169 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:38:54,170 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:38:54,182 [canton-env-ec-144] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 14:38:54,200 [canton-env-ec-158] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:e6a0256ec826635372372255e2dca56a - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:42030: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:38:54,215 [canton-env-ec-159] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 14:38:54,233 [canton-env-ec-158] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 14:38:54,244 [canton-env-ec-158] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 14:38:54,245 [canton-env-ec-159] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 14:38:54,246 [canton-env-ec-159] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 14:38:54,247 [canton-env-ec-159] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 14:38:54,251 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 14:38:54,276 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 14:38:54,277 [canton-env-ec-36] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 14:38:54,278 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 14:38:54,279 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 14:38:54,280 [canton-env-ec-158] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 14:38:54,280 [canton-env-ec-158] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 14:38:54,283 [canton-env-ec-35] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 14:38:54,287 [canton-env-ec-35] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 14:38:54,297 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:38:54,298 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:38:54,299 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:38:54,300 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Not Initialized.
2025-08-24 14:38:54,300 [canton-env-ec-144] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:38:54,301 [canton-env-ec-144] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:38:54,302 [canton-env-ec-144] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 14:38:54,315 [canton-env-ec-144] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 14:38:54,317 [canton-env-ec-144] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:38:54,318 [canton-env-ec-144] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:38:54,324 [canton-env-ec-144] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 14:38:54,329 [canton-env-ec-144] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:38:54,330 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:38:54,335 [canton-env-ec-144] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:38:54,336 [canton-env-ec-144] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:38:54,339 [canton-env-ec-36] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:38:54,339 [canton-env-ec-38] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:22026b5a9db84a69cc30b0352d6afa2b - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:35376: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:38:54,352 [canton-env-ec-144] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:38:54,352 [canton-env-ec-144] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:38:54,354 [canton-env-ec-38] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:38:54,355 [canton-env-ec-36] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:301c211fa9576d77c78e18ba45f5b207 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:35386: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:38:54,362 [canton-env-ec-144] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:38:54,363 [canton-env-ec-144] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:38:54,364 [canton-env-ec-144] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:38:54,377 [canton-env-ec-144] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:38:54,378 [canton-env-ec-144] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:38:54,380 [canton-env-ec-144] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 14:38:54,397 [canton-env-ec-144] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 14:38:54,399 [canton-env-ec-144] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:38:54,400 [canton-env-ec-144] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:38:54,428 [canton-env-ec-144] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 14:38:54,458 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 14:38:54,458 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 14:39:09,925 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 14:39:11,084 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 14:39:11,106 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 14:39:11,328 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 14:39:11,412 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 14:39:11,491 [main] INFO  c.d.c.e.CommunityEnvironment tid:3cd936998b4a67758c78a41205946849 - Automatically starting all instances
2025-08-24 14:39:11,546 [canton-env-ec-37] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 14:39:11,546 [canton-env-ec-41] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 14:39:11,571 [canton-env-ec-37] INFO  c.d.c.r.DbStorage:participant1 tid:e3c446ba2a883f6e393ddce4aa17b493 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:39:11,571 [canton-env-ec-41] INFO  c.d.c.resource.DbStorage:mydomain tid:52829baa59dd01097b3ad00da26d391e - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:39:11,601 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 14:39:11,601 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 14:39:12,084 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 14:39:12,084 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 14:39:12,228 [canton-env-ec-41] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:39:12,228 [canton-env-ec-37] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:39:12,241 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:39:12,241 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:39:12,241 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:39:12,241 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:39:12,243 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:39:12,243 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:39:12,357 [canton-env-ec-41] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 14:39:12,357 [canton-env-ec-37] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:39:12,364 [canton-env-ec-37] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:39:12,364 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:39:12,412 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 14:39:12,412 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 14:39:12,415 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 14:39:12,420 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 14:39:12,462 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 14:39:12,465 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 14:39:12,506 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 14:39:12,612 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 14:39:12,614 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 14:39:12,635 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 14:39:12,712 [canton-env-ec-35] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:12,712 [canton-env-ec-65] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:12,719 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:39:12,720 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:39:13,275 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:39:13,276 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:39:13,394 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 14:39:13,394 [canton-env-ec-70] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 14:39:13,692 [canton-env-ec-65] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 14:39:13,700 [canton-env-ec-70] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (8 ms)
2025-08-24 14:39:13,791 [canton-env-ec-37] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:5046257277893b345ca74d576819f1b2 - Recovering published timely rejections
2025-08-24 14:39:13,832 [canton-env-ec-64] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:5046257277893b345ca74d576819f1b2 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 14:39:13,838 [canton-env-ec-64] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:5046257277893b345ca74d576819f1b2 - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 2)) (exclusive) up to None (inclusive)
2025-08-24 14:39:13,962 [canton-env-ec-37] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 14:39:14,126 [canton-env-ec-64] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 14:39:14,178 [canton-env-ec-60] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 14:39:14,181 [canton-env-ec-60] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:39:14,369 [canton-env-ec-37] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 14:39:14,439 [canton-env-ec-36] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 14:39:14,489 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:39:14,492 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:39:14,628 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:39:14,665 [canton-env-ec-36] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.030s)
2025-08-24 14:39:14,694 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 14:39:14,695 [canton-env-ec-36] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 14:39:14,699 [canton-env-ec-36] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 14:39:14,759 [canton-env-ec-65] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 14:39:14,769 [canton-env-ec-37] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 14:39:14,775 [canton-env-ec-37] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 14:39:14,787 [canton-env-ec-70] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 14:39:14,797 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:39:14,798 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 14:39:14,805 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 14:39:14,805 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 14:39:14,808 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 14:39:14,814 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:39:14,817 [canton-env-ec-70] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 14:39:14,818 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 14:39:14,849 [canton-env-ec-35] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 14:39:14,854 [canton-env-ec-60] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 14:39:14,884 [canton-env-ec-65] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 14:39:14,892 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 14:39:14,924 [canton-env-ec-65] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000003)),0,0)
2025-08-24 14:39:14,946 [canton-env-ec-37] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 14:39:14,948 [canton-env-ec-64] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 14:39:14,957 [canton-env-ec-70] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 14:39:14,969 [canton-env-ec-70] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:15,062 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 14:39:15,493 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 14:39:15,571 [canton-env-ec-60] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:15,602 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 14:39:15,647 [canton-env-ec-36] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (717 ms)
2025-08-24 14:39:15,666 [canton-env-ec-64] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (18 ms)
2025-08-24 14:39:15,669 [canton-env-ec-35] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000003)).
2025-08-24 14:39:15,706 [canton-env-ec-60] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T13:33:14.056515Z on
2025-08-24 14:39:15,717 [canton-env-ec-64] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:39:15.711535Z
2025-08-24 14:39:15,738 [canton-env-ec-64] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T13:33:14.056514Z); next counter 0
2025-08-24 14:39:15,740 [canton-env-ec-64] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:15,756 [canton-env-ec-35] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 14:39:15,757 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 14:39:15,779 [canton-env-ec-41] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 14:39:15,790 [canton-env-ec-37] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 14:39:15,801 [canton-env-ec-35] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:39:15.798605Z
2025-08-24 14:39:15,834 [canton-env-ec-60] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 14:39:15,854 [canton-env-ec-64] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=0
2025-08-24 14:39:15,854 [canton-env-ec-41] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 14:39:15,926 [canton-env-ec-65] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=mediator tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 14:39:15,926 [canton-env-ec-70] INFO  c.d.c.s.c.SequencedEventValidatorImpl:domain=mydomain/client=topology-manager tid:18af563c52f25f92a9e6c6e4ff6fe9f8 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 14:39:15,983 [canton-env-ec-60] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 14:39:15,984 [canton-env-ec-60] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 14:39:15,984 [canton-env-ec-60] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 14:39:16,132 [canton-env-ec-60] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 14:39:16,134 [canton-env-ec-60] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 14:39:16,208 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:5f15e2da8689d6740142a49076fd51ce - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:39:17,043 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6ab77756dbdb905b16455c2940f39dad - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 14:39:17,045 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ae3fbf90b62ffa921a10808d773bb31b - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 14:39:17,044 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fe1c5325f48c50f83a7255c9f15ce6f2 - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 14:39:17,046 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ea286ac1f51eba9d581db89fe0ab01d8 - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 14:39:17,044 [canton-env-ec-154] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ef296d03d254f2936a7bab30549ee5da - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 14:39:17,044 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c31a5904e22870799ea6675ff86a7dd8 - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 14:39:17,057 [canton-env-ec-158] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:4f677289df08b5a4b91af3016b24f758 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:39:17,058 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6f29da67ba7f3f58945fbcc8b9a82b5e - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 14:39:17,044 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f2e360ad2debe7a512ef46733ace638e - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 14:39:17,060 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2ff0f5452f93bec9de72aa3e80af4761 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 14:39:17,061 [canton-env-ec-166] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f3380dbd0baaf70e825ce23db659c58b - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 14:39:17,043 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0196bc2f595a37509571553ba5c9c230 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 14:39:17,043 [canton-env-ec-41] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b66ccd7479dee34efc37d44fae6e0d54 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 14:39:17,060 [canton-env-ec-70] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e65af6194a61e33c8b16a16ae588187c - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 14:39:17,065 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8139cae2192a0f381b559f42fba5ee53 - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 14:39:17,062 [canton-env-ec-153] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2a3af7cc81be72835ed91b06a9161bc9 - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 14:39:17,088 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5a1f79bc1d33c66124c95e2e6489e7e4 - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 14:39:17,089 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2f56300b94f6dbf70c76884b6d535aca - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 14:39:17,091 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2c33c651153b82589c051ac9f71f7a14 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 14:39:17,094 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:677cb9540fa5a7d3360e05e6dad8c5f6 - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 14:39:17,093 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:86bb12421c238992618085251c0af4a1 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 14:39:17,096 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:060aae9561675d0b890c62ad66abbcf4 - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 14:39:17,088 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1695137a1adb927592714b4be6ba3fcd - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 14:39:17,100 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:038985139cb0805669f7e2ba38519c6f - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 14:39:17,101 [canton-env-ec-41] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9e9bf23312ec0e7f8a69057faffd18ed - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 14:39:17,101 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d2bf5dbc57a63c931237bcafc9086723 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 14:39:17,102 [canton-env-ec-41] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1f7bf95c4dc699d574fba5bf20e7efd2 - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 14:39:17,087 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5e00ada25034074362e546ce334160d2 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 14:39:17,108 [canton-env-ec-70] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b3a37e9f7c59cee2bfe0cfae2c6c771f - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 14:39:17,102 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:4e5e45f08b86b81799504759e7bf6a99 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 14:39:17,110 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7671fd502342303ae8f070a499769d30 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 14:39:17,111 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:81a74b13a9b9723ce8fdfc461a9d37f5 - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 14:39:17,197 [canton-env-ec-70] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:c1541a336e94b9c4f5ce1906d3363fc0 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:39:17,221 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d3085ad08b46988ea3a2295aa09a1b1b - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:39:17,303 [canton-env-ec-41] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:f40e3f5302676e41514c42e653cf8cdf - Received request for transactions, startExclusive -> '000000000000000003', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 14:39:17,387 [canton-env-ec-37] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:3cd936998b4a67758c78a41205946849 - Reconnecting to domains List(). Already connected: Set()
2025-08-24 14:39:17,390 [canton-env-ec-37] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:3cd936998b4a67758c78a41205946849 - Successfully re-connected to domains List()
2025-08-24 14:39:17,393 [main] INFO  c.d.c.e.CommunityEnvironment tid:3cd936998b4a67758c78a41205946849 - Successfully started all nodes
2025-08-24 14:39:53,846 [canton-env-ec-41] INFO  c.d.c.p.a.DomainConnectivityService:participant=participant1 tid:58a8db2bac8e3dba5cca73b278a786be - Registering mydomain with DomainConnectionConfig(
  domain = Domain 'mydomain',
  sequencerConnections = Sequencer 'DefaultSequencer' -> GrpcSequencerConnection(endpoints = https://0.0.0.0:5018, transportSecurity = true, customTrustCertificates),
  manualConnect = true
)
2025-08-24 14:39:53,962 [canton-env-ec-159] INFO  c.d.c.c.d.g.SequencerInfoLoader:participant=participant1 tid:68ac6e42abf2b2e6df7d6cb42a71de09 - Version handshake with sequencer Sequencer 'DefaultSequencer' and domain using protocol version 7 succeeded.
2025-08-24 14:39:54,132 [canton-env-ec-37] INFO  c.d.c.c.d.g.SequencerInfoLoader:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Version handshake with sequencer Sequencer 'DefaultSequencer' and domain using protocol version 7 succeeded.
2025-08-24 14:39:54,384 [canton-env-ec-159] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Applied topology transaction Add ParticipantState(To, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) at 2025-08-24T14:39:54.382801Z
2025-08-24 14:39:54,537 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - No event found up to 0001-01-01T00:00:00Z. Resubscribing from the beginning.
2025-08-24 14:39:54,538 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Processing events from the SequencedEventStore from 0001-01-01T00:00:00.000001Z on
2025-08-24 14:39:54,543 [canton-env-ec-41] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp None; next counter 0
2025-08-24 14:39:54,544 [canton-env-ec-41] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:54,552 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - UNM::9393931172476573ee44461d598d8e2eb6655079::1220b845dcf0... subscribes from counter=0
2025-08-24 14:39:54,734 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'UNM::9393931172476573ee44461d598d8e2eb6655079::1220b845dcf0...' sends request with id '440ef980-b1ec-4eb9-8ebe-8d4a7758dec0' of size 1433 bytes with 1 envelopes.
2025-08-24 14:39:54,832 [canton-env-ec-159] INFO  c.d.c.d.s.s.SequencerReader$EventsReader:domain=mydomain/subscriber=UNM::9393931172476573ee44461d598d8e2eb6655079::1220b845dcf0 tid:ac0ec30f2287fdc404fc11b78b99e678 - Using approximate topology snapshot at 2025-08-24T13:33:14.056515Z for desired timestamp 2025-08-24T14:39:54.784009Z
2025-08-24 14:39:55,008 [canton-env-ec-36] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Applied topology transaction Add NamespaceDelegation(1220b845dcf0..., SigningPublicKey(id = 1220b845dcf0..., format = Tink, scheme = Ed25519), true) at 2025-08-24T14:39:55.005591Z
2025-08-24 14:39:55,038 [canton-env-ec-154] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Applied topology transaction Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., SigningPublicKey(id = 12205c0df38e..., format = Tink, scheme = Ed25519)) at 2025-08-24T14:39:55.037909Z
2025-08-24 14:39:55,052 [canton-env-ec-36] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Applied topology transaction Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., EncryptionPublicKey(id = 12204d828285..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) at 2025-08-24T14:39:55.051913Z
2025-08-24 14:39:55,078 [canton-env-ec-159] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'DOM::mydomain::122060f2dea2...' sends request with id '83857357-f3ee-4625-9ad9-351965a9c7ac' of size 921 bytes with 1 envelopes.
2025-08-24 14:39:55,085 [canton-env-ec-160] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Setting participant PAR::participant1::1220b845dcf0... state to ParticipantAttributes(Submission,Ordinary)
2025-08-24 14:39:55,086 [canton-env-ec-160] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Applied topology transaction Add ParticipantState(From, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) at 2025-08-24T14:39:55.084301Z
2025-08-24 14:39:55,107 [canton-env-ec-164] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Applied topology transaction Add ParticipantState(To, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) at 2025-08-24T14:39:55.106268Z
2025-08-24 14:39:55,111 [canton-env-ec-41] INFO  c.d.c.d.t.RequestProcessingStrategy$Impl:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Successfully onboarded PAR::participant1::1220b845dcf0...
2025-08-24 14:39:55,119 [canton-env-ec-160] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Register topology request by UNM::9393931172476573ee44461d598d8e2eb6655079::1220b845dcf0... for PAR::participant1::1220b845dcf0... yielded
  Accepted -> Add NamespaceDelegation(1220b845dcf0..., SigningPublicKey(id = 1220b845dcf0..., format = Tink, scheme = Ed25519), true)
  Accepted -> Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., SigningPublicKey(id = 12205c0df38e..., format = Tink, scheme = Ed25519))
  Accepted -> Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., EncryptionPublicKey(id = 12204d828285..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM))
  Accepted -> Add ParticipantState(To, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary)
2025-08-24 14:39:55,135 [canton-env-ec-154] INFO  c.d.c.d.m.Mediator:domain=mydomain/node=mediator tid:ac0ec30f2287fdc404fc11b78b99e678 - Caught up with batch with counter=1 with sequencer with 47 ms delay
2025-08-24 14:39:55,148 [canton-env-ec-166] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/1 Add NamespaceDelegation(1220b845dcf0..., SigningPublicKey(id = 1220b845dcf0..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T14:39:55.338560Z) (epsilon=250 ms)
2025-08-24 14:39:55,151 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'DOM::mydomain::122060f2dea2...' sends request with id '68e54aa3-c331-4a7f-9ad0-7ef61af5b4b7' of size 1567 bytes with 1 envelopes.
2025-08-24 14:39:55,151 [canton-env-ec-160] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'DOM::mydomain::122060f2dea2...' sends request with id '535e6cef-7bd4-4776-9fb2-a8eb5721b72e' of size 558 bytes with 1 envelopes.
2025-08-24 14:39:55,157 [canton-env-ec-60] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/1 Add NamespaceDelegation(1220b845dcf0..., SigningPublicKey(id = 1220b845dcf0..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T14:39:55.338560Z) (epsilon=250 ms)
2025-08-24 14:39:55,206 [canton-env-ec-166] INFO  c.d.c.s.c.ResilientSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:39:55,207 [canton-env-ec-166] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:39:55,213 [canton-env-ec-36] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:39:55,222 [canton-env-ec-167] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Request c.d.c.d.a.v.SequencerService/SubscribeUnauthenticatedVersioned by /[0:0:0:0:0:0:0:1%0]:45976: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:39:55,228 [canton-env-ec-154] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/3 Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., SigningPublicKey(id = 12205c0df38e..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T14:39:55.408256Z) (epsilon=250 ms)
2025-08-24 14:39:55,229 [canton-env-ec-154] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 2/3 Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., EncryptionPublicKey(id = 12204d828285..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) with ts=EffectiveTime(2025-08-24T14:39:55.408256Z) (epsilon=250 ms)
2025-08-24 14:39:55,231 [canton-env-ec-60] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/3 Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., SigningPublicKey(id = 12205c0df38e..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T14:39:55.408256Z) (epsilon=250 ms)
2025-08-24 14:39:55,232 [canton-env-ec-60] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 2/3 Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., EncryptionPublicKey(id = 12204d828285..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) with ts=EffectiveTime(2025-08-24T14:39:55.408256Z) (epsilon=250 ms)
2025-08-24 14:39:55,232 [canton-env-ec-60] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 3/3 Add ParticipantState(From, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T14:39:55.408256Z) (epsilon=250 ms)
2025-08-24 14:39:55,230 [canton-env-ec-154] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 3/3 Add ParticipantState(From, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T14:39:55.408256Z) (epsilon=250 ms)
2025-08-24 14:39:55,238 [canton-env-ec-64] INFO  c.d.c.d.t.MemberTopologyCatchup:domain=mydomain/identity tid:ac0ec30f2287fdc404fc11b78b99e678 - Participant PAR::participant1::1220b845dcf0... is changing from None to Some(Submission), requires to catch up 10
2025-08-24 14:39:55,258 [canton-env-ec-60] INFO  c.d.c.c.d.g.GrpcSequencerConnectClient:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - The operation 'verify active' was not successful. New kind of error: no success error (request infinite retries). Retrying after 0.5s. Result: Right(false). 
2025-08-24 14:39:55,266 [canton-env-ec-64] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'DOM::mydomain::122060f2dea2...' sends request with id 'a11932a4-5aca-499c-adc2-7e9064eaed63' of size 2667 bytes with 1 envelopes.
2025-08-24 14:39:55,336 [canton-env-ec-166] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'DOM::mydomain::122060f2dea2...' sends request with id '85c0cc14-7a9d-4d76-8e8e-2b092a2f18a5' of size 944 bytes with 1 envelopes.
2025-08-24 14:39:55,388 [canton-env-ec-160] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/1 Add ParticipantState(To, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T14:39:55.595054Z) (epsilon=250 ms)
2025-08-24 14:39:55,392 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/1 Add ParticipantState(To, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T14:39:55.595054Z) (epsilon=250 ms)
2025-08-24 14:39:55,762 [canton-env-ec-160] INFO  c.d.c.c.d.g.GrpcSequencerConnectClient:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Now retrying operation 'verify active'. 
2025-08-24 14:39:55,824 [canton-env-ec-167] INFO  c.d.c.p.s.SyncDomainEphemeralStateFactoryImpl:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Computed starting points: ProcessingStartingPoints(
  clean replay = MessageCleanReplayStartingPoint(next request counter = 0, next sequencer counter = 0, prenext timestamp = 0001-01-01T00:00:00Z),
  processing = MessageProcessingStartingPoint(next request counter = 0, next sequencer counter = 0, prenext timestamp = 0001-01-01T00:00:00Z),
  rewound sequencer counter prehead = None()
)
2025-08-24 14:39:55,824 [canton-env-ec-167] INFO  c.d.c.s.d.DbCursorPreheadStore:participant=participant1/domain-alias=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Rewinding prehead to None
2025-08-24 14:39:56,049 [canton-env-ec-159] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Will use parallelism 8 when computing ACS commitments
2025-08-24 14:39:56,064 [canton-env-ec-159] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,064 [canton-env-ec-159] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,064 [canton-env-ec-159] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,065 [canton-env-ec-159] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,065 [canton-env-ec-159] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,065 [canton-env-ec-159] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Connected to domain and starting synchronisation: Domain 'mydomain'
2025-08-24 14:39:56,067 [canton-env-ec-37] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Initialized from stored snapshot at RecordTime(timestamp = 0001-01-01T00:00:00Z, tieBreaker = -9223372036854775808) (might be incomplete)
2025-08-24 14:39:56,069 [canton-env-ec-160] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Initialized the ACS commitment processor queue
2025-08-24 14:39:56,103 [canton-env-ec-164] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Found 0 repair requests at request counters Seq()
2025-08-24 14:39:56,117 [canton-env-ec-36] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - No event found up to 0001-01-01T00:00:00Z. Resubscribing from the beginning.
2025-08-24 14:39:56,117 [canton-env-ec-36] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Processing events from the SequencedEventStore from 0001-01-01T00:00:00.000001Z on
2025-08-24 14:39:56,133 [canton-env-ec-36] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp None; next counter 0
2025-08-24 14:39:56,134 [canton-env-ec-36] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,135 [canton-env-ec-36] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,136 [canton-env-ec-36] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,185 [canton-env-ec-64] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T15:39:56.182173Z
2025-08-24 14:39:56,194 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - PAR::participant1::1220b845dcf0... subscribes from counter=0
2025-08-24 14:39:56,223 [canton-env-ec-37] INFO  c.d.c.s.c.SequencedEventValidatorImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Skipping signature verification of the first sequenced event due to a fresh subscription from SEQ::mydomain::122060f2dea2...
2025-08-24 14:39:56,234 [canton-env-ec-64] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Caught up with batch with counter=0 with sequencer with 956 ms delay
2025-08-24 14:39:56,235 [canton-env-ec-64] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Replaying requests 0 up to clean prehead -1
2025-08-24 14:39:56,236 [canton-env-ec-64] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Replaying or processing locally stored events with sequencer counters 0 to -1
2025-08-24 14:39:56,257 [canton-env-ec-64] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Processing event at sc=0, ts=2025-08-24T14:39:55.278795Z, with contents=DomainTopologyTransactionMessage
2025-08-24 14:39:56,260 [canton-env-ec-37] INFO  c.d.c.t.p.DomainTopologyTransactionMessageValidator$Impl:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Using approximate topology snapshot at 0001-01-01T00:00:00.000001Z for desired timestamp 2025-08-24T14:39:55.278795Z
2025-08-24 14:39:56,293 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Updated topology change delay from=0s to 0.25s
2025-08-24 14:39:56,295 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/10 Add NamespaceDelegation(122060f2dea2..., SigningPublicKey(id = 122060f2dea2..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,296 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 2/10 Add OwnerToKeyMapping(DOM::mydomain::122060f2dea2..., SigningPublicKey(id = 1220289626ce..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,296 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 3/10 Add OwnerToKeyMapping(MED::mydomain::122060f2dea2..., SigningPublicKey(id = 1220120ea179..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,297 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 4/10 Add OwnerToKeyMapping(SEQ::mydomain::122060f2dea2..., SigningPublicKey(id = 122096c87f5e..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,297 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 5/10 Add NamespaceDelegation(1220b845dcf0..., SigningPublicKey(id = 1220b845dcf0..., format = Tink, scheme = Ed25519), true) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,298 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 6/10 Add ParticipantState(From, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,298 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 7/10 Add MediatorDomainState(Both, mydomain::122060f2dea2..., MED::mydomain::122060f2dea2...) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,299 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 8/10 Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., SigningPublicKey(id = 12205c0df38e..., format = Tink, scheme = Ed25519)) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,299 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 9/10 Add OwnerToKeyMapping(PAR::participant1::1220b845dcf0..., EncryptionPublicKey(id = 12204d828285..., format = Tink, scheme = ECIES-P256_HMAC256_AES128-GCM)) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,303 [canton-env-ec-35] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 10/10 Replace DomainParametersChange(
  mydomain::122060f2dea2...,
  DynamicDomainParameters(
    participant response timeout = 30s,
    mediator reaction timeout = 30s,
    transfer exclusivity timeout = 1m,
    topology change delay = 0.25s,
    ledger time record time tolerance = 1m,
    mediator deduplication timeout = 2m,
    reconciliation interval = 1m,
    max rate per participant = 1000000,
    max request size = 10485760,
    catchup config = CatchUpConfig(catchUpIntervalSkip = 5, nrIntervalsToTriggerCatchUp = 2)
  )
) with ts=EffectiveTime(2025-08-24T14:39:55.278795Z) (epsilon=0 ms)
2025-08-24 14:39:56,354 [canton-env-ec-64] INFO  c.d.c.p.t.c.MissingKeysAlerter:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Domain mydomain::122060f2dea2... update my participant permission as of 2025-08-24T14:39:55.278795Z to Submission, Ordinary
2025-08-24 14:39:56,432 [canton-env-ec-37] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Processing event at sc=1, ts=2025-08-24T14:39:55.345054Z, with contents=DomainTopologyTransactionMessage
2025-08-24 14:39:56,445 [canton-env-ec-154] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/1 Add ParticipantState(To, mydomain::122060f2dea2..., PAR::participant1::1220b845dcf0..., Submission, Ordinary) with ts=EffectiveTime(2025-08-24T14:39:55.595054Z) (epsilon=250 ms)
2025-08-24 14:39:56,485 [input-mapping-pool-0] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing at offset=000000000000000004 PartyAddedToParticipant(recordTime = 2025-08-24T14:39:56.366982Z, party = participant1::1220b845dcf0..., displayName = '', participantId = participant1::1220b845dcf0..., ...)
2025-08-24 14:39:56,761 [canton-env-ec-160] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'PAR::participant1::1220b845dcf0...' sends request with id '2ea59bde-b381-405a-917d-15461a30944f' of size 2097 bytes with 1 envelopes.
2025-08-24 14:39:56,825 [canton-env-ec-159] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Processing event at sc=2, ts=2025-08-24T14:39:56.772753Z, messageId=2ea59bde-b381-405a-917d-15461a30944f, with contents=Seq()
2025-08-24 14:39:56,835 [canton-env-ec-167] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Applied topology transaction Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) at 2025-08-24T14:39:56.833699Z
2025-08-24 14:39:56,841 [canton-env-ec-35] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Register topology request by PAR::participant1::1220b845dcf0... for PAR::participant1::1220b845dcf0... yielded
  Accepted -> Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
)
2025-08-24 14:39:56,858 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'DOM::mydomain::122060f2dea2...' sends request with id '7e790f33-f448-4f4c-94fe-448cbf7dada8' of size 536 bytes with 1 envelopes.
2025-08-24 14:39:56,862 [canton-env-ec-164] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'DOM::mydomain::122060f2dea2...' sends request with id 'b0a7ec44-0eba-43e1-8ead-0950637ef72c' of size 2225 bytes with 1 envelopes.
2025-08-24 14:39:56,895 [canton-env-ec-60] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Processing event at sc=3, ts=2025-08-24T14:39:56.865612Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 14:39:56,904 [canton-env-ec-160] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,905 [canton-env-ec-160] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,905 [canton-env-ec-160] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:39:56,910 [canton-env-ec-167] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:ac0ec30f2287fdc404fc11b78b99e678 - Processing event at sc=4, ts=2025-08-24T14:39:56.869534Z, with contents=DomainTopologyTransactionMessage
2025-08-24 14:39:56,923 [canton-env-ec-37] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T14:39:57.119534Z) (epsilon=250 ms)
2025-08-24 14:39:56,923 [canton-env-ec-160] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T14:39:57.119534Z) (epsilon=250 ms)
2025-08-24 14:39:56,927 [canton-env-ec-154] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    1cc0ce9df772...,
    65921e553a35...,
    6851f194e144...,
    57b5c520512c...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    e8b3e0650dc0...,
    18597917bc74...,
    852d8e3a8ccf...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T14:39:57.119534Z) (epsilon=250 ms)
2025-08-24 14:39:56,949 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiActiveContractsService:participant=participant1 tid:12f64fb1df10add08fd62f2d09e4c9f1 - Received request for active contracts: GetActiveContractsRequest(participant1,Some(TransactionFilter(Map(participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d -> Filters(Some(InclusiveFilters(Vector(Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,PingProposal), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Ping), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Pong), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Explode), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Merge), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Collapse)),Vector(),Vector())))))),false,), filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: ['65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Pong', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Collapse', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:PingProposal', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Explode', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Merge', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Ping']}.
2025-08-24 14:39:57,433 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - 'PAR::participant1::1220b845dcf0...' sends request with id 'tick-23a3e7f2-3c96-47c8-94f6-273e5b97b856' of size 342 bytes with 0 envelopes.
2025-08-24 14:40:38,701 [canton-env-ec-36] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:6d7dd79f4cf582592ca28906464d19e9 - Applied topology transaction Add PartyToParticipant(Both, NewBank::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) at 2025-08-24T14:40:38.700385Z
2025-08-24 14:40:38,727 [canton-env-ec-160] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:4e46dcfd8bf3730485afdcdea6edb31b - 'PAR::participant1::1220b845dcf0...' sends request with id 'ce6b372e-d7d7-464e-abf5-b60df2e95a43' of size 840 bytes with 1 envelopes.
2025-08-24 14:40:38,759 [canton-env-ec-41] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:4e46dcfd8bf3730485afdcdea6edb31b - Processing event at sc=6, ts=2025-08-24T14:40:38.733362Z, messageId=ce6b372e-d7d7-464e-abf5-b60df2e95a43, with contents=Seq()
2025-08-24 14:40:38,769 [canton-env-ec-153] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:4e46dcfd8bf3730485afdcdea6edb31b - Applied topology transaction Add PartyToParticipant(Both, NewBank::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) at 2025-08-24T14:40:38.769205Z
2025-08-24 14:40:38,773 [canton-env-ec-159] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:4e46dcfd8bf3730485afdcdea6edb31b - Register topology request by PAR::participant1::1220b845dcf0... for PAR::participant1::1220b845dcf0... yielded
  Accepted -> Add PartyToParticipant(Both, NewBank::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission)
2025-08-24 14:40:38,788 [canton-env-ec-167] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:4e46dcfd8bf3730485afdcdea6edb31b - 'DOM::mydomain::122060f2dea2...' sends request with id '56ed0155-e645-4a1a-b77e-23f02b5397e5' of size 535 bytes with 1 envelopes.
2025-08-24 14:40:38,795 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:4e46dcfd8bf3730485afdcdea6edb31b - 'DOM::mydomain::122060f2dea2...' sends request with id '6f8f1caa-1167-49c0-bb0f-df6df5856460' of size 954 bytes with 1 envelopes.
2025-08-24 14:40:38,828 [canton-env-ec-60] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:4e46dcfd8bf3730485afdcdea6edb31b - Processing event at sc=7, ts=2025-08-24T14:40:38.794814Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 14:40:38,837 [canton-env-ec-70] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:4e46dcfd8bf3730485afdcdea6edb31b - Processing event at sc=8, ts=2025-08-24T14:40:38.804079Z, with contents=DomainTopologyTransactionMessage
2025-08-24 14:40:38,845 [canton-env-ec-166] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:4e46dcfd8bf3730485afdcdea6edb31b - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBank::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.054079Z) (epsilon=250 ms)
2025-08-24 14:40:38,850 [canton-env-ec-64] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:4e46dcfd8bf3730485afdcdea6edb31b - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBank::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.054079Z) (epsilon=250 ms)
2025-08-24 14:40:38,854 [canton-env-ec-166] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:4e46dcfd8bf3730485afdcdea6edb31b - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBank::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.054079Z) (epsilon=250 ms)
2025-08-24 14:40:39,040 [canton-env-ec-65] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:43ccce2e3bba5bb7809c149778f5ca85 - Applied topology transaction Add PartyToParticipant(Both, NewAlice::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) at 2025-08-24T14:40:39.039650Z
2025-08-24 14:40:39,056 [canton-env-ec-70] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:038c43d3f334e8689ddfd3330dd73e9f - 'PAR::participant1::1220b845dcf0...' sends request with id 'ac80665e-5b63-49e6-b0c2-52b0951ce711' of size 840 bytes with 1 envelopes.
2025-08-24 14:40:39,090 [canton-env-ec-164] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:038c43d3f334e8689ddfd3330dd73e9f - Processing event at sc=9, ts=2025-08-24T14:40:39.062152Z, messageId=ac80665e-5b63-49e6-b0c2-52b0951ce711, with contents=Seq()
2025-08-24 14:40:39,100 [canton-env-ec-153] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:038c43d3f334e8689ddfd3330dd73e9f - Applied topology transaction Add PartyToParticipant(Both, NewAlice::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) at 2025-08-24T14:40:39.100409Z
2025-08-24 14:40:39,104 [canton-env-ec-37] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:038c43d3f334e8689ddfd3330dd73e9f - Register topology request by PAR::participant1::1220b845dcf0... for PAR::participant1::1220b845dcf0... yielded
  Accepted -> Add PartyToParticipant(Both, NewAlice::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission)
2025-08-24 14:40:39,125 [canton-env-ec-70] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:038c43d3f334e8689ddfd3330dd73e9f - 'DOM::mydomain::122060f2dea2...' sends request with id '1e37239e-b746-47cd-98fd-ef439ac02dbf' of size 533 bytes with 1 envelopes.
2025-08-24 14:40:39,126 [canton-env-ec-166] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:038c43d3f334e8689ddfd3330dd73e9f - 'DOM::mydomain::122060f2dea2...' sends request with id '11f4d8f7-1844-4241-b2c5-0d4cb9bc9430' of size 950 bytes with 1 envelopes.
2025-08-24 14:40:39,154 [input-mapping-pool-1] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:4e46dcfd8bf3730485afdcdea6edb31b - Storing at offset=000000000000000005 PartyAddedToParticipant(recordTime = 2025-08-24T14:40:39.110697Z, party = NewBank::1220b845dcf0..., displayName = '', participantId = participant1::1220b845dcf0..., ...)
2025-08-24 14:40:39,176 [canton-env-ec-153] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:038c43d3f334e8689ddfd3330dd73e9f - Processing event at sc=10, ts=2025-08-24T14:40:39.133099Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 14:40:39,177 [canton-env-ec-65] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:038c43d3f334e8689ddfd3330dd73e9f - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewAlice::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.385230Z) (epsilon=250 ms)
2025-08-24 14:40:39,185 [canton-env-ec-166] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:038c43d3f334e8689ddfd3330dd73e9f - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewAlice::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.385230Z) (epsilon=250 ms)
2025-08-24 14:40:39,186 [canton-env-ec-167] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:038c43d3f334e8689ddfd3330dd73e9f - Processing event at sc=11, ts=2025-08-24T14:40:39.135230Z, with contents=DomainTopologyTransactionMessage
2025-08-24 14:40:39,195 [canton-env-ec-164] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:038c43d3f334e8689ddfd3330dd73e9f - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewAlice::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.385230Z) (epsilon=250 ms)
2025-08-24 14:40:39,298 [canton-env-ec-64] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:f4d1332c9f3d1be49d73a9306f2bc1d9 - Applied topology transaction Add PartyToParticipant(Both, NewBob::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) at 2025-08-24T14:40:39.297754Z
2025-08-24 14:40:39,316 [canton-env-ec-64] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:2a1a1000f0fbea3361f13548fd500efb - 'PAR::participant1::1220b845dcf0...' sends request with id '78a9f077-a45e-4625-b4cb-007a5d6cdfb6' of size 840 bytes with 1 envelopes.
2025-08-24 14:40:39,346 [canton-env-ec-65] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:2a1a1000f0fbea3361f13548fd500efb - Processing event at sc=12, ts=2025-08-24T14:40:39.324396Z, messageId=78a9f077-a45e-4625-b4cb-007a5d6cdfb6, with contents=Seq()
2025-08-24 14:40:39,354 [canton-env-ec-167] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:2a1a1000f0fbea3361f13548fd500efb - Applied topology transaction Add PartyToParticipant(Both, NewBob::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) at 2025-08-24T14:40:39.354183Z
2025-08-24 14:40:39,357 [canton-env-ec-65] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:2a1a1000f0fbea3361f13548fd500efb - Register topology request by PAR::participant1::1220b845dcf0... for PAR::participant1::1220b845dcf0... yielded
  Accepted -> Add PartyToParticipant(Both, NewBob::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission)
2025-08-24 14:40:39,367 [canton-env-ec-64] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:2a1a1000f0fbea3361f13548fd500efb - 'DOM::mydomain::122060f2dea2...' sends request with id '7f7aa3d6-0ab6-4984-baf3-af3bea598e9c' of size 536 bytes with 1 envelopes.
2025-08-24 14:40:39,376 [canton-env-ec-154] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:2a1a1000f0fbea3361f13548fd500efb - 'DOM::mydomain::122060f2dea2...' sends request with id '26955059-e294-41c8-a2ce-ea48fa049bf1' of size 954 bytes with 1 envelopes.
2025-08-24 14:40:39,403 [canton-env-ec-159] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:2a1a1000f0fbea3361f13548fd500efb - Processing event at sc=13, ts=2025-08-24T14:40:39.372329Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 14:40:39,420 [canton-env-ec-65] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:2a1a1000f0fbea3361f13548fd500efb - Processing event at sc=14, ts=2025-08-24T14:40:39.383249Z, with contents=DomainTopologyTransactionMessage
2025-08-24 14:40:39,423 [canton-env-ec-41] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:2a1a1000f0fbea3361f13548fd500efb - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBob::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.633249Z) (epsilon=250 ms)
2025-08-24 14:40:39,425 [canton-env-ec-36] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:2a1a1000f0fbea3361f13548fd500efb - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBob::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.633249Z) (epsilon=250 ms)
2025-08-24 14:40:39,427 [canton-env-ec-166] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:2a1a1000f0fbea3361f13548fd500efb - Storing topology transaction 1/1 Add PartyToParticipant(Both, NewBob::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission) with ts=EffectiveTime(2025-08-24T14:40:39.633249Z) (epsilon=250 ms)
2025-08-24 14:40:39,475 [input-mapping-pool-2] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:038c43d3f334e8689ddfd3330dd73e9f - Storing at offset=000000000000000006 PartyAddedToParticipant(recordTime = 2025-08-24T14:40:39.456782Z, party = NewAlice::1220b845dcf0..., displayName = '', participantId = participant1::1220b845dcf0..., ...)
2025-08-24 14:40:39,716 [input-mapping-pool-3] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:2a1a1000f0fbea3361f13548fd500efb - Storing at offset=000000000000000007 PartyAddedToParticipant(recordTime = 2025-08-24T14:40:39.687509Z, party = NewBob::1220b845dcf0..., displayName = '', participantId = participant1::1220b845dcf0..., ...)
2025-08-24 14:41:35,726 [input-mapping-pool-4] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:1dd9007aa02e83c34e337f71acbbcee3 - Storing at offset=000000000000000008 PublicPackageUpload(
  recordTime = 2025-08-24T14:41:35.600419Z,
  archives = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  ),
  sourceDescription = 'RWA'
)
2025-08-24 14:41:35,892 [canton-env-ec-159] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:1dd9007aa02e83c34e337f71acbbcee3 - Applied topology transaction Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) at 2025-08-24T14:41:35.891720Z
2025-08-24 14:41:35,920 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:70331b148bef8338e81d346f0210bd3a - 'PAR::participant1::1220b845dcf0...' sends request with id '193b8bcd-1fd0-4bf3-8355-7a9fde7d2307' of size 1985 bytes with 1 envelopes.
2025-08-24 14:41:35,950 [canton-env-ec-167] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:70331b148bef8338e81d346f0210bd3a - Processing event at sc=15, ts=2025-08-24T14:41:35.925931Z, messageId=193b8bcd-1fd0-4bf3-8355-7a9fde7d2307, with contents=Seq()
2025-08-24 14:41:35,956 [canton-env-ec-153] INFO  c.d.c.d.t.DomainTopologyManager:domain=mydomain tid:70331b148bef8338e81d346f0210bd3a - Applied topology transaction Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) at 2025-08-24T14:41:35.955997Z
2025-08-24 14:41:35,960 [canton-env-ec-37] INFO  c.d.c.d.t.DomainTopologyManagerRequestService:domain=mydomain tid:70331b148bef8338e81d346f0210bd3a - Register topology request by PAR::participant1::1220b845dcf0... for PAR::participant1::1220b845dcf0... yielded
  Accepted -> Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
)
2025-08-24 14:41:35,968 [canton-env-ec-41] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:70331b148bef8338e81d346f0210bd3a - 'DOM::mydomain::122060f2dea2...' sends request with id '951a00e7-7a87-4d69-a678-e18d46c5cf94' of size 534 bytes with 1 envelopes.
2025-08-24 14:41:35,972 [canton-env-ec-153] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:70331b148bef8338e81d346f0210bd3a - 'DOM::mydomain::122060f2dea2...' sends request with id 'a1760093-04c1-46bc-a2e9-9de05ed29c79' of size 2111 bytes with 1 envelopes.
2025-08-24 14:41:35,995 [canton-env-ec-159] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:70331b148bef8338e81d346f0210bd3a - Processing event at sc=16, ts=2025-08-24T14:41:35.974496Z, with contents=RegisterTopologyTransactionResponse
2025-08-24 14:41:36,004 [canton-env-ec-164] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:70331b148bef8338e81d346f0210bd3a - Processing event at sc=17, ts=2025-08-24T14:41:35.978547Z, with contents=DomainTopologyTransactionMessage
2025-08-24 14:41:36,009 [canton-env-ec-70] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/client=topology-manager tid:70331b148bef8338e81d346f0210bd3a - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T14:41:36.228547Z) (epsilon=250 ms)
2025-08-24 14:41:36,011 [canton-env-ec-166] INFO  c.d.c.t.p.TopologyTransactionProcessor:participant=participant1 tid:70331b148bef8338e81d346f0210bd3a - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T14:41:36.228547Z) (epsilon=250 ms)
2025-08-24 14:41:36,015 [canton-env-ec-159] INFO  c.d.c.t.p.TopologyTransactionProcessor:domain=mydomain/node=mediator/client=mediator tid:70331b148bef8338e81d346f0210bd3a - Storing topology transaction 1/1 Add VettedPackages(
  participant = participant1::1220b845dcf0...,
  packages = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  )
) with ts=EffectiveTime(2025-08-24T14:41:36.228547Z) (epsilon=250 ms)
2025-08-24 14:41:36,372 [canton-env-ec-41] INFO  c.d.c.p.a.s.a.ApiPackageManagementService:participant=participant1 tid:fe8bb85c232a50fcf9f7b3108af9ae04 - Listing known packages.
2025-08-24 14:43:30,255 [main] ERROR c.d.c.c.LocalParticipantReference:participant=participant1 - The command is currently disabled. You need to enable it explicitly by setting `canton.features.enable-testing-commands = yes` in your Canton configuration file (`.conf`)
2025-08-24 14:43:42,647 [main] ERROR c.d.c.c.LocalParticipantReference:participant=participant1 - The command is currently disabled. You need to enable it explicitly by setting `canton.features.enable-testing-commands = yes` in your Canton configuration file (`.conf`)
2025-08-24 14:44:35,329 [main] ERROR c.d.c.c.LocalParticipantReference:participant=participant1 - The command is currently disabled. You need to enable it explicitly by setting `canton.features.enable-testing-commands = yes` in your Canton configuration file (`.conf`)
2025-08-24 14:45:14,891 [daml.index.db.threadpool.connection.indexer-13] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes()),2025-08-24T14:00:00Z)
2025-08-24 14:45:14,893 [daml.index.db.threadpool.connection.indexer-13] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes()),2025-08-24T14:00:00Z)
2025-08-24 14:45:49,813 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 14:45:49,816 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 14:45:49,819 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,820 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,820 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,821 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,835 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 14:45:49,840 [canton-env-ec-41] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:f40e3f5302676e41514c42e653cf8cdf - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:50184: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:45:49,851 [canton-env-ec-41] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 14:45:49,861 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 14:45:49,865 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 14:45:49,866 [canton-env-ec-167] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 14:45:49,867 [canton-env-ec-167] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 14:45:49,868 [canton-env-ec-167] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 14:45:49,870 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 14:45:49,889 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 14:45:49,890 [canton-env-ec-41] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 14:45:49,891 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 14:45:49,892 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 14:45:49,892 [canton-env-ec-70] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 14:45:49,892 [canton-env-ec-70] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 14:45:49,897 [canton-env-ec-160] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 14:45:49,900 [canton-env-ec-160] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 14:45:49,905 [canton-env-ec-37] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,905 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,907 [canton-env-ec-37] INFO  c.d.c.s.c.ResilientSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,907 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,907 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,910 [canton-env-ec-154] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:45:49,910 [canton-env-ec-159] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:ac0ec30f2287fdc404fc11b78b99e678 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:45988: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:45:49,911 [canton-env-ec-167] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:ac0ec30f2287fdc404fc11b78b99e678 - Domain 'mydomain' disconnected because sequencer client was closed
2025-08-24 14:45:49,911 [canton-env-ec-37] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,912 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,919 [canton-env-ec-37] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,919 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,929 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 14:45:49,929 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 14:45:49,930 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 14:45:49,930 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 14:45:49,931 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,931 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:45:49,932 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 14:45:49,938 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 14:45:49,939 [canton-env-ec-37] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,940 [canton-env-ec-37] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,944 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 14:45:49,948 [canton-env-ec-37] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,949 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,950 [canton-env-ec-37] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,950 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,951 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:45:49,952 [canton-env-ec-159] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:891056e2190709de46384f7b4cc74d15 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:33590: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:45:49,956 [canton-env-ec-37] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,957 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:45:49,957 [canton-env-ec-65] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:45:49,958 [canton-env-ec-65] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:c7fa7eec868a2d4e155088ccb7d06c56 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:33616: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:45:49,963 [canton-env-ec-37] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:45:49,964 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:45:49,964 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:45:49,977 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,977 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:45:49,979 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 14:45:49,985 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 14:45:49,985 [canton-env-ec-37] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:49,986 [canton-env-ec-37] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:45:50,005 [canton-env-ec-154] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 14:45:50,036 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 14:45:50,037 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 14:46:53,510 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 14:46:54,577 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 14:46:54,599 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 14:46:54,815 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 14:46:54,901 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 14:46:54,973 [main] INFO  c.d.c.e.CommunityEnvironment tid:752d26f1ff7e2e00d323d2992329e0e5 - Automatically starting all instances
2025-08-24 14:46:55,026 [canton-env-ec-41] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 14:46:55,026 [canton-env-ec-36] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 14:46:55,060 [canton-env-ec-41] INFO  c.d.c.r.DbStorage:participant1 tid:074237c4621ae8268a55862d45b72f03 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:46:55,060 [canton-env-ec-36] INFO  c.d.c.resource.DbStorage:mydomain tid:23d4da8f0a5d124e9a56e9105ef5e629 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:46:55,091 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Starting...
2025-08-24 14:46:55,091 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Starting...
2025-08-24 14:46:55,597 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Start completed.
2025-08-24 14:46:55,597 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Start completed.
2025-08-24 14:46:55,776 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:46:55,776 [canton-env-ec-41] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:46:55,786 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:46:55,787 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:46:55,787 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:46:55,787 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:46:55,787 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:46:55,788 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:46:55,939 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 14:46:55,939 [canton-env-ec-41] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:46:55,948 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:46:55,948 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:46:56,002 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown initiated...
2025-08-24 14:46:56,002 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown initiated...
2025-08-24 14:46:56,012 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-1 - Shutdown completed.
2025-08-24 14:46:56,015 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-2 - Shutdown completed.
2025-08-24 14:46:56,048 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 14:46:56,051 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 14:46:56,092 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 14:46:56,231 [canton-env-ec-41] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 14:46:56,234 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 14:46:56,268 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 14:46:56,289 [canton-env-ec-37] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:46:56,289 [canton-env-ec-67] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:46:56,297 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:46:56,297 [canton-env-ec-41] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:46:56,974 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:46:56,975 [canton-env-ec-41] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:46:57,074 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 14:46:57,074 [canton-env-ec-67] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 14:46:57,382 [canton-env-ec-58] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 14:46:57,396 [canton-env-ec-79] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (13 ms)
2025-08-24 14:46:57,734 [canton-env-ec-67] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 14:46:57,755 [canton-env-ec-35] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:5407e34a5b5e2a86429254a55e5aa5dc - Recovering published timely rejections
2025-08-24 14:46:57,777 [canton-env-ec-58] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:5407e34a5b5e2a86429254a55e5aa5dc - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 14:46:57,781 [canton-env-ec-58] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:5407e34a5b5e2a86429254a55e5aa5dc - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 7)) (exclusive) up to None (inclusive)
2025-08-24 14:46:57,782 [canton-env-ec-79] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 14:46:57,785 [canton-env-ec-79] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:46:57,930 [canton-env-ec-41] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 14:46:58,294 [canton-env-ec-78] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 14:46:58,392 [canton-env-ec-67] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 14:46:58,436 [canton-env-ec-58] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T14:41:35.978548Z on
2025-08-24 14:46:58,455 [canton-env-ec-67] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:46:58,458 [canton-env-ec-67] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:46:58,526 [canton-env-ec-41] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:41:35.978547Z); next counter 21
2025-08-24 14:46:58,537 [canton-env-ec-41] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:46:58,565 [canton-env-ec-67] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:46:58,610 [canton-env-ec-67] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.040s)
2025-08-24 14:46:58,641 [canton-env-ec-35] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:46:58,643 [canton-env-ec-67] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 14:46:58,644 [canton-env-ec-67] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 14:46:58,656 [canton-env-ec-67] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 14:46:58,730 [canton-env-ec-58] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 14:46:58,740 [canton-env-ec-35] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 14:46:58,746 [canton-env-ec-35] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 14:46:58,755 [canton-env-ec-67] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 14:46:58,764 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:46:58,765 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 14:46:58,774 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 14:46:58,775 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 14:46:58,778 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 14:46:58,785 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:46:58,786 [canton-env-ec-67] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 14:46:58,789 [canton-env-ec-104] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 14:46:58,800 [canton-env-ec-67] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:46:58.794001Z
2025-08-24 14:46:58,819 [canton-env-ec-104] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 14:46:58,829 [canton-env-ec-58] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T14:41:35.978548Z on
2025-08-24 14:46:58,858 [canton-env-ec-104] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=21
2025-08-24 14:46:58,862 [canton-env-ec-58] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 14:46:58,862 [canton-env-ec-115] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:41:35.978547Z); next counter 8
2025-08-24 14:46:58,865 [canton-env-ec-115] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:46:58,868 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 14:46:58,904 [canton-env-ec-58] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000008)),0,4)
2025-08-24 14:46:58,914 [canton-env-ec-67] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 14:46:58,919 [canton-env-ec-67] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 14:46:58,948 [canton-env-ec-58] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:46:58.942085Z
2025-08-24 14:46:58,999 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=8
2025-08-24 14:46:59,025 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 14:46:59,528 [canton-env-ec-78] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:510e5755b0b88483db04565bbbc4a17c - 'DOM::mydomain::122060f2dea2...' sends request with id 'tick-0835424e-313b-496e-9d0e-6283d242a62d' of size 338 bytes with 0 envelopes.
2025-08-24 14:46:59,600 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 14:46:59,646 [canton-env-ec-78] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:2d4d8068ea3f01a5955596ca465aa60f - 'MED::mydomain::122060f2dea2...' sends request with id 'tick-2240deeb-6b6b-432a-9259-859566b70192' of size 338 bytes with 0 envelopes.
2025-08-24 14:46:59,728 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 14:46:59,731 [canton-env-ec-116] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (805 ms)
2025-08-24 14:46:59,751 [canton-env-ec-115] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (20 ms)
2025-08-24 14:46:59,754 [canton-env-ec-36] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000008)).
2025-08-24 14:46:59,780 [canton-env-ec-78] INFO  c.d.c.d.m.Mediator:domain=mydomain/node=mediator tid:2d4d8068ea3f01a5955596ca465aa60f - Caught up with batch with counter=9 with sequencer with 87 ms delay
2025-08-24 14:46:59,824 [canton-env-ec-36] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 14:46:59,824 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 14:46:59,875 [canton-env-ec-116] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 14:46:59,923 [canton-env-ec-58] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 14:46:59,950 [canton-env-ec-67] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 14:47:00,115 [canton-env-ec-67] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 14:47:00,116 [canton-env-ec-67] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 14:47:00,116 [canton-env-ec-67] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 14:47:00,276 [canton-env-ec-78] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 14:47:00,279 [canton-env-ec-41] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 14:47:00,366 [canton-env-ec-116] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:e4a5b12f393be2a2be810406044d3138 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:47:01,090 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:56f73157a2d8496b7c6f7842b806761f - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 14:47:01,091 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:04d439db9d103b1d594a3b2d78380f73 - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 14:47:01,091 [canton-env-ec-104] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ea8438219e4342906acda164c99f681e - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 14:47:01,090 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fcdf0357c1b4d2820d856b51c003fa93 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 14:47:01,093 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:369cf707d159712edb0d9ef971f812b3 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 14:47:01,093 [canton-env-ec-78] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:bff9aae9ae13dd38ba3c1ee74ffc1213 - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 14:47:01,092 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:12b08a1367a7d522123c0a9a0181b9e1 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 14:47:01,096 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b4744078117e7cdf14be6c74356f5a58 - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 14:47:01,097 [canton-env-ec-116] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7b6960734457ba8211aaf310636c2960 - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 14:47:01,120 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:bb69d038e9d1fb3deec42855b0098af8 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:47:01,123 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0f5cc81064f4997cae2bed254b48cd37 - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 14:47:01,123 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8d3e10208aaee82b1e941b5dbd6d9104 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 14:47:01,124 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:332d8015e0b4019eb9a290b0bcc8d6ac - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 14:47:01,125 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:db7bb054ddf9cecc5c7f013b1a299ca3 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 14:47:01,120 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c5674787913a2ed9c19971002cc9df3c - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 14:47:01,127 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b69188e5e5402fe96b9c5eddfaa2a61a - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 14:47:01,124 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2861545ed2547ad79a3d488a32dc6758 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 14:47:01,128 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:35e7b86f8aae81c22dae5fc13008bf65 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 14:47:01,129 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:34b8aac44c9e6c3f9376786a5331abf8 - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 14:47:01,122 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:403b5a3433bec41d226a7e7e5219c51c - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 14:47:01,130 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:29386a0d706089cd08f2ca0f5f44c700 - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 14:47:01,131 [canton-env-ec-104] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:5fe944241015ab420918f81c03c6257f - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 14:47:01,132 [canton-env-ec-104] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d1b841bbdb279ce6d4964dd5e96f6b36 - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 14:47:01,134 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:3b6ec8fe12a4df2aaad28f8340680d66 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 14:47:01,134 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:1fbffcb70ff8a11dbce0e169829bfbbe - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 14:47:01,135 [canton-env-ec-116] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fa7757dcf7b1b28b7dbcc8158a6bb4e4 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 14:47:01,121 [canton-env-ec-166] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:85caabd266b184a84a5968fe919473ac - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 14:47:01,136 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d4e45cf8c59977491ec457a0b434c5be - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 14:47:01,148 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:02b39f8b33b6d568473c44f20fe81ec7 - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 14:47:01,130 [canton-env-ec-115] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ad1a2a00c0841356e99ba5b8c15337f8 - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 14:47:01,127 [canton-env-ec-157] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:eeda46ff9d4a229ec7b23b6d978960da - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 14:47:01,127 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c9613dac205a54c0b38011dfae44e532 - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 14:47:01,211 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:de2aa2598ea84254902d33d66861188e - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:47:01,228 [canton-env-ec-58] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:354b85dd17a2f61ac276406c7afacd95 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:47:01,310 [canton-env-ec-58] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:68d77c457951d9c37521211e6a01ca80 - Received request for transactions, startExclusive -> '000000000000000008', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 14:47:01,399 [canton-env-ec-58] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - Reconnecting to domains List(mydomain). Already connected: Set()
2025-08-24 14:47:01,475 [canton-env-ec-36] INFO  c.d.c.c.d.g.SequencerInfoLoader:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - Version handshake with sequencer Sequencer 'DefaultSequencer' and domain using protocol version 7 succeeded.
2025-08-24 14:47:01,659 [canton-env-ec-58] INFO  c.d.c.p.s.SyncDomainEphemeralStateFactoryImpl:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - Computed starting points: ProcessingStartingPoints(
  clean replay = MessageCleanReplayStartingPoint(next request counter = 0, next sequencer counter = 18, prenext timestamp = 2025-08-24T14:41:35.978547Z),
  processing = MessageProcessingStartingPoint(next request counter = 0, next sequencer counter = 18, prenext timestamp = 2025-08-24T14:41:35.978547Z),
  rewound sequencer counter prehead = Some(CursorPrehead(counter = 17, timestamp = 2025-08-24T14:41:35.978547Z))
)
2025-08-24 14:47:01,660 [canton-env-ec-58] INFO  c.d.c.s.d.DbCursorPreheadStore:participant=participant1/domain-alias=mydomain tid:752d26f1ff7e2e00d323d2992329e0e5 - Rewinding prehead to Some(CursorPrehead(counter = 17, timestamp = 2025-08-24T14:41:35.978547Z))
2025-08-24 14:47:01,812 [canton-env-ec-58] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Will use parallelism 8 when computing ACS commitments
2025-08-24 14:47:01,826 [canton-env-ec-36] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Initialized from stored snapshot at RecordTime(timestamp = 0001-01-01T00:00:00Z, tieBreaker = -9223372036854775808) (might be incomplete)
2025-08-24 14:47:01,827 [canton-env-ec-166] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Initialized the ACS commitment processor queue
2025-08-24 14:47:01,830 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,831 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,831 [canton-env-ec-58] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:752d26f1ff7e2e00d323d2992329e0e5 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,832 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,833 [canton-env-ec-58] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,833 [canton-env-ec-58] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - Connected to domain: Domain 'mydomain', without starting synchronisation
2025-08-24 14:47:01,841 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - Starting sync-domains for global reconnect of domains
2025-08-24 14:47:01,868 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiActiveContractsService:participant=participant1 tid:984ae9eadb3b1f16ee395bca13c28d6b - Received request for active contracts: GetActiveContractsRequest(participant1,Some(TransactionFilter(Map(participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d -> Filters(Some(InclusiveFilters(Vector(Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,PingProposal), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Ping), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Pong), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Explode), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Merge), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Collapse)),Vector(),Vector())))))),false,), filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: ['65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Pong', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Collapse', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:PingProposal', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Explode', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Merge', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Ping']}.
2025-08-24 14:47:01,888 [canton-env-ec-165] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:752d26f1ff7e2e00d323d2992329e0e5 - Found 0 repair requests at request counters Seq()
2025-08-24 14:47:01,911 [canton-env-ec-58] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:752d26f1ff7e2e00d323d2992329e0e5 - Processing events from the SequencedEventStore from 2025-08-24T14:41:35.978548Z on
2025-08-24 14:47:01,944 [canton-env-ec-67] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:752d26f1ff7e2e00d323d2992329e0e5 - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:41:35.978547Z); next counter 17
2025-08-24 14:47:01,945 [canton-env-ec-67] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,946 [canton-env-ec-67] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,946 [canton-env-ec-67] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,978 [canton-env-ec-36] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain tid:752d26f1ff7e2e00d323d2992329e0e5 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,979 [canton-env-ec-36] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,981 [canton-env-ec-36] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:47:01,985 [canton-env-ec-37] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - Successfully re-connected to domains List(Domain 'mydomain')
2025-08-24 14:47:01,988 [main] INFO  c.d.c.e.CommunityEnvironment tid:752d26f1ff7e2e00d323d2992329e0e5 - Successfully started all nodes
2025-08-24 14:47:01,996 [canton-env-ec-116] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T15:47:01.993899Z
2025-08-24 14:47:02,006 [canton-env-ec-58] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:752d26f1ff7e2e00d323d2992329e0e5 - PAR::participant1::1220b845dcf0... subscribes from counter=17
2025-08-24 14:47:02,433 [canton-env-ec-41] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:752d26f1ff7e2e00d323d2992329e0e5 - 'PAR::participant1::1220b845dcf0...' sends request with id 'tick-8d193eb4-a0c4-4cf2-80ba-270a6b6293a2' of size 342 bytes with 0 envelopes.
2025-08-24 14:47:02,488 [canton-env-ec-58] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:752d26f1ff7e2e00d323d2992329e0e5 - Caught up with batch with counter=18 with sequencer with 43 ms delay
2025-08-24 14:47:02,489 [canton-env-ec-58] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::122060f2dea2 tid:752d26f1ff7e2e00d323d2992329e0e5 - Replaying requests 0 up to clean prehead -1
2025-08-24 14:47:02,489 [canton-env-ec-58] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::122060f2dea2 tid:752d26f1ff7e2e00d323d2992329e0e5 - Replaying or processing locally stored events with sequencer counters 18 to 17
2025-08-24 14:48:38,711 [slick-mydomain-3 housekeeper] WARN  com.zaxxer.hikari.pool.HikariPool - slick-mydomain-3 - Thread starvation or clock leap detected (housekeeper delta=1m12s518ms442s911ns).
2025-08-24 14:48:38,712 [daml.index.db.connection.indexer housekeeper] WARN  com.zaxxer.hikari.pool.HikariPool - daml.index.db.connection.indexer - Thread starvation or clock leap detected (housekeeper delta=1m9s790ms68s569ns).
2025-08-24 14:48:38,712 [slick-participant1-4 housekeeper] WARN  com.zaxxer.hikari.pool.HikariPool - slick-participant1-4 - Thread starvation or clock leap detected (housekeeper delta=1m12s343ms9s384ns).
2025-08-24 14:48:38,711 [daml.index.db.connection.api-server housekeeper] WARN  com.zaxxer.hikari.pool.HikariPool - daml.index.db.connection.api-server - Thread starvation or clock leap detected (housekeeper delta=1m9s820ms164s981ns).
2025-08-24 14:48:38,716 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 14:48:38,714 [canton-env-sched-0] WARN  c.d.c.c.ExecutionContextMonitor - Task runner canton-env-ec is stuck or overloaded for 1 millisecond. (queue-size=0).
ForkJoinIdlenessExecutorService-canton-env-ec: java.util.concurrent.ForkJoinPool@33fc7560[Running, parallelism = 8, size = 18, active = 4, running = 3, steals = 3606, tasks = 0, submissions = 0]
2025-08-24 14:48:38,719 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 14:48:38,727 [canton-env-sched-0] INFO  c.d.c.c.ExecutionContextMonitor - Here is the stack-trace of threads for canton-env-ec:
  Thread[#115,canton-env-ec-115,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#162,canton-env-ec-162,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#67,canton-env-ec-67,5,main] is-daemon=true state=WAITING
    app//com.digitalasset.canton.crypto.Hash$.tryCreate(Hash.scala:139)
    app//com.digitalasset.canton.crypto.HashBuilderFromMessageDigest.$anonfun$finish$1(HashBuilder.scala:104)
    app//com.digitalasset.canton.crypto.HashBuilderFromMessageDigest$$Lambda/0x00007a79bcb43378.apply(Unknown Source)
    app//com.digitalasset.canton.package$.checked(package.scala:118)
    app//com.digitalasset.canton.crypto.HashBuilderFromMessageDigest.finish(HashBuilder.scala:104)
    app//com.digitalasset.canton.crypto.Hash$.digest(Hash.scala:159)
    app//com.digitalasset.canton.crypto.HashOps.digest(Hash.scala:221)
    app//com.digitalasset.canton.crypto.HashOps.digest$(Hash.scala:216)
    app//com.digitalasset.canton.crypto.provider.tink.TinkPureCrypto.digest(TinkPureCrypto.scala:34)
    app//com.digitalasset.canton.sequencing.protocol.SignedContent$.hashContent(SignedContent.scala:176)
    app//com.digitalasset.canton.sequencing.protocol.SignedContent$.create(SignedContent.scala:165)
    app//com.digitalasset.canton.sequencing.client.RequestSigner$$anon$1.signRequest(RequestSigner.scala:64)
    app//com.digitalasset.canton.sequencing.client.SequencerClientImpl.acknowledgeSigned(SequencerClient.scala:1063)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements$.$anonfun$create$1(PeriodicAcknowledgements.scala:127)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements$$$Lambda/0x00007a79bcf3a138.apply(Unknown Source)
    app//com.digitalasset.canton.tracing.Traced$.$anonfun$lift$1(Traced.scala:42)
    app//com.digitalasset.canton.tracing.Traced$$$Lambda/0x00007a79bcc44800.apply(Unknown Source)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements.ackIfChanged$1(PeriodicAcknowledgements.scala:67)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements.$anonfun$update$7(PeriodicAcknowledgements.scala:85)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements$$Lambda/0x00007a79bcf3d0b8.apply(Unknown Source)
    app//scala.Option.fold(Option.scala:263)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements.$anonfun$update$5(PeriodicAcknowledgements.scala:85)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements$$Lambda/0x00007a79bcf3b7c8.apply(Unknown Source)
    app//scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)
    app//com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
    java.base@21.0.8/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
    java.base@21.0.8/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#78,canton-env-ec-78,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#35,canton-env-ec-35,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#163,canton-env-ec-163,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#116,canton-env-ec-116,5,main] is-daemon=true state=RUNNABLE
    java.base@21.0.8/java.lang.invoke.MethodHandleNatives.resolve(Native Method)
    java.base@21.0.8/java.lang.invoke.MemberName$Factory.resolve(MemberName.java:962)
    java.base@21.0.8/java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:991)
    java.base@21.0.8/java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:3750)
    java.base@21.0.8/java.lang.invoke.MethodHandles$Lookup.findConstructor(MethodHandles.java:2837)
    java.base@21.0.8/java.lang.invoke.InnerClassLambdaMetafactory.buildCallSite(InnerClassLambdaMetafactory.java:232)
    java.base@21.0.8/java.lang.invoke.LambdaMetafactory.metafactory(LambdaMetafactory.java:341)
    java.base@21.0.8/java.lang.invoke.LambdaForm$DMH/0x00007a79bc1f9000.invokeStatic(LambdaForm$DMH)
    java.base@21.0.8/java.lang.invoke.Invokers$Holder.invokeExact_MT(Invokers$Holder)
    java.base@21.0.8/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:134)
    java.base@21.0.8/java.lang.invoke.CallSite.makeSite(CallSite.java:316)
    java.base@21.0.8/java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:274)
    java.base@21.0.8/java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:264)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.evictEntry(BoundedLocalCache.java:1011)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.expireAfterAccessEntries(BoundedLocalCache.java:918)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.expireAfterAccessEntries(BoundedLocalCache.java:904)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.expireEntries(BoundedLocalCache.java:882)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.maintenance(BoundedLocalCache.java:1667)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1636)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask.run(BoundedLocalCache.java:3824)
    app//com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
    java.base@21.0.8/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
    java.base@21.0.8/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#161,canton-env-ec-161,5,main] is-daemon=true state=RUNNABLE
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#157,canton-env-ec-157,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#104,canton-env-ec-104,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#166,canton-env-ec-166,5,main] is-daemon=true state=RUNNABLE
    java.base@21.0.8/java.lang.ClassLoader.defineClass0(Native Method)
    java.base@21.0.8/java.lang.System$2.defineClass(System.java:2394)
    java.base@21.0.8/java.lang.invoke.MethodHandles$Lookup$ClassDefiner.defineClass(MethodHandles.java:2505)
    java.base@21.0.8/java.lang.invoke.InnerClassLambdaMetafactory.generateInnerClass(InnerClassLambdaMetafactory.java:361)
    java.base@21.0.8/java.lang.invoke.InnerClassLambdaMetafactory.spinInnerClass(InnerClassLambdaMetafactory.java:286)
    java.base@21.0.8/java.lang.invoke.InnerClassLambdaMetafactory.buildCallSite(InnerClassLambdaMetafactory.java:221)
    java.base@21.0.8/java.lang.invoke.LambdaMetafactory.metafactory(LambdaMetafactory.java:341)
    java.base@21.0.8/java.lang.invoke.LambdaForm$DMH/0x00007a79bc1f9000.invokeStatic(LambdaForm$DMH)
    java.base@21.0.8/java.lang.invoke.Invokers$Holder.invokeExact_MT(Invokers$Holder)
    java.base@21.0.8/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:134)
    java.base@21.0.8/java.lang.invoke.CallSite.makeSite(CallSite.java:316)
    java.base@21.0.8/java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:274)
    java.base@21.0.8/java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:264)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.evictEntry(BoundedLocalCache.java:1011)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.expireAfterAccessEntries(BoundedLocalCache.java:918)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.expireAfterAccessEntries(BoundedLocalCache.java:904)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.expireEntries(BoundedLocalCache.java:882)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.maintenance(BoundedLocalCache.java:1667)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1636)
    app//com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask.run(BoundedLocalCache.java:3824)
    app//com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
    java.base@21.0.8/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
    java.base@21.0.8/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#36,canton-env-ec-36,5,main] is-daemon=true state=RUNNABLE
    java.base@21.0.8/java.lang.Long.numberOfLeadingZeros(Long.java:1802)
    java.base@21.0.8/java.lang.Long.toUnsignedString0(Long.java:393)
    java.base@21.0.8/java.lang.Long.toHexString(Long.java:311)
    java.base@21.0.8/java.util.Formatter$FormatSpecifier.print(Formatter.java:3397)
    java.base@21.0.8/java.util.Formatter$FormatSpecifier.print(Formatter.java:3340)
    java.base@21.0.8/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:3056)
    java.base@21.0.8/java.util.Formatter$FormatSpecifier.print(Formatter.java:3021)
    java.base@21.0.8/java.util.Formatter.format(Formatter.java:2791)
    java.base@21.0.8/java.util.Formatter.format(Formatter.java:2728)
    java.base@21.0.8/java.lang.String.format(String.java:4390)
    app//scala.collection.StringOps$.format$extension(StringOps.scala:989)
    app//com.digitalasset.canton.util.HexString$.$anonfun$toHexString$1(HexString.scala:25)
    app//com.digitalasset.canton.util.HexString$.$anonfun$toHexString$1$adapted(HexString.scala:25)
    app//com.digitalasset.canton.util.HexString$$$Lambda/0x00007a79bc8f93b8.apply(Unknown Source)
    app//scala.collection.ArrayOps$.map$extension(ArrayOps.scala:940)
    app//com.digitalasset.canton.util.HexString$.toHexString(HexString.scala:25)
    app//com.digitalasset.canton.util.HexString$.toHexString(HexString.scala:11)
    app//com.digitalasset.canton.crypto.Hash.<init>(Hash.scala:98)
    app//com.digitalasset.canton.crypto.Hash$.$anonfun$create$1(Hash.scala:144)
    app//com.digitalasset.canton.crypto.Hash$$$Lambda/0x00007a79bcb43cb0.apply(Unknown Source)
    app//scala.util.Either$.cond(Either.scala:500)
    app//com.digitalasset.canton.crypto.Hash$.create(Hash.scala:145)
    app//com.digitalasset.canton.crypto.Hash$.tryCreate(Hash.scala:139)
    app//com.digitalasset.canton.crypto.HashBuilderFromMessageDigest.$anonfun$finish$1(HashBuilder.scala:104)
    app//com.digitalasset.canton.crypto.HashBuilderFromMessageDigest$$Lambda/0x00007a79bcb43378.apply(Unknown Source)
    app//com.digitalasset.canton.package$.checked(package.scala:118)
    app//com.digitalasset.canton.crypto.HashBuilderFromMessageDigest.finish(HashBuilder.scala:104)
    app//com.digitalasset.canton.crypto.Hash$.digest(Hash.scala:159)
    app//com.digitalasset.canton.crypto.HashOps.digest(Hash.scala:221)
    app//com.digitalasset.canton.crypto.HashOps.digest$(Hash.scala:216)
    app//com.digitalasset.canton.crypto.provider.tink.TinkPureCrypto.digest(TinkPureCrypto.scala:34)
    app//com.digitalasset.canton.sequencing.protocol.SignedContent$.hashContent(SignedContent.scala:176)
    app//com.digitalasset.canton.sequencing.protocol.SignedContent$.create(SignedContent.scala:165)
    app//com.digitalasset.canton.sequencing.client.RequestSigner$$anon$1.signRequest(RequestSigner.scala:64)
    app//com.digitalasset.canton.sequencing.client.SequencerClientImpl.acknowledgeSigned(SequencerClient.scala:1063)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements$.$anonfun$create$1(PeriodicAcknowledgements.scala:127)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements$$$Lambda/0x00007a79bcf3a138.apply(Unknown Source)
    app//com.digitalasset.canton.tracing.Traced$.$anonfun$lift$1(Traced.scala:42)
    app//com.digitalasset.canton.tracing.Traced$$$Lambda/0x00007a79bcc44800.apply(Unknown Source)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements.ackIfChanged$1(PeriodicAcknowledgements.scala:67)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements.$anonfun$update$7(PeriodicAcknowledgements.scala:85)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements$$Lambda/0x00007a79bcf3d0b8.apply(Unknown Source)
    app//scala.Option.fold(Option.scala:263)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements.$anonfun$update$5(PeriodicAcknowledgements.scala:85)
    app//com.digitalasset.canton.sequencing.client.PeriodicAcknowledgements$$Lambda/0x00007a79bcf3b7c8.apply(Unknown Source)
    app//scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)
    app//com.daml.executors.QueueAwareExecutorService$TrackingRunnable.run(QueueAwareExecutorService.scala:98)
    java.base@21.0.8/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)
    java.base@21.0.8/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#79,canton-env-ec-79,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#165,canton-env-ec-165,5,main] is-daemon=true state=RUNNABLE
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#164,canton-env-ec-164,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#37,canton-env-ec-37,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#58,canton-env-ec-58,5,main] is-daemon=true state=WAITING
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

  Thread[#41,canton-env-ec-41,5,main] is-daemon=true state=RUNNABLE
    java.base@21.0.8/jdk.internal.misc.Unsafe.park(Native Method)
    java.base@21.0.8/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1893)
    java.base@21.0.8/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1809)
    java.base@21.0.8/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

2025-08-24 14:48:38,731 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,734 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,736 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,738 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,757 [canton-env-ec-165] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 14:48:38,767 [canton-env-ec-67] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:68d77c457951d9c37521211e6a01ca80 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:55278: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:48:38,786 [canton-env-ec-41] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 14:48:38,798 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 14:48:38,800 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 14:48:38,801 [canton-env-ec-37] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 14:48:38,802 [canton-env-ec-37] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 14:48:38,803 [canton-env-ec-37] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 14:48:38,805 [canton-env-ec-67] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 14:48:38,818 [canton-env-ec-67] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 14:48:38,818 [canton-env-ec-67] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 14:48:38,819 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 14:48:38,820 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 14:48:38,821 [canton-env-ec-36] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 14:48:38,821 [canton-env-ec-36] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 14:48:38,823 [canton-env-ec-166] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 14:48:38,827 [canton-env-ec-166] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 14:48:38,832 [canton-env-ec-165] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,833 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,838 [canton-env-ec-165] INFO  c.d.c.s.c.ResilientSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,839 [canton-env-ec-165] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,839 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,841 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:48:38,842 [canton-env-ec-37] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:752d26f1ff7e2e00d323d2992329e0e5 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:44112: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:48:38,846 [canton-env-ec-116] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:752d26f1ff7e2e00d323d2992329e0e5 - Domain 'mydomain' disconnected because sequencer client was closed
2025-08-24 14:48:38,848 [canton-env-ec-165] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,848 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,853 [canton-env-ec-165] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,854 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,866 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 14:48:38,866 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 14:48:38,867 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 14:48:38,867 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 14:48:38,868 [canton-env-ec-165] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,868 [canton-env-ec-165] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:48:38,870 [canton-env-ec-165] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 14:48:38,874 [canton-env-ec-165] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 14:48:38,875 [canton-env-ec-165] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,876 [canton-env-ec-165] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,880 [canton-env-ec-165] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 14:48:38,886 [canton-env-ec-165] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,887 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,889 [canton-env-ec-165] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,890 [canton-env-ec-165] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,890 [canton-env-ec-41] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:48:38,891 [canton-env-ec-41] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:f3218b2d5147127d245eb121bfb5679a - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:44012: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:48:38,897 [canton-env-ec-165] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,897 [canton-env-ec-165] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 14:48:38,899 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 14:48:38,899 [canton-env-ec-161] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:692e1043125f8415c8ffad5917128f14 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:44040: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 14:48:38,907 [canton-env-ec-165] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:48:38,909 [canton-env-ec-165] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 14:48:38,909 [canton-env-ec-165] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:48:38,924 [canton-env-ec-165] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,925 [canton-env-ec-165] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 14:48:38,926 [canton-env-ec-165] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 14:48:38,936 [canton-env-ec-165] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 14:48:38,937 [canton-env-ec-165] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,938 [canton-env-ec-165] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 14:48:38,966 [canton-env-ec-67] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 14:48:38,995 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 14:48:38,995 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 14:49:13,753 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 14:49:15,031 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=no
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 14:49:15,053 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 14:49:15,273 [canton-env-ec-36] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 14:49:15,346 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 14:49:15,405 [main] INFO  c.d.c.e.CommunityEnvironment tid:4f277dbdf0882443ba642f66cd07aa19 - Automatically starting all instances
2025-08-24 14:49:15,456 [canton-env-ec-39] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 14:49:15,456 [canton-env-ec-36] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 14:49:15,477 [canton-env-ec-36] INFO  c.d.c.resource.DbStorage:mydomain tid:28c4a30297bcfb839238c09682a8c34b - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:49:15,478 [canton-env-ec-39] INFO  c.d.c.r.DbStorage:participant1 tid:28ef2e88a34589817536e347ca7bf649 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 14:49:15,504 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 14:49:15,504 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 14:49:15,992 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Start completed.
2025-08-24 14:49:15,992 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Start completed.
2025-08-24 14:49:16,172 [canton-env-ec-39] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:49:16,172 [canton-env-ec-36] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 14:49:16,184 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:49:16,184 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:49:16,184 [canton-env-ec-36] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:49:16,184 [canton-env-ec-39] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 14:49:16,185 [canton-env-ec-39] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 14:49:16,185 [canton-env-ec-39] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 14:49:16,291 [canton-env-ec-39] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:49:16,291 [canton-env-ec-36] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 14:49:16,300 [canton-env-ec-39] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:49:16,300 [canton-env-ec-36] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:49:16,360 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown initiated...
2025-08-24 14:49:16,360 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown initiated...
2025-08-24 14:49:16,366 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown completed.
2025-08-24 14:49:16,374 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown completed.
2025-08-24 14:49:16,412 [canton-env-ec-36] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 14:49:16,415 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 14:49:16,455 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 14:49:16,552 [canton-env-ec-39] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 14:49:16,554 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 14:49:16,578 [canton-env-ec-39] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 14:49:16,669 [canton-env-ec-67] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:16,669 [canton-env-ec-69] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:16,677 [canton-env-ec-39] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:49:16,677 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 14:49:17,254 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:49:17,263 [canton-env-ec-39] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:49:17,373 [canton-env-ec-36] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 14:49:17,373 [canton-env-ec-68] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 14:49:17,747 [canton-env-ec-65] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 14:49:17,774 [canton-env-ec-66] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (25 ms)
2025-08-24 14:49:17,998 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:5b0e651a5d84e49ad4b2319e7039e448 - Recovering published timely rejections
2025-08-24 14:49:18,019 [canton-env-ec-39] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:5b0e651a5d84e49ad4b2319e7039e448 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 14:49:18,021 [canton-env-ec-39] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:5b0e651a5d84e49ad4b2319e7039e448 - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 7)) (exclusive) up to None (inclusive)
2025-08-24 14:49:18,053 [canton-env-ec-37] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 14:49:18,098 [canton-env-ec-69] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 14:49:18,101 [canton-env-ec-69] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 14:49:18,143 [canton-env-ec-36] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 14:49:18,488 [canton-env-ec-37] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 14:49:18,595 [canton-env-ec-69] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 14:49:18,678 [canton-env-ec-69] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 14:49:18,681 [canton-env-ec-69] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:49:18,749 [canton-env-ec-39] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T14:46:59.655414Z on
2025-08-24 14:49:18,784 [canton-env-ec-69] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 14:49:18,820 [canton-env-ec-69] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.031s)
2025-08-24 14:49:18,833 [canton-env-ec-36] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:46:59.655413Z); next counter 22
2025-08-24 14:49:18,846 [canton-env-ec-36] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:18,859 [canton-env-ec-69] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 14:49:18,860 [canton-env-ec-69] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 14:49:18,864 [canton-env-ec-69] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 14:49:18,913 [canton-env-ec-67] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 14:49:18,925 [canton-env-ec-39] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 14:49:18,931 [canton-env-ec-39] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 14:49:18,950 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 14:49:18,952 [canton-env-ec-105] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:18,958 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:49:18,959 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 14:49:18,965 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 14:49:18,965 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 14:49:18,968 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 14:49:18,974 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 14:49:18,977 [canton-env-ec-68] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 14:49:18,978 [canton-env-ec-38] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 14:49:19,002 [canton-env-ec-68] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 14:49:19,035 [canton-env-ec-67] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 14:49:19,044 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 14:49:19,071 [canton-env-ec-105] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 14:49:19,072 [canton-env-ec-67] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000008)),0,4)
2025-08-24 14:49:19,073 [canton-env-ec-66] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 14:49:19,085 [canton-env-ec-38] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:49:19.079098Z
2025-08-24 14:49:19,115 [canton-env-ec-67] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T14:46:59.693257Z on
2025-08-24 14:49:19,136 [canton-env-ec-38] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=22
2025-08-24 14:49:19,158 [canton-env-ec-39] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:46:59.693256Z); next counter 9
2025-08-24 14:49:19,161 [canton-env-ec-39] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:19,175 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 14:49:19,638 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 14:49:19,694 [canton-env-ec-39] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:4e50569cc1337d6f592204b7ab68ff3a - 'DOM::mydomain::122060f2dea2...' sends request with id 'tick-55fb672f-b658-4cc7-8492-c6e6bf6886b6' of size 339 bytes with 0 envelopes.
2025-08-24 14:49:19,751 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 14:49:19,761 [canton-env-ec-105] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T15:49:19.745674Z
2025-08-24 14:49:19,786 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:36cd05f09027ec76073cea073b388b2a - 'MED::mydomain::122060f2dea2...' sends request with id 'tick-3e9e9241-f92a-41bc-9213-362bc162fa29' of size 339 bytes with 0 envelopes.
2025-08-24 14:49:19,787 [canton-env-ec-105] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=9
2025-08-24 14:49:19,846 [canton-env-ec-37] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (753 ms)
2025-08-24 14:49:19,852 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 5/3000. Retrying again in 100 milliseconds.
2025-08-24 14:49:19,864 [canton-env-ec-105] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (17 ms)
2025-08-24 14:49:19,867 [canton-env-ec-155] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000008)).
2025-08-24 14:49:19,950 [canton-env-ec-68] INFO  c.d.c.d.m.Mediator:domain=mydomain/node=mediator tid:36cd05f09027ec76073cea073b388b2a - Caught up with batch with counter=10 with sequencer with 100 ms delay
2025-08-24 14:49:19,964 [canton-env-ec-66] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 14:49:19,964 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 14:49:20,003 [canton-env-ec-69] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 14:49:20,041 [canton-env-ec-37] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 14:49:20,061 [canton-env-ec-37] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 14:49:20,212 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 14:49:20,213 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 14:49:20,213 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 14:49:20,355 [canton-env-ec-105] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 14:49:20,357 [canton-env-ec-36] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 14:49:20,450 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:0ac187ae4db769d43b6af1546935f2c1 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:49:21,220 [canton-env-ec-155] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b026f2f08355a635264d75af0ad24682 - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 14:49:21,228 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:80ce4fff3de85d7ef7a72e6a58846bef - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 14:49:21,230 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:acbdcdbc9ec66c9b5ac7ded856965fdd - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 14:49:21,232 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:640a9a85e19b3039daca3727557b8e84 - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 14:49:21,220 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0ce4921b8980161f0a5c6331789cd3bd - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 14:49:21,229 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:bfcb4207802cc77452dcaced5779b321 - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 14:49:21,237 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a500a5229ab7051599c6a34b44cc498d - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 14:49:21,238 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:081099415489abcba5452913f88c404a - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 14:49:21,240 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:47cdd18024d2acc3021f98851c587cf7 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 14:49:21,243 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a81b9ce8bc0d835f92c71587b8b4c0ab - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 14:49:21,244 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:3fd4484d17792747bff2fff6c6435f6f - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 14:49:21,244 [canton-env-ec-168] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:40fc8b043594c207c7f43abde619c896 - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 14:49:21,245 [canton-env-ec-39] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c97a9980b83faf7408572f5362c8e933 - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 14:49:21,246 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:191984555cf9a5836326778eb13328ba - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 14:49:21,247 [canton-env-ec-67] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9b28d89a71f356683e217e18e7662ba3 - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 14:49:21,249 [canton-env-ec-169] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:88c93d717fc36c38ca6c602233b506be - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 14:49:21,251 [canton-env-ec-163] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:ea2e2fea015b808149813d0ee97fc6d7 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 14:49:21,242 [canton-env-ec-105] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:deb981d2e6a1078ebba550dbf7172a00 - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 14:49:21,242 [canton-env-ec-165] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:41a045b4274e23e46f3286da201c44ce - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 14:49:21,253 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b7d3e179a851f5254f165b6c191d19f9 - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 14:49:21,240 [canton-env-ec-155] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:08b3b840bd422f23f1ce37f6fce1305f - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:49:21,240 [canton-env-ec-160] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7fca2bf351279f407c3d2b4118499983 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 14:49:21,239 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2b446f74e915bbc7ad2202e26d51aaf9 - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 14:49:21,264 [canton-env-ec-37] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9d2e1ae0e9c86b80a41bc001a0436475 - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 14:49:21,268 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c82b58c029e3db266591162c81c9a7f7 - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 14:49:21,259 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9269fcb2d356289764257631c48555ca - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 14:49:21,255 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:250e36be9c0810797143fa1ee9131ac6 - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 14:49:21,249 [canton-env-ec-69] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7336735a618f2e615d88453ce4614c79 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 14:49:21,270 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:6f340bc63ae605494b1b2f773c244f0b - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 14:49:21,248 [canton-env-ec-168] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:43c70fca72635ca347a894b47abf4af5 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 14:49:21,246 [canton-env-ec-38] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d1577bb50822045ae6ee775463f826ef - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 14:49:21,273 [canton-env-ec-39] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:65c10cc99972754ad5a32e70826343d5 - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 14:49:21,386 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:b06757a63fb476889cef0db8cc235890 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 14:49:21,407 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:49492209510a79c3ea31ebd05750692c - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 14:49:21,514 [canton-env-ec-69] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:54f83ec1620a05699ae16e4c4c753c10 - Received request for transactions, startExclusive -> '000000000000000008', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 14:49:21,587 [canton-env-ec-69] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - Reconnecting to domains List(mydomain). Already connected: Set()
2025-08-24 14:49:21,679 [canton-env-ec-168] INFO  c.d.c.c.d.g.SequencerInfoLoader:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - Version handshake with sequencer Sequencer 'DefaultSequencer' and domain using protocol version 7 succeeded.
2025-08-24 14:49:21,910 [canton-env-ec-37] INFO  c.d.c.p.s.SyncDomainEphemeralStateFactoryImpl:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - Computed starting points: ProcessingStartingPoints(
  clean replay = MessageCleanReplayStartingPoint(next request counter = 0, next sequencer counter = 19, prenext timestamp = 2025-08-24T14:47:02.445483Z),
  processing = MessageProcessingStartingPoint(next request counter = 0, next sequencer counter = 19, prenext timestamp = 2025-08-24T14:47:02.445483Z),
  rewound sequencer counter prehead = Some(CursorPrehead(counter = 18, timestamp = 2025-08-24T14:47:02.445483Z))
)
2025-08-24 14:49:21,911 [canton-env-ec-37] INFO  c.d.c.s.d.DbCursorPreheadStore:participant=participant1/domain-alias=mydomain tid:4f277dbdf0882443ba642f66cd07aa19 - Rewinding prehead to Some(CursorPrehead(counter = 18, timestamp = 2025-08-24T14:47:02.445483Z))
2025-08-24 14:49:22,099 [canton-env-ec-168] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Will use parallelism 8 when computing ACS commitments
2025-08-24 14:49:22,111 [canton-env-ec-161] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Last computed and sent timestamp: 2025-08-24T14:47:00Z
2025-08-24 14:49:22,116 [canton-env-ec-161] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Initialized from stored snapshot at RecordTime(timestamp = 0001-01-01T00:00:00Z, tieBreaker = -9223372036854775808) (might be incomplete)
2025-08-24 14:49:22,123 [canton-env-ec-168] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,123 [canton-env-ec-37] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Initialized the ACS commitment processor queue
2025-08-24 14:49:22,123 [canton-env-ec-168] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,124 [canton-env-ec-168] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:4f277dbdf0882443ba642f66cd07aa19 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,125 [canton-env-ec-168] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,125 [canton-env-ec-168] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,125 [canton-env-ec-168] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - Connected to domain: Domain 'mydomain', without starting synchronisation
2025-08-24 14:49:22,130 [canton-env-ec-69] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - Starting sync-domains for global reconnect of domains
2025-08-24 14:49:22,156 [canton-env-ec-68] INFO  c.d.c.p.a.s.ApiActiveContractsService:participant=participant1 tid:2b633aa10aa4e3588e6d2207e33becf1 - Received request for active contracts: GetActiveContractsRequest(participant1,Some(TransactionFilter(Map(participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d -> Filters(Some(InclusiveFilters(Vector(Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,PingProposal), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Ping), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Pong), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Explode), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Merge), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Collapse)),Vector(),Vector())))))),false,), filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: ['65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Pong', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Collapse', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:PingProposal', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Explode', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Merge', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Ping']}.
2025-08-24 14:49:22,184 [canton-env-ec-164] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:4f277dbdf0882443ba642f66cd07aa19 - Found 0 repair requests at request counters Seq()
2025-08-24 14:49:22,208 [canton-env-ec-164] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:4f277dbdf0882443ba642f66cd07aa19 - Processing events from the SequencedEventStore from 2025-08-24T14:47:02.445484Z on
2025-08-24 14:49:22,233 [canton-env-ec-38] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:4f277dbdf0882443ba642f66cd07aa19 - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:47:02.445483Z); next counter 18
2025-08-24 14:49:22,234 [canton-env-ec-38] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,235 [canton-env-ec-38] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,235 [canton-env-ec-38] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,264 [canton-env-ec-37] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain tid:4f277dbdf0882443ba642f66cd07aa19 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,265 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,265 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 14:49:22,268 [canton-env-ec-69] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - Successfully re-connected to domains List(Domain 'mydomain')
2025-08-24 14:49:22,271 [main] INFO  c.d.c.e.CommunityEnvironment tid:4f277dbdf0882443ba642f66cd07aa19 - Successfully started all nodes
2025-08-24 14:49:22,291 [canton-env-ec-65] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T15:49:22.288597Z
2025-08-24 14:49:22,299 [canton-env-ec-38] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:4f277dbdf0882443ba642f66cd07aa19 - PAR::participant1::1220b845dcf0... subscribes from counter=18
2025-08-24 14:49:22,740 [canton-env-ec-168] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:4f277dbdf0882443ba642f66cd07aa19 - 'PAR::participant1::1220b845dcf0...' sends request with id 'tick-01d2d6dc-47c5-47d0-83d6-f33896b87415' of size 343 bytes with 0 envelopes.
2025-08-24 14:49:22,795 [canton-env-ec-168] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:4f277dbdf0882443ba642f66cd07aa19 - Caught up with batch with counter=19 with sequencer with 46 ms delay
2025-08-24 14:49:22,796 [canton-env-ec-168] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::122060f2dea2 tid:4f277dbdf0882443ba642f66cd07aa19 - Replaying requests 0 up to clean prehead -1
2025-08-24 14:49:22,796 [canton-env-ec-168] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::122060f2dea2 tid:4f277dbdf0882443ba642f66cd07aa19 - Replaying or processing locally stored events with sequencer counters 19 to 18
2025-08-24 14:49:29,991 [canton-env-ec-37] INFO  c.d.c.p.t.ParticipantTopologyManager:participant=participant1 tid:22a8f0906f81e759c744dba2d896aff2 - TOPOLOGY_MAPPING_ALREADY_EXISTS(10,22a8f090): A matching topology mapping authorized with the same key already exists in this state err-context:{authKey=1220b845dcf0..., existing=TopologyStateUpdateElement(id = DXyvjd9GR7fZzrxqplzPzYkDf7cluonF, mapping = PartyToParticipant(Both, NewBank::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission)), location=TopologyManager.scala:298} 
2025-08-24 14:49:30,047 [canton-env-ec-37] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:22a8f0906f81e759c744dba2d896aff2 - Request c.d.c.t.a.v.TopologyManagerWriteService/AuthorizePartyToParticipant by /[0:0:0:0:0:0:0:1%0]:42276: failed with ALREADY_EXISTS/TOPOLOGY_MAPPING_ALREADY_EXISTS(10,22a8f090): A matching topology mapping authorized with the same key already exists in this state
2025-08-24 14:49:30,069 [main] ERROR c.d.c.e.CommunityConsoleEnvironment - Request failed for participant1.
  GrpcRequestRefusedByServer: ALREADY_EXISTS/TOPOLOGY_MAPPING_ALREADY_EXISTS(10,22a8f090): A matching topology mapping authorized with the same key already exists in this state
  Request: AuthorizePartyToParticipant(Add,None,Both,NewBank::1220b845dcf0...,PAR::participant1::1220b845dcf0...,Submission,false,false)
  CorrelationId: 22a8f0906f81e759c744dba2d896aff2
  Context: HashMap(participant -> participant1, authKey -> 1220b845dcf0..., existing -> TopologyStateUpdateElement(id = DXyvjd9GR7fZzrxqplzPzYkDf7cluonF, mapping = PartyToParticipant(Both, NewBank::1220b845dcf0..., PAR::participant1::1220b845dcf0..., Submission)), tid -> 22a8f0906f81e759c744dba2d896aff2)
  Command ParticipantPartiesAdministrationGroup.enable invoked from cmd0.sc:1
2025-08-24 15:01:19,040 [daml.index.db.threadpool.connection.indexer-6] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes()),2025-08-24T15:00:00Z)
2025-08-24 15:01:19,044 [daml.index.db.threadpool.connection.indexer-6] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes()),2025-08-24T15:00:00Z)
2025-08-24 15:07:21,374 [main] ERROR c.d.c.c.LocalParticipantReference:participant=participant1 - The command is currently disabled. You need to enable it explicitly by setting `canton.features.enable-testing-commands = yes` in your Canton configuration file (`.conf`)
2025-08-24 15:07:43,298 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 15:07:43,301 [Thread-0] INFO  c.d.c.e.CommunityEnvironment - Closing environment...
2025-08-24 15:07:43,309 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,311 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,312 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,313 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,328 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNode:participant=participant1 - Stopping participant node
2025-08-24 15:07:43,340 [canton-env-ec-161] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:54f83ec1620a05699ae16e4c4c753c10 - Request c.d.l.a.v.TransactionService/GetTransactions by /[0:0:0:0:0:0:0:1%0]:47098: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 15:07:43,358 [canton-env-ec-168] INFO  c.d.c.p.a.s.c.CommandServiceImpl:participant=participant1 - Shutting down Command Service.
2025-08-24 15:07:43,379 [canton-env-ec-167] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown initiated...
2025-08-24 15:07:43,382 [canton-env-ec-167] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Shutdown completed.
2025-08-24 15:07:43,383 [canton-env-ec-168] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopping Indexer Server
2025-08-24 15:07:43,384 [canton-env-ec-168] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown called!
2025-08-24 15:07:43,384 [canton-env-ec-168] INFO  c.d.c.p.i.h.KillSwitchCaptor:participant=participant1 - Shutdown call delegated!
2025-08-24 15:07:43,387 [canton-env-ec-155] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown initiated...
2025-08-24 15:07:43,406 [canton-env-ec-155] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Shutdown completed.
2025-08-24 15:07:43,406 [canton-env-ec-155] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexing finished.
2025-08-24 15:07:43,407 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepping down as leader, stopping DB connectivity polling
2025-08-24 15:07:43,408 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Stepped down as leader, IndexDB HA Coordinator shut down
2025-08-24 15:07:43,408 [canton-env-ec-167] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Successfully finished processing state updates
2025-08-24 15:07:43,408 [canton-env-ec-167] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Stopped Indexer Server
2025-08-24 15:07:43,411 [canton-env-ec-65] INFO  c.d.c.p.DispatcherState:participant=participant1 - Shutting down Ledger API offset dispatcher state.
2025-08-24 15:07:43,419 [canton-env-ec-65] INFO  c.d.c.p.DispatcherState:participant=participant1 - Ledger API offset dispatcher shutdown.
2025-08-24 15:07:43,426 [canton-env-ec-37] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,427 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,434 [canton-env-ec-37] INFO  c.d.c.s.c.ResilientSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,434 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,435 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,439 [canton-env-ec-164] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - GRPC subscription successfully closed due to client shutdown.
2025-08-24 15:07:43,439 [canton-env-ec-168] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:4f277dbdf0882443ba642f66cd07aa19 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:46658: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 15:07:43,445 [canton-env-ec-167] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:4f277dbdf0882443ba642f66cd07aa19 - Domain 'mydomain' disconnected because sequencer client was closed
2025-08-24 15:07:43,446 [canton-env-ec-37] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,446 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,451 [canton-env-ec-37] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,452 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,460 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 15:07:43,461 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sync-domain-ephemeral' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 15:07:43,461 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 15:07:43,462 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Failed(Component is closed). Previous state was Failed(Disconnected from domain).
2025-08-24 15:07:43,462 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle:participant=participant1 - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,462 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 15:07:43,463 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown initiated...
2025-08-24 15:07:43,465 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Shutdown completed.
2025-08-24 15:07:43,466 [canton-env-ec-37] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,467 [canton-env-ec-37] INFO  c.d.c.c.p.t.TinkPrivateCrypto:participant=participant1 - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,471 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Stopping domain node
2025-08-24 15:07:43,475 [canton-env-ec-37] INFO  c.d.c.d.t.DomainTopologySender$Impl:domain=mydomain/identity - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,475 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,477 [canton-env-ec-37] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,478 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,479 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - GRPC subscription successfully closed due to client shutdown.
2025-08-24 15:07:43,479 [canton-env-ec-167] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:17c67a369b39d4667640c245d67ab050 - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:46566: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 15:07:43,486 [canton-env-ec-37] INFO  c.d.c.s.c.ResilientSequencerSubscription:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,487 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Failed(Disconnected from domain). Previous state was Ok().
2025-08-24 15:07:43,488 [canton-env-ec-38] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - GRPC subscription successfully closed due to client shutdown.
2025-08-24 15:07:43,488 [canton-env-ec-38] INFO  c.d.c.n.g.ApiRequestLogger:domain=mydomain tid:4dafcb27b591958f5adb7bb9c9fdc10c - Request c.d.c.d.a.v.SequencerService/SubscribeVersioned by /[0:0:0:0:0:0:0:1%0]:46588: cancelled. Cancellations can be caused by timeouts, explicit cancellation by the client, network errors, etc.
2025-08-24 15:07:43,493 [canton-env-ec-37] INFO  c.d.c.d.s.s.DatabaseSequencer:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 15:07:43,494 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = false, details = Sequencer is closed). Previous state was SequencerHealthStatus(active = true).
2025-08-24 15:07:43,494 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 15:07:43,505 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle:domain=mydomain - 'db-storage' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,505 [canton-env-ec-37] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'NOT_SERVING'). Previous state was ServingStatus(status = 'SERVING').
2025-08-24 15:07:43,506 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown initiated...
2025-08-24 15:07:43,513 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Shutdown completed.
2025-08-24 15:07:43,514 [canton-env-ec-37] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,514 [canton-env-ec-37] INFO  c.d.c.c.p.t.TinkPrivateCrypto:domain=mydomain - 'tink-private-crypto' is now in state Failed(Component is closed). Previous state was Ok().
2025-08-24 15:07:43,537 [canton-env-ec-164] INFO  o.a.pekko.actor.CoordinatedShutdown - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-08-24 15:07:43,561 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 15:07:43,561 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 15:07:47,302 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 15:07:48,485 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting up with resolved config:
domains {
    mydomain {
        admin-api {
            address="0.0.0.0"
            port=5019
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            domain-parameters {
                protocol-version=7
            }
        }
        public-api {
            address="0.0.0.0"
            port=5018
            tls {
                cert-chain-file="config/tls/public-api.crt"
                private-key-file="config/tls/public-api.key"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_domain"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}
features {
    enable-testing-commands=yes
}
participants {
    participant1 {
        admin-api {
            address="0.0.0.0"
            port=5012
            tls {
                cert-chain-file="config/tls/admin-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/admin-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        init {
            identity {
                node-identifier {
                    name=participant1
                    type=explicit
                }
            }
        }
        ledger-api {
            address="0.0.0.0"
            auth-services=[
                {
                    certificate="config/jwt/jwt-sign.crt"
                    type=jwt-rs-256-crt
                }
            ]
            port=5011
            tls {
                cert-chain-file="config/tls/ledger-api.crt"
                client-auth {
                    type=optional
                }
                private-key-file="config/tls/ledger-api.key"
                trust-collection-file="config/tls/root-ca.crt"
            }
        }
        storage {
            config {
                dataSourceClass="org.postgresql.ds.PGSimpleDataSource"
                properties {
                    databaseName="canton_participant"
                    password="****"
                    portNumber="5432"
                    serverName=localhost
                    sslMode=require
                    user=canton
                }
            }
            parameters {
                fail-fast-on-startup=true
                migrate-and-start=true
            }
            type=postgres
        }
    }
}

2025-08-24 15:07:48,505 [main] INFO  c.d.c.e.CommunityEnvironment - Deriving 8 as number of threads from 'sys.runtime.availableProcessors()'. Please use '-Dscala.concurrent.context.numThreads' to override.
2025-08-24 15:07:48,714 [canton-env-ec-35] INFO  o.a.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started
2025-08-24 15:07:48,799 [main] INFO  c.d.c.t.OpenTelemetryFactory$ - Initializing open telemetry with Exporter.Disabled
2025-08-24 15:07:48,861 [main] INFO  c.d.c.e.CommunityEnvironment tid:75cff155b17fb2be7f6891189d3aab94 - Automatically starting all instances
2025-08-24 15:07:48,912 [canton-env-ec-41] INFO  c.d.canton.environment.DomainNodes - Setting up database schemas for mydomain
2025-08-24 15:07:48,912 [canton-env-ec-37] INFO  c.d.c.environment.ParticipantNodes - Setting up database schemas for participant1
2025-08-24 15:07:48,933 [canton-env-ec-41] INFO  c.d.c.resource.DbStorage:mydomain tid:a05037e1a6a16e96ffabf8d83770ea6a - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 15:07:48,933 [canton-env-ec-37] INFO  c.d.c.r.DbStorage:participant1 tid:a4a19b77f5f6ce8ce0ca51d369394549 - Overriding numThreads from 1 to 2 for the purpose of db migration, as flyway needs at least 2 threads.
2025-08-24 15:07:48,960 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Starting...
2025-08-24 15:07:48,960 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Starting...
2025-08-24 15:07:49,451 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Start completed.
2025-08-24 15:07:49,451 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Start completed.
2025-08-24 15:07:49,599 [canton-env-ec-41] WARN  c.d.c.r.DbVersionCheck$:mydomain - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 15:07:49,599 [canton-env-ec-37] WARN  c.d.c.r.DbVersionCheck$:participant1 - Expected Postgres version 11, 12, 13, 14, or 15 but got higher version 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1)
2025-08-24 15:07:49,608 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 15:07:49,608 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 15:07:49,608 [canton-env-ec-41] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 15:07:49,608 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - Flyway Community Edition 9.15.2 by Redgate
2025-08-24 15:07:49,608 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - See release notes here: https://rd.gt/416ObMi
2025-08-24 15:07:49,608 [canton-env-ec-37] INFO  o.f.c.i.license.VersionPrinter - 
2025-08-24 15:07:49,671 [canton-env-ec-41] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_domain (PostgreSQL 16.9)
2025-08-24 15:07:49,671 [canton-env-ec-37] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 15:07:49,680 [canton-env-ec-41] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 15:07:49,680 [canton-env-ec-37] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 15:07:49,734 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown initiated...
2025-08-24 15:07:49,734 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown initiated...
2025-08-24 15:07:49,746 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-2 - Shutdown completed.
2025-08-24 15:07:49,747 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-1 - Shutdown completed.
2025-08-24 15:07:49,798 [canton-env-ec-41] INFO  c.d.c.r.DbStorageSingle$:domain=mydomain - Creating storage, num-combined: 8
2025-08-24 15:07:49,801 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Starting...
2025-08-24 15:07:49,832 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - slick-mydomain-3 - Start completed.
2025-08-24 15:07:49,902 [canton-env-ec-37] INFO  c.d.c.r.DbStorageSingle$:participant=participant1 - Creating storage, num-combined: 4
2025-08-24 15:07:49,904 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Starting...
2025-08-24 15:07:49,934 [canton-env-ec-37] INFO  com.zaxxer.hikari.HikariDataSource - slick-participant1-4 - Start completed.
2025-08-24 15:07:50,048 [canton-env-ec-64] INFO  c.d.c.c.Crypto:participant=participant1 - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:50,049 [canton-env-ec-66] INFO  c.d.c.crypto.Crypto:domain=mydomain - 'crypto' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:50,057 [canton-env-ec-41] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5019),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 15:07:50,058 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Starting admin-api services on CommunityAdminServerConfig(0.0.0.0,Some(5012),Some(TlsServerConfig(ExistingFile(config/tls/admin-api.crt),ExistingFile(config/tls/admin-api.key),Some(ExistingFile(config/tls/root-ca.crt)),None,Optional,Some(TLSv1.2),Some(List(TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256)),false)),Some(BasicKeepAliveServerConfig(40s,20s,20s,false)),10485760,List(),None)
2025-08-24 15:07:50,708 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - 'participant' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 15:07:50,708 [canton-env-ec-41] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'domain' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 15:07:50,824 [canton-env-ec-35] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - Resuming as existing instance with uid=NodeId(mydomain::122060f2dea2...)
2025-08-24 15:07:50,824 [canton-env-ec-37] INFO  c.d.c.p.ParticipantNodeBootstrap:participant=participant1 - Resuming as existing instance with uid=NodeId(participant1::1220b845dcf0...)
2025-08-24 15:07:51,260 [canton-env-ec-62] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Initializing Upgradable Package Resolution View
2025-08-24 15:07:51,280 [canton-env-ec-36] INFO  c.d.c.p.a.PackageUploader:participant=participant1 - Upgradable Package Resolution View has been initialized (19 ms)
2025-08-24 15:07:51,514 [canton-env-ec-37] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:4f6ce2415771c44fb0245d8a8aa8f571 - Recovering published timely rejections
2025-08-24 15:07:51,529 [canton-env-ec-35] INFO  c.d.c.d.s.SequencerRuntime:domain=mydomain - Sequencer is healthy
2025-08-24 15:07:51,534 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:4f6ce2415771c44fb0245d8a8aa8f571 - Publishing the unpublished events from the ParticipantEventLog
2025-08-24 15:07:51,537 [canton-env-ec-36] INFO  c.d.c.p.s.d.DbMultiDomainEventLog:participant=participant1 tid:4f6ce2415771c44fb0245d8a8aa8f571 - Fetch unpublished in log ParticipantEventLogId(index = 0), from Some(LocalOffset(request counter = 7)) (exclusive) up to None (inclusive)
2025-08-24 15:07:51,568 [canton-env-ec-64] INFO  c.d.c.h.MutableHealthQuasiComponent:domain=mydomain - 'sequencer' is now in state SequencerHealthStatus(active = true). Previous state was SequencerHealthStatus(active = false).
2025-08-24 15:07:51,570 [canton-env-ec-64] INFO  c.d.c.d.DomainNodeBootstrap:domain=mydomain - 'sequencer-health-check-service' is now in state ServingStatus(status = 'SERVING'). Previous state was ServingStatus(status = 'NOT_SERVING').
2025-08-24 15:07:51,665 [canton-env-ec-62] INFO  c.d.c.p.l.a.StartableStoppableLedgerApiServer:participant=participant1 - Creating storage, num-indexer: 16, num-ledger-api: 4
2025-08-24 15:07:52,023 [canton-env-ec-35] INFO  c.d.c.p.s.b.VerifiedDataSource$:participant=participant1 - Attempting to connect to the database (attempt 1/600)
2025-08-24 15:07:52,129 [canton-env-ec-62] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Running Flyway migration...
2025-08-24 15:07:52,185 [canton-env-ec-35] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Processing events from the SequencedEventStore from 2025-08-24T14:49:19.783300Z on
2025-08-24 15:07:52,197 [canton-env-ec-62] INFO  o.f.c.i.d.base.BaseDatabaseType - Database: jdbc:postgresql://localhost:5432/canton_participant (PostgreSQL 16.9)
2025-08-24 15:07:52,200 [canton-env-ec-62] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 15:07:52,261 [canton-env-ec-35] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=topology-manager - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:49:19.783299Z); next counter 23
2025-08-24 15:07:52,271 [canton-env-ec-35] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=topology-manager - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:52,307 [canton-env-ec-62] WARN  o.f.c.i.database.base.Database - Flyway upgrade recommended: PostgreSQL 16.9 is newer than this version of Flyway and support has not been tested. The latest supported version of PostgreSQL is 15.
2025-08-24 15:07:52,358 [canton-env-ec-62] INFO  o.f.c.internal.command.DbValidate - Successfully validated 99 migrations (execution time 00:00.045s)
2025-08-24 15:07:52,387 [canton-env-ec-37] INFO  c.d.c.h.MutableHealthComponent:domain=mydomain - 'domain-topology-sender' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:52,391 [canton-env-ec-62] INFO  o.f.core.internal.command.DbMigrate - Current version of schema "ledger_api": 143
2025-08-24 15:07:52,392 [canton-env-ec-62] INFO  o.f.core.internal.command.DbMigrate - Schema "ledger_api" is up to date. No migration necessary.
2025-08-24 15:07:52,397 [canton-env-ec-62] INFO  c.d.c.p.s.FlywayMigrations:participant=participant1 - Flyway schema migration finished successfully, applying 0 steps.
2025-08-24 15:07:52,451 [canton-env-ec-41] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Starting Indexer Server
2025-08-24 15:07:52,458 [canton-env-ec-36] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Starting IndexDB HA Coordinator
2025-08-24 15:07:52,461 [canton-env-ec-36] INFO  c.d.c.p.i.RecoveringIndexer:participant=participant1 - Started Indexer Server
2025-08-24 15:07:52,468 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Starting...
2025-08-24 15:07:52,482 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 15:07:52,482 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting to be elected as leader
2025-08-24 15:07:52,493 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: starting initialization
2025-08-24 15:07:52,494 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Waiting for previous IndexDB HA Coordinator to finish work
2025-08-24 15:07:52,498 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Previous IndexDB HA Coordinator finished work, starting DB connectivity polling
2025-08-24 15:07:52,508 [canton-env-ec-41] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.api-server - Start completed.
2025-08-24 15:07:52,512 [ha-coordinator-0] INFO  c.d.c.p.i.h.PreemptableSequence$:participant=participant1 - Registered release function
2025-08-24 15:07:52,514 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Starting...
2025-08-24 15:07:52,540 [canton-env-ec-64] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T16:07:52.533013Z
2025-08-24 15:07:52,541 [canton-env-ec-36] INFO  com.zaxxer.hikari.HikariDataSource - daml.index.db.connection.indexer - Start completed.
2025-08-24 15:07:52,559 [canton-env-ec-41] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Processing events from the SequencedEventStore from 2025-08-24T14:49:19.850583Z on
2025-08-24 15:07:52,576 [canton-env-ec-102] INFO  c.d.c.p.i.p.InitializeParallelIngestion:participant=participant1 - Attempting to initialize with participant ID participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d
2025-08-24 15:07:52,581 [daml.index.db.threadpool.connection.indexer-1] INFO  c.d.c.p.s.b.c.ParameterStorageBackendImpl$:participant=participant1 - Found existing database for participantId 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'
2025-08-24 15:07:52,590 [canton-env-ec-37] INFO  c.d.c.s.c.SequencerClientImpl:domain=mydomain/client=mediator - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:49:19.850582Z); next counter 10
2025-08-24 15:07:52,592 [canton-env-ec-37] INFO  c.d.c.h.DelegatingMutableHealthComponent:domain=mydomain/client=mediator - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:52,598 [canton-env-ec-113] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=23
2025-08-24 15:07:52,611 [canton-env-ec-35] INFO  c.d.c.p.InMemoryState:participant=participant1 - Initializing participant in-memory state to ledger end: LedgerEnd(Offset(Bytes(000000000000000008)),0,4)
2025-08-24 15:07:52,626 [canton-env-ec-64] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Found existing participant with ID: participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d`
2025-08-24 15:07:52,628 [canton-env-ec-64] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 1/3000. Retrying again in 100 milliseconds.
2025-08-24 15:07:52,675 [canton-env-ec-62] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T16:07:52.671519Z
2025-08-24 15:07:52,729 [canton-env-ec-113] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=10
2025-08-24 15:07:52,735 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 2/3000. Retrying again in 100 milliseconds.
2025-08-24 15:07:52,907 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 3/3000. Retrying again in 100 milliseconds.
2025-08-24 15:07:52,955 [canton-env-ec-113] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:382327afebad81152fe7eda6a3607095 - 'DOM::mydomain::122060f2dea2...' sends request with id 'tick-1dc81810-111f-4ba6-86b3-08b4824b3359' of size 339 bytes with 0 envelopes.
2025-08-24 15:07:53,014 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 4/3000. Retrying again in 100 milliseconds.
2025-08-24 15:07:53,155 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 5/3000. Retrying again in 100 milliseconds.
2025-08-24 15:07:53,222 [canton-env-ec-41] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:142f56a56823c2c0df0669f65955e0c1 - 'MED::mydomain::122060f2dea2...' sends request with id 'tick-dd855764-a925-4806-80d8-4d09e6dc51c4' of size 339 bytes with 0 envelopes.
2025-08-24 15:07:53,261 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 6/3000. Retrying again in 100 milliseconds.
2025-08-24 15:07:53,430 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 7/3000. Retrying again in 100 milliseconds.
2025-08-24 15:07:53,533 [canton-env-ec-62] INFO  c.d.c.d.m.Mediator:domain=mydomain/node=mediator tid:142f56a56823c2c0df0669f65955e0c1 - Caught up with batch with counter=11 with sequencer with 196 ms delay
2025-08-24 15:07:53,535 [timer-utils] INFO  c.d.c.p.i.IndexServiceOwner:participant=participant1 - Participant in-memory state not initialized on attempt 8/3000. Retrying again in 100 milliseconds.
2025-08-24 15:07:53,538 [canton-env-ec-65] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Package Metadata View has been initialized (892 ms)
2025-08-24 15:07:53,560 [canton-env-ec-102] INFO  c.d.c.p.i.UpdatePackageMetadataView$:participant=participant1 - Checking loaded packages for upgrade compatibility finished after (20 ms)
2025-08-24 15:07:53,565 [canton-env-ec-65] INFO  c.d.c.p.DispatcherState:participant=participant1 - Started a Ledger API offset dispatcher at initialization offset: Offset(Bytes(000000000000000008)).
2025-08-24 15:07:53,634 [canton-env-ec-37] INFO  c.d.c.p.i.p.ParallelIndexerFactory$:participant=participant1 - Indexer initialized, indexing started.
2025-08-24 15:07:53,635 [ha-coordinator-0] INFO  c.d.c.p.i.h.HaCoordinator$:participant=participant1 - Elected as leader: initialization complete
2025-08-24 15:07:53,686 [canton-env-ec-35] INFO  c.d.c.p.a.r.MemoryCheck$:participant=participant1 - Using 'tenured' memory pool G1 Old Gen.  Setting its collection pool threshold to 4093640704
2025-08-24 15:07:53,726 [canton-env-ec-65] INFO  c.d.c.p.a.ApiServices$Owner:participant=participant1 - Daml-LF Engine supports LF versions: 1.14, 1.15, 1.17
2025-08-24 15:07:53,749 [canton-env-ec-65] INFO  c.d.c.p.a.c.LedgerConfigurationSubscriptionFromIndex:participant=participant1 - Initial ledger configuration lookup found configuration Configuration(1,LedgerTimeModel(PT0S,PT8760H,PT8760H),PT168H) at Absolute(000000000000000002). Looking for new ledger configurations from this offset.
2025-08-24 15:07:53,914 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled via OPENSSL
2025-08-24 15:07:53,915 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled protocols: TLSv1.2, TLSv1.3.
2025-08-24 15:07:53,915 [canton-env-ec-37] INFO  c.d.c.l.api.tls.TlsConfiguration - Server TLS - enabled cipher suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256.
2025-08-24 15:07:54,062 [canton-env-ec-65] INFO  c.d.c.p.a.LedgerApiService:participant=participant1 - Listening on 0.0.0.0:5011 over TLS with LedgerApiKeepAliveServerConfig(10m,20s,10s,true).
2025-08-24 15:07:54,064 [canton-env-ec-65] INFO  c.d.c.p.a.ApiServiceOwner$:participant=participant1 - Initialized API server version {component version not found on classpath} with ledger-id = participant1, port = 5011.
2025-08-24 15:07:54,130 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:77bfdf91328608d7f62ab2598daf4321 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 15:07:54,959 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2f961363e06e26cd6efd226f86553af6 - Received request for a package status: GetPackageStatusRequest(participant1,18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702), packageId -> '18597917bc74b69da52b6868f118979353b62ebec4363329cd3d843b46e76702'.
2025-08-24 15:07:54,961 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:e8f662d55833e0c862f05a3e832113ca - Received request for a package status: GetPackageStatusRequest(participant1,8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb), packageId -> '8a7806365bbd98d88b4c13832ebfa305f6abaeaf32cfa2b7dd25c4fa489b79fb'.
2025-08-24 15:07:54,963 [canton-env-ec-35] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:883f780c1972fe6f30d49df4b3ab8815 - Received request for a package status: GetPackageStatusRequest(participant1,cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7), packageId -> 'cc348d369011362a5190fe96dd1f0dfbc697fdfd10e382b9e9666f0da05961b7'.
2025-08-24 15:07:54,966 [canton-env-ec-62] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a192d30413f41d8f63d5a94a2614c683 - Received request for a package status: GetPackageStatusRequest(participant1,86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb), packageId -> '86828b9843465f419db1ef8a8ee741d1eef645df02375ebf509cdc8c3ddd16cb'.
2025-08-24 15:07:54,970 [canton-env-ec-65] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a8e47b91d3f333e9f0a68f8075e7e272 - Received request for a package status: GetPackageStatusRequest(participant1,e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d), packageId -> 'e8b3e0650dc0245a4a42133eb3b71b2ac7c3e03f25ee4df378b8089210a1bb6d'.
2025-08-24 15:07:54,970 [canton-env-ec-41] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9c8efe50fc10ff272bed69e39fb66a76 - Received request for a package status: GetPackageStatusRequest(participant1,99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235), packageId -> '99a2705ed38c1c26cbb8fe7acf36bbf626668e167a33335de932599219e0a235'.
2025-08-24 15:07:54,970 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7e8fcb3a0755682c1cc18db0aaad93ed - Received request for a package status: GetPackageStatusRequest(participant1,6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06), packageId -> '6c2c0667393c5f92f1885163068cd31800d2264eb088eb6fc740e11241b2bf06'.
2025-08-24 15:07:54,970 [canton-env-ec-159] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:fbe9ef695024d504a2f485ffc90a29f0 - Received request for a package status: GetPackageStatusRequest(participant1,733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a), packageId -> '733e38d36a2759688a4b2c4cec69d48e7b55ecc8dedc8067b815926c917a182a'.
2025-08-24 15:07:54,971 [canton-env-ec-116] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:7d7801ba9a3819509030932d0094e433 - Received request for a package status: GetPackageStatusRequest(participant1,d58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97), packageId -> 'd58cf9939847921b2aab78eaa7b427dc4c649d25e6bee3c749ace4c3f52f5c97'.
2025-08-24 15:07:54,971 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a299562d5853edb45426bb8bf5233404 - Received request for a package status: GetPackageStatusRequest(participant1,e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c), packageId -> 'e22bce619ae24ca3b8e6519281cb5a33b64b3190cc763248b4c3f9ad5087a92c'.
2025-08-24 15:07:54,974 [canton-env-ec-155] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:753ed470129919f9b7259624f313d117 - Received request for a package status: GetPackageStatusRequest(participant1,c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd), packageId -> 'c1f1f00558799eec139fb4f4c76f95fb52fa1837a5dd29600baa1c8ed1bdccfd'.
2025-08-24 15:07:54,976 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:14cdecee15fa1c6a3cafa2aac533d9f0 - Received request for a package status: GetPackageStatusRequest(participant1,5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff), packageId -> '5921708ce82f4255deb1b26d2c05358b548720938a5a325718dc69f381ba47ff'.
2025-08-24 15:07:54,981 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:01dc7a194fd9da29bb0c3e3040423d4a - Received request for a package status: GetPackageStatusRequest(participant1,1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9), packageId -> '1cc0ce9df7722b71fdfd172e9684c4855a7946a4a6933e784c7d9de8fac081c9'.
2025-08-24 15:07:54,990 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:80f891587c9e211dca18f2cdd2e38123 - Received request for a package status: GetPackageStatusRequest(participant1,f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78), packageId -> 'f20de1e4e37b92280264c08bf15eca0be0bc5babd7a7b5e574997f154c00cb78'.
2025-08-24 15:07:54,987 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:649bc2ee1dcb015c0557ce3578bd551b - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 15:07:54,991 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:a22c6a72df44ffc53d21c50a6170cd54 - Received request for a package status: GetPackageStatusRequest(participant1,6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b), packageId -> '6851f194e144b693e63e9034b956c76cef6b5088dd8c66a657ab652a204dba2b'.
2025-08-24 15:07:54,991 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d083c71583d9e91dd4afadf5792561eb - Received request for a package status: GetPackageStatusRequest(participant1,3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415), packageId -> '3f4deaf145a15cdcfa762c058005e2edb9baa75bb7f95a4f8f6f937378e86415'.
2025-08-24 15:07:54,993 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:d48b56fb8da78463a34c2460907f48ff - Received request for a package status: GetPackageStatusRequest(participant1,d14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662), packageId -> 'd14e08374fc7197d6a0de468c968ae8ba3aadbf9315476fd39071831f5923662'.
2025-08-24 15:07:54,995 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:021d0df1fc5ce53f108af40cefdccbca - Received request for a package status: GetPackageStatusRequest(participant1,057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba), packageId -> '057eed1fd48c238491b8ea06b9b5bf85a5d4c9275dd3f6183e0e6b01730cc2ba'.
2025-08-24 15:07:54,996 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0594034aa4a9e821d59c2d3c6cb94e41 - Received request for a package status: GetPackageStatusRequest(participant1,cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a), packageId -> 'cb0552debf219cc909f51cbb5c3b41e9981d39f8f645b1f35e2ef5be2e0b858a'.
2025-08-24 15:07:54,998 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8c02ab4231bd49550835cc14e18014fa - Received request for a package status: GetPackageStatusRequest(participant1,38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7), packageId -> '38e6274601b21d7202bb995bc5ec147decda5a01b68d57dda422425038772af7'.
2025-08-24 15:07:54,999 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:9973f29c5a35e7cd68ad729a25728671 - Received request for a package status: GetPackageStatusRequest(participant1,518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c), packageId -> '518032f41fd0175461b35ae0c9691e08b4aea55e62915f8360af2cc7a1f2ba6c'.
2025-08-24 15:07:55,002 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:8b5398fb99bf44aacd1b3df6adf17615 - Received request for a package status: GetPackageStatusRequest(participant1,40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7), packageId -> '40f452260bef3f29dede136108fc08a88d5a5250310281067087da6f0baddff7'.
2025-08-24 15:07:55,002 [canton-env-ec-167] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:564aab919b8a0a1014b546c46ffa228e - Received request for a package status: GetPackageStatusRequest(participant1,76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f), packageId -> '76bf0fd12bd945762a01f8fc5bbcdfa4d0ff20f8762af490f8f41d6237c6524f'.
2025-08-24 15:07:55,010 [canton-env-ec-36] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:95170d22cdae133fb886cb08ba2a93a0 - Received request for a package status: GetPackageStatusRequest(participant1,97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657), packageId -> '97b883cd8a2b7f49f90d5d39c981cf6e110cf1f1c64427a28a6d58ec88c43657'.
2025-08-24 15:07:54,998 [canton-env-ec-166] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:313a8e9a307e50fc3d5fc8888a05a3a2 - Received request for a package status: GetPackageStatusRequest(participant1,e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b), packageId -> 'e491352788e56ca4603acc411ffe1a49fefd76ed8b163af86cf5ee5f4c38645b'.
2025-08-24 15:07:55,016 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:c9680ffcc0fd86d5ca37e2d0c0985054 - Received request for a package status: GetPackageStatusRequest(participant1,10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05), packageId -> '10e0333b52bba1ff147fc408a6b7d68465b157635ee230493bd6029b750dcb05'.
2025-08-24 15:07:55,020 [canton-env-ec-66] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:b14fa0824ecac3d0799c677c12555140 - Received request for a package status: GetPackageStatusRequest(participant1,bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f), packageId -> 'bfcd37bd6b84768e86e432f5f6c33e25d9e7724a9d42e33875ff74f6348e733f'.
2025-08-24 15:07:55,028 [canton-env-ec-161] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:0738aeb919c012ca6651beef161d9ef5 - Received request for a package status: GetPackageStatusRequest(participant1,e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254), packageId -> 'e4cc67c3264eba4a19c080cac5ab32d87551578e0f5f58b6a9460f91c7abc254'.
2025-08-24 15:07:54,995 [canton-env-ec-116] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:f7652c12b8299b1e52982bda4a06db05 - Received request for a package status: GetPackageStatusRequest(participant1,6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d), packageId -> '6839a6d3d430c569b2425e9391717b44ca324b88ba621d597778811b2d05031d'.
2025-08-24 15:07:54,994 [canton-env-ec-64] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:944fe04bfdd82252ee07b585343e2b1b - Received request for a package status: GetPackageStatusRequest(participant1,852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671), packageId -> '852d8e3a8ccf952acc73e17522846bc1eb41498e840d637e519ddcca7dbc7671'.
2025-08-24 15:07:55,035 [canton-env-ec-155] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:2697e000d94ac6531262b2a07809ef81 - Received request for a package status: GetPackageStatusRequest(participant1,57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66), packageId -> '57b5c520512c24035057aa4c783cb7ac7f3f49db29806280962e188be7aadb66'.
2025-08-24 15:07:55,114 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiLedgerIdentityService:participant=participant1 tid:f2f4bde4e1d3dd0df5cb9ed066e48220 - Received request for ledger identity: GetLedgerIdentityRequest().
2025-08-24 15:07:55,136 [canton-env-ec-164] INFO  c.d.c.p.a.s.ApiPackageService:participant=participant1 tid:918c21f37b0e0ddfa526fdd520656fc5 - Received request for a package status: GetPackageStatusRequest(participant1,65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e), packageId -> '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e'.
2025-08-24 15:07:55,209 [canton-env-ec-116] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:9a113ba386373be77feb65ce765ad990 - Received request for transactions, startExclusive -> '000000000000000008', endInclusive -> , filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: 'all-templates'}, verbose -> false, ledgerId -> 'participant1'.
2025-08-24 15:07:55,296 [canton-env-ec-102] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - Reconnecting to domains List(mydomain). Already connected: Set()
2025-08-24 15:07:55,385 [canton-env-ec-116] INFO  c.d.c.c.d.g.SequencerInfoLoader:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - Version handshake with sequencer Sequencer 'DefaultSequencer' and domain using protocol version 7 succeeded.
2025-08-24 15:07:55,581 [canton-env-ec-155] INFO  c.d.c.p.s.SyncDomainEphemeralStateFactoryImpl:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - Computed starting points: ProcessingStartingPoints(
  clean replay = MessageCleanReplayStartingPoint(next request counter = 0, next sequencer counter = 20, prenext timestamp = 2025-08-24T14:49:22.749575Z),
  processing = MessageProcessingStartingPoint(next request counter = 0, next sequencer counter = 20, prenext timestamp = 2025-08-24T14:49:22.749575Z),
  rewound sequencer counter prehead = Some(CursorPrehead(counter = 19, timestamp = 2025-08-24T14:49:22.749575Z))
)
2025-08-24 15:07:55,581 [canton-env-ec-155] INFO  c.d.c.s.d.DbCursorPreheadStore:participant=participant1/domain-alias=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - Rewinding prehead to Some(CursorPrehead(counter = 19, timestamp = 2025-08-24T14:49:22.749575Z))
2025-08-24 15:07:55,775 [canton-env-ec-116] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Will use parallelism 8 when computing ACS commitments
2025-08-24 15:07:55,788 [canton-env-ec-37] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Last computed and sent timestamp: 2025-08-24T14:49:00Z
2025-08-24 15:07:55,794 [canton-env-ec-37] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Initialized from stored snapshot at RecordTime(timestamp = 0001-01-01T00:00:00Z, tieBreaker = -9223372036854775808) (might be incomplete)
2025-08-24 15:07:55,798 [canton-env-ec-116] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,812 [canton-env-ec-116] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'acs-commitment-processor' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,812 [canton-env-ec-155] INFO  c.d.c.p.p.AcsCommitmentProcessor:participant=participant1/domainId=mydomain::122060f2dea2 - Initialized the ACS commitment processor queue
2025-08-24 15:07:55,813 [canton-env-ec-116] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,814 [canton-env-ec-116] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,816 [canton-env-ec-116] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - 'sync-domain' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,816 [canton-env-ec-116] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - Connected to domain: Domain 'mydomain', without starting synchronisation
2025-08-24 15:07:55,822 [canton-env-ec-155] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - Starting sync-domains for global reconnect of domains
2025-08-24 15:07:55,851 [canton-env-ec-102] INFO  c.d.c.p.a.s.ApiActiveContractsService:participant=participant1 tid:6b6df27ab83c108b0de1bd64efb4e69b - Received request for active contracts: GetActiveContractsRequest(participant1,Some(TransactionFilter(Map(participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d -> Filters(Some(InclusiveFilters(Vector(Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,PingProposal), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Ping), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Pong), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Explode), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Merge), Identifier(65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e,PingPong,Collapse)),Vector(),Vector())))))),false,), filters -> {participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d: ['65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Pong', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Collapse', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:PingProposal', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Explode', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Merge', '65921e553a353588e950cbc87e98a127730e63295f7ad8d3adae952ef0133b3e:PingPong:Ping']}.
2025-08-24 15:07:55,877 [canton-env-ec-65] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Found 0 repair requests at request counters Seq()
2025-08-24 15:07:55,906 [canton-env-ec-161] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Processing events from the SequencedEventStore from 2025-08-24T14:49:22.749576Z on
2025-08-24 15:07:55,930 [canton-env-ec-155] INFO  c.d.c.s.c.SequencerClientImpl:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Starting subscription for alias=SEQ::mydomain::122060f2dea2... at timestamp Some(2025-08-24T14:49:22.749575Z); next counter 19
2025-08-24 15:07:55,931 [canton-env-ec-155] INFO  c.d.c.h.DelegatingMutableHealthComponent:participant=participant1/domainId=mydomain::122060f2dea2 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,932 [canton-env-ec-155] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,933 [canton-env-ec-155] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 - 'sequencer-client' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,966 [canton-env-ec-161] INFO  c.d.c.p.s.SyncDomainEphemeralState:participant=participant1/domain-alias=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,968 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,970 [canton-env-ec-161] INFO  c.d.c.h.MutableHealthComponent:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - 'sync-domain-ephemeral' is now in state Ok(). Previous state was Not Initialized.
2025-08-24 15:07:55,975 [canton-env-ec-36] INFO  c.d.c.p.s.CantonSyncService:participant=participant1 tid:75cff155b17fb2be7f6891189d3aab94 - Successfully re-connected to domains List(Domain 'mydomain')
2025-08-24 15:07:55,977 [main] INFO  c.d.c.e.CommunityEnvironment tid:75cff155b17fb2be7f6891189d3aab94 - Successfully started all nodes
2025-08-24 15:07:55,978 [canton-env-ec-36] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T16:07:55.975525Z
2025-08-24 15:07:55,987 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - PAR::participant1::1220b845dcf0... subscribes from counter=19
2025-08-24 15:07:56,429 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - 'PAR::participant1::1220b845dcf0...' sends request with id 'tick-005b0c46-123f-4afc-871a-83586d841445' of size 343 bytes with 0 envelopes.
2025-08-24 15:07:56,470 [canton-env-ec-65] INFO  c.d.c.p.s.SyncDomain:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Caught up with batch with counter=20 with sequencer with 33 ms delay
2025-08-24 15:07:56,471 [canton-env-ec-65] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Replaying requests 0 up to clean prehead -1
2025-08-24 15:07:56,471 [canton-env-ec-65] INFO  c.d.c.p.s.SyncDomain$EventProcessingMonitor:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Replaying or processing locally stored events with sequencer counters 20 to 19
2025-08-24 15:08:11,849 [canton-env-ec-161] INFO  c.d.c.p.a.s.a.ApiUserManagementService:participant=participant1 tid:a0a79565ddf3b265574ac578f2388287 - USER_NOT_FOUND(11,a0a79565): grant user rights failed for unknown user "bank_admin" err-context:{location=ApiUserManagementService.scala:670} 
2025-08-24 15:08:11,894 [canton-env-ec-161] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:a0a79565ddf3b265574ac578f2388287 - Request c.d.l.a.v.a.UserManagementService/GrantUserRights by /[0:0:0:0:0:0:0:1%0]:57906: failed with NOT_FOUND/USER_NOT_FOUND(11,a0a79565): grant user rights failed for unknown user "bank_admin"
2025-08-24 15:08:11,928 [main] ERROR c.d.c.e.CommunityConsoleEnvironment - Request failed for participant1.
  GrpcRequestRefusedByServer: NOT_FOUND/USER_NOT_FOUND(11,a0a79565): grant user rights failed for unknown user "bank_admin"
  Request: Grant(bank_admin,Set(NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d),Set(NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d),false,)
  CorrelationId: a0a79565ddf3b265574ac578f2388287
  Context: HashMap(participant -> 'participant1', submissionId -> '3a9f3333-25ae-4639-88b5-3bd71d89d221', tid -> a0a79565ddf3b265574ac578f2388287, definite_answer -> false)
  Command BaseLedgerApiAdministration$ledger_api$users$rights$.grant invoked from cmd9.sc:1
2025-08-24 15:08:12,170 [canton-env-ec-116] INFO  c.d.c.p.a.s.a.ApiUserManagementService:participant=participant1 tid:db25f6d6c6e2d41385a3dcfc56b19b3d - USER_NOT_FOUND(11,db25f6d6): grant user rights failed for unknown user "bank_admin" err-context:{location=ApiUserManagementService.scala:670} 
2025-08-24 15:08:12,171 [canton-env-ec-116] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:db25f6d6c6e2d41385a3dcfc56b19b3d - Request c.d.l.a.v.a.UserManagementService/GrantUserRights by /[0:0:0:0:0:0:0:1%0]:57906: failed with NOT_FOUND/USER_NOT_FOUND(11,db25f6d6): grant user rights failed for unknown user "bank_admin"
2025-08-24 15:08:12,174 [main] ERROR c.d.c.e.CommunityConsoleEnvironment - Request failed for participant1.
  GrpcRequestRefusedByServer: NOT_FOUND/USER_NOT_FOUND(11,db25f6d6): grant user rights failed for unknown user "bank_admin"
  Request: Grant(bank_admin,Set(NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d),Set(NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d),false,)
  CorrelationId: db25f6d6c6e2d41385a3dcfc56b19b3d
  Context: HashMap(participant -> 'participant1', submissionId -> '8ebd8411-ea8c-4a5c-8735-16d2d339455a', tid -> db25f6d6c6e2d41385a3dcfc56b19b3d, definite_answer -> false)
  Command BaseLedgerApiAdministration$ledger_api$users$rights$.grant invoked from cmd10.sc:1
2025-08-24 15:08:58,827 [input-mapping-pool-0] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:ebe75ab0d957dde276830c015e375874 - Storing at offset=000000000000000009 PublicPackageUpload(
  recordTime = 2025-08-24T15:08:58.639024Z,
  archives = Seq(
    323be96aa0b9...,
    315cb5676675...,
    cb0552debf21...,
    3f4deaf145a1...,
    86828b984346...,
    f20de1e4e37b...,
    76bf0fd12bd9...,
    38e6274601b2...,
    d58cf9939847...,
    40f452260bef...,
    e491352788e5...,
    6839a6d3d430...,
    518032f41fd0...,
    bf5d87e92f33...,
    10e0333b52bb...,
    bfcd37bd6b84...,
    cc348d369011...,
    057eed1fd48c...,
    6df2d1fd8ea9...,
    d14e08374fc7...,
    c1f1f0055879...,
    6c2c0667393c...,
    e22bce619ae2...,
    e4cc67c3264e...,
    8a7806365bbd...,
    97b883cd8a2b...,
    5921708ce82f...,
    733e38d36a27...,
    99a2705ed38c...
  ),
  sourceDescription = 'RWA'
)
2025-08-24 15:08:58,995 [canton-env-ec-164] INFO  c.d.c.p.a.s.a.ApiPackageManagementService:participant=participant1 tid:2263a536163b5b70d2da6be3f5a5d160 - Listing known packages.
2025-08-24 15:24:22,095 [canton-env-ec-161] INFO  c.d.c.p.a.s.c.CommandSubmissionServiceImpl:participant=participant1 tid:71f58d75d3f22b08fe257eb96f479934 - Phase 1 started: Submitting commands for interpretation: Commands(
  commandId = c302c358-0162-4f08-91da-87ccd3b4ce6d,
  submissionId = 3471cde1-e081-4521-90ef-879302fc06f1,
  applicationId = CantonConsole,
  actAs = NewBank::1220b845dcf0...,
  submittedAt = 2025-08-24T15:24:22.087878Z,
  ledgerEffectiveTime = 2025-08-24T15:24:22.087867Z,
  deduplicationPeriod = (duration=PT168H),
  ...
).
2025-08-24 15:24:23,871 [canton-env-ec-164] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::122060f2dea2 tid:71f58d75d3f22b08fe257eb96f479934 - Phase 1 completed: Submitting 4 envelopes for Transaction request, submitters NewBank::1220b845dcf0..., command-id c302c358-0162-4f08-91da-87ccd3b4ce6d
2025-08-24 15:24:23,909 [canton-env-ec-164] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:71f58d75d3f22b08fe257eb96f479934 - 'PAR::participant1::1220b845dcf0...' sends request with id '5a0ba32f-38bc-44bb-80e7-04721c60fc0a' of size 3824 bytes with 4 envelopes.
2025-08-24 15:24:24,019 [canton-env-ec-161] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:71f58d75d3f22b08fe257eb96f479934 - Processing event at sc=21, ts=2025-08-24T15:24:23.930437Z, messageId=5a0ba32f-38bc-44bb-80e7-04721c60fc0a, with contents=Seq(
  RootHashMessage(root hash = SHA-256:5c57cd5e4129..., payload size = 0),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:2bf23a6fa420...), view type = TransactionViewType, size = 1049),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:a19a60ad9bce...), view type = TransactionViewType, size = 1047)
)
2025-08-24 15:24:24,063 [canton-env-ec-161] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::122060f2dea2 tid:71f58d75d3f22b08fe257eb96f479934 - Phase 3: Validating Transaction request=2025-08-24T15:24:23.930437Z with 2 envelope(s)
2025-08-24 15:24:24,119 [canton-env-ec-102] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:71f58d75d3f22b08fe257eb96f479934 - Phase 2: Registered request=2025-08-24T15:24:23.930437Z with 2 view(s). Initial state: Map(
  ViewPosition(L) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::1220b845dcf0... -> 1, threshold = 1), Quorum(confirmers = participant1::1220b845dcf0... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::1220b845dcf0... -> ConsortiumVotingState(), participant1::1220b845dcf0... -> ConsortiumVotingState()),
    rejections = Seq()
  ),
  ViewPosition(R) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::1220b845dcf0... -> 1, threshold = 1), Quorum(confirmers = participant1::1220b845dcf0... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::1220b845dcf0... -> ConsortiumVotingState(), participant1::1220b845dcf0... -> ConsortiumVotingState()),
    rejections = Seq()
  )
)
2025-08-24 15:24:24,509 [canton-env-ec-167] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::122060f2dea2 tid:71f58d75d3f22b08fe257eb96f479934 - Phase 4: Sending for request=2025-08-24T15:24:23.930437Z with msgId=d80802e8-a868-414c-991c-3a2176e9ab3b approved=2, rejected=0
2025-08-24 15:24:24,532 [canton-env-ec-161] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:71f58d75d3f22b08fe257eb96f479934 - 'PAR::participant1::1220b845dcf0...' sends request with id 'd80802e8-a868-414c-991c-3a2176e9ab3b' of size 838 bytes with 2 envelopes.
2025-08-24 15:24:24,570 [canton-env-ec-164] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:71f58d75d3f22b08fe257eb96f479934 - Processing event at sc=22, ts=2025-08-24T15:24:24.540249Z, messageId=d80802e8-a868-414c-991c-3a2176e9ab3b, with contents=Seq()
2025-08-24 15:24:24,591 [canton-env-ec-65] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:71f58d75d3f22b08fe257eb96f479934 - Phase 5: Received responses for request=RequestId(2025-08-24T15:24:23.930437Z): ParticipantResponse(sender = PAR::participant1::1220b845dcf0..., ts = 2025-08-24T15:24:24.540249Z, approved = 2)
2025-08-24 15:24:24,659 [canton-env-ec-36] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:71f58d75d3f22b08fe257eb96f479934 - Phase 6: Finalized request=RequestId(2025-08-24T15:24:23.930437Z) with verdict Approve
2025-08-24 15:24:24,690 [canton-env-ec-163] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:71f58d75d3f22b08fe257eb96f479934 - 'MED::mydomain::122060f2dea2...' sends request with id 'mmid:verdict:1756049063930437' of size 720 bytes with 1 envelopes.
2025-08-24 15:24:24,745 [canton-env-ec-102] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:71f58d75d3f22b08fe257eb96f479934 - Processing event at sc=23, ts=2025-08-24T15:24:24.702007Z, with contents=SignedProtocolMessage(
  TransactionResultMessage(requestId = 2025-08-24T15:24:23.930437Z, verdict = Approve, rootHash = SHA-256:5c57cd5e4129..., domainId = mydomain::122060f2dea2...),
  signatures = Signature(signature = b8c7fc46d415, signedBy = 1220120ea179...)
)
2025-08-24 15:24:24,982 [input-mapping-pool-1] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:71f58d75d3f22b08fe257eb96f479934 - Phase 7: Storing at offset=00000000000000000a TransactionAccepted(
  recordTime = 2025-08-24T15:24:23.930437Z,
  transactionId = 12205c57cd5e41292cb105e261e8b9ca13c5886649cf58511db8924cb194322b2a44,
  transactionMeta = TransactionMeta(ledgerEffectiveTime = 2025-08-24T15:24:22.087867Z, submissionTime = 2025-08-24T15:24:22.087867Z, domainId = mydomain::122060f2dea2..., ...),
  completion = CompletionInfo(
    actAs = NewBank::1220b845dcf0...,
    commandId = c302c358-0162-4f08-91da-87ccd3b4ce6d,
    applicationId = CantonConsole,
    deduplication period = (offset=Offset(Bytes(000000000000000001))),
    submissionId = Some(3471cde1-e081-4521-90ef-879302fc06f1),
    ...
  ),
  nodes = 2,
  roots = 2,
  ...
)
2025-08-24 15:24:25,047 [canton-env-ec-116] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '12205c57cd5e41292cb105e261e8b9ca13c5886649cf58511db8924cb194322b2a44', parties -> ['NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'].
2025-08-24 15:24:25,161 [canton-env-ec-102] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:28dc633032bdfce577911df941d13bed - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '12205c57cd5e41292cb105e261e8b9ca13c5886649cf58511db8924cb194322b2a44', parties -> ['NewAlice::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d', 'NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d', 'NewBob::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d', 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'].
2025-08-24 15:24:25,198 [canton-env-ec-161] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:97954697410fe3e7b53b9f007844f0af - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '12205c57cd5e41292cb105e261e8b9ca13c5886649cf58511db8924cb194322b2a44', parties -> ['NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'].
2025-08-24 15:24:54,193 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:71f58d75d3f22b08fe257eb96f479934 - 'MED::mydomain::122060f2dea2...' sends request with id 'tick-7220f442-2837-4c0e-8d3c-703369cd2280' of size 339 bytes with 0 envelopes.
2025-08-24 15:25:22,348 [canton-env-ec-155] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:71f58d75d3f22b08fe257eb96f479934 - 'PAR::participant1::1220b845dcf0...' sends request with id 'tick-6c8ce698-44f9-4c39-a798-9b16ed350b8c' of size 343 bytes with 0 envelopes.
2025-08-24 15:25:24,192 [canton-env-ec-164] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:71f58d75d3f22b08fe257eb96f479934 - 'PAR::participant1::1220b845dcf0...' sends request with id 'tick-90f3e9ea-0177-4160-8b67-727fb370be47' of size 343 bytes with 0 envelopes.
2025-08-24 15:51:25,762 [canton-env-ec-64] WARN  c.d.c.l.a.a.i.UserBasedAuthorizationInterceptor:participant=participant1 tid:d6ae76fbd94d2e086976d2fa5860e978 - PERMISSION_DENIED(7,d6ae76fb): Could not resolve is_deactivated status for user 'bank_admin' and identity_provider_id 'Default' due to 'UserNotFound(bank_admin)' err-context:{location=UserBasedAuthorizationInterceptor.scala:133} 
2025-08-24 15:51:25,767 [canton-env-ec-64] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:d6ae76fbd94d2e086976d2fa5860e978 - Request c.d.l.a.v.a.UserManagementService/ListUserRights by /127.0.0.1:58714: failed with PERMISSION_DENIED/An error occurred. Please contact the operator and inquire about the request d6ae76fbd94d2e086976d2fa5860e978 with tid d6ae76fbd94d2e086976d2fa5860e978
2025-08-24 16:01:52,581 [daml.index.db.threadpool.connection.indexer-2] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000a)),2025-08-24T16:00:00Z)
2025-08-24 16:01:52,586 [daml.index.db.threadpool.connection.indexer-2] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000a)),2025-08-24T16:00:00Z)
2025-08-24 16:07:32,582 [canton-env-ec-102] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T17:07:32.580198Z
2025-08-24 16:07:32,719 [canton-env-ec-164] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T17:07:32.717453Z
2025-08-24 16:07:36,031 [canton-env-ec-161] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T17:07:36.028404Z
2025-08-24 16:07:52,549 [canton-env-ec-64] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 16:07:52,554 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 16:07:52,611 [canton-env-ec-164] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=24
2025-08-24 16:07:52,674 [canton-env-ec-41] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 16:07:52,675 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 16:07:52,731 [canton-env-ec-64] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=15
2025-08-24 16:07:55,978 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - The sequencer subscription has been terminated by the server.
2025-08-24 16:07:55,979 [canton-env-ec-102] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 16:07:56,035 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - PAR::participant1::1220b845dcf0... subscribes from counter=25
2025-08-24 16:10:32,184 [canton-env-ec-64] INFO  c.d.c.p.a.s.c.CommandSubmissionServiceImpl:participant=participant1 tid:ad594c20f073cc72d329a72f962b6ac1 - Phase 1 started: Submitting commands for interpretation: Commands(
  commandId = 5795d248-736a-4d4a-b8d6-52ef4e36550f,
  submissionId = 1db439d0-374f-4cf7-be0b-b5c6100b42cc,
  applicationId = CantonConsole,
  actAs = NewBank::1220b845dcf0...,
  submittedAt = 2025-08-24T16:10:32.183593Z,
  ledgerEffectiveTime = 2025-08-24T16:10:32.183592Z,
  deduplicationPeriod = (duration=PT168H),
  ...
).
2025-08-24 16:10:32,328 [canton-env-ec-41] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::122060f2dea2 tid:ad594c20f073cc72d329a72f962b6ac1 - Phase 1 completed: Submitting 4 envelopes for Transaction request, submitters NewBank::1220b845dcf0..., command-id 5795d248-736a-4d4a-b8d6-52ef4e36550f
2025-08-24 16:10:32,341 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ad594c20f073cc72d329a72f962b6ac1 - 'PAR::participant1::1220b845dcf0...' sends request with id '5e7a4e8a-fbf8-4289-b3c3-5befd91beb6b' of size 3819 bytes with 4 envelopes.
2025-08-24 16:10:32,394 [canton-env-ec-102] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:ad594c20f073cc72d329a72f962b6ac1 - Processing event at sc=26, ts=2025-08-24T16:10:32.352613Z, messageId=5e7a4e8a-fbf8-4289-b3c3-5befd91beb6b, with contents=Seq(
  RootHashMessage(root hash = SHA-256:345b0b1edfec..., payload size = 0),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:76dc0b264950...), view type = TransactionViewType, size = 1050),
  EncryptedViewMessageV2(view hash = ViewHash(SHA-256:42ff1e0215f4...), view type = TransactionViewType, size = 1047)
)
2025-08-24 16:10:32,397 [canton-env-ec-116] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::122060f2dea2 tid:ad594c20f073cc72d329a72f962b6ac1 - Phase 3: Validating Transaction request=2025-08-24T16:10:32.352613Z with 2 envelope(s)
2025-08-24 16:10:32,408 [canton-env-ec-65] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:ad594c20f073cc72d329a72f962b6ac1 - Phase 2: Registered request=2025-08-24T16:10:32.352613Z with 2 view(s). Initial state: Map(
  ViewPosition(L) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::1220b845dcf0... -> 1, threshold = 1), Quorum(confirmers = participant1::1220b845dcf0... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::1220b845dcf0... -> ConsortiumVotingState(), participant1::1220b845dcf0... -> ConsortiumVotingState()),
    rejections = Seq()
  ),
  ViewPosition(R) -> ViewState(
    quorumsState = Seq(Quorum(confirmers = NewBank::1220b845dcf0... -> 1, threshold = 1), Quorum(confirmers = participant1::1220b845dcf0... -> 1, threshold = 1)),
    consortiumVoting = Map(NewBank::1220b845dcf0... -> ConsortiumVotingState(), participant1::1220b845dcf0... -> ConsortiumVotingState()),
    rejections = Seq()
  )
)
2025-08-24 16:10:32,572 [canton-env-ec-37] INFO  c.d.c.p.p.TransactionProcessor:participant=participant1/domainId=mydomain::122060f2dea2 tid:ad594c20f073cc72d329a72f962b6ac1 - Phase 4: Sending for request=2025-08-24T16:10:32.352613Z with msgId=1fc20676-6e9f-4e81-ba76-583c35af02a6 approved=2, rejected=0
2025-08-24 16:10:32,583 [canton-env-ec-163] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ad594c20f073cc72d329a72f962b6ac1 - 'PAR::participant1::1220b845dcf0...' sends request with id '1fc20676-6e9f-4e81-ba76-583c35af02a6' of size 837 bytes with 2 envelopes.
2025-08-24 16:10:32,653 [canton-env-ec-116] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:ad594c20f073cc72d329a72f962b6ac1 - Processing event at sc=27, ts=2025-08-24T16:10:32.588555Z, messageId=1fc20676-6e9f-4e81-ba76-583c35af02a6, with contents=Seq()
2025-08-24 16:10:32,663 [canton-env-ec-161] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:ad594c20f073cc72d329a72f962b6ac1 - Phase 5: Received responses for request=RequestId(2025-08-24T16:10:32.352613Z): ParticipantResponse(sender = PAR::participant1::1220b845dcf0..., ts = 2025-08-24T16:10:32.588555Z, approved = 2)
2025-08-24 16:10:32,687 [canton-env-ec-164] INFO  c.d.c.d.m.ConfirmationResponseProcessor:domain=mydomain/node=mediator tid:ad594c20f073cc72d329a72f962b6ac1 - Phase 6: Finalized request=RequestId(2025-08-24T16:10:32.352613Z) with verdict Approve
2025-08-24 16:10:32,705 [canton-env-ec-116] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ad594c20f073cc72d329a72f962b6ac1 - 'MED::mydomain::122060f2dea2...' sends request with id 'mmid:verdict:1756051832352613' of size 724 bytes with 1 envelopes.
2025-08-24 16:10:32,735 [canton-env-ec-167] INFO  c.d.c.p.p.DefaultMessageDispatcher:participant=participant1/domainId=mydomain::122060f2dea2 tid:ad594c20f073cc72d329a72f962b6ac1 - Processing event at sc=28, ts=2025-08-24T16:10:32.713285Z, with contents=SignedProtocolMessage(
  TransactionResultMessage(requestId = 2025-08-24T16:10:32.352613Z, verdict = Approve, rootHash = SHA-256:345b0b1edfec..., domainId = mydomain::122060f2dea2...),
  signatures = Signature(signature = b5771e9621e8, signedBy = 1220120ea179...)
)
2025-08-24 16:10:32,803 [input-mapping-pool-2] INFO  c.d.c.p.i.p.ParallelIndexerSubscription:participant=participant1 tid:ad594c20f073cc72d329a72f962b6ac1 - Phase 7: Storing at offset=00000000000000000b TransactionAccepted(
  recordTime = 2025-08-24T16:10:32.352613Z,
  transactionId = 1220345b0b1edfecf8c207aa68d2ec48a93e3580d93dd7021764954ce6de99005b55,
  transactionMeta = TransactionMeta(ledgerEffectiveTime = 2025-08-24T16:10:32.183592Z, submissionTime = 2025-08-24T16:10:32.183592Z, domainId = mydomain::122060f2dea2..., ...),
  completion = CompletionInfo(
    actAs = NewBank::1220b845dcf0...,
    commandId = 5795d248-736a-4d4a-b8d6-52ef4e36550f,
    applicationId = CantonConsole,
    deduplication period = (offset=Offset(Bytes(000000000000000001))),
    submissionId = Some(1db439d0-374f-4cf7-be0b-b5c6100b42cc),
    ...
  ),
  nodes = 2,
  roots = 2,
  ...
)
2025-08-24 16:10:32,816 [canton-env-ec-36] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220345b0b1edfecf8c207aa68d2ec48a93e3580d93dd7021764954ce6de99005b55', parties -> ['NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'].
2025-08-24 16:10:32,863 [canton-env-ec-167] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:4244c5c5d7ab5748ff69d7db843da674 - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220345b0b1edfecf8c207aa68d2ec48a93e3580d93dd7021764954ce6de99005b55', parties -> ['NewAlice::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d', 'NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d', 'NewBob::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d', 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'].
2025-08-24 16:10:32,882 [canton-env-ec-167] INFO  c.d.c.p.a.s.t.TransactionServiceImpl:participant=participant1 tid:f84a10be0561fe9d04ff204599e3a87d - Received request for transaction by ID, ledgerId -> '<empty-ledger-id>', transactionId -> '1220345b0b1edfecf8c207aa68d2ec48a93e3580d93dd7021764954ce6de99005b55', parties -> ['NewBank::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d'].
2025-08-24 16:11:02,614 [canton-env-ec-37] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ad594c20f073cc72d329a72f962b6ac1 - 'MED::mydomain::122060f2dea2...' sends request with id 'tick-72dd8d91-7174-4588-9988-25fdb5a6fc63' of size 339 bytes with 0 envelopes.
2025-08-24 16:11:32,442 [canton-env-ec-161] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:ad594c20f073cc72d329a72f962b6ac1 - 'PAR::participant1::1220b845dcf0...' sends request with id 'tick-c2626169-ebe7-4d52-a1db-a014d17c56bd' of size 343 bytes with 0 envelopes.
2025-08-24 16:36:25,649 [canton-env-ec-116] WARN  c.d.c.l.a.a.i.UserBasedAuthorizationInterceptor:participant=participant1 tid:6f7d7f7c50a73d39a1f2fbd670be8eee - PERMISSION_DENIED(7,6f7d7f7c): Could not resolve is_deactivated status for user 'bank_admin' and identity_provider_id 'Default' due to 'UserNotFound(bank_admin)' err-context:{location=UserBasedAuthorizationInterceptor.scala:133} 
2025-08-24 16:36:25,650 [canton-env-ec-155] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:6f7d7f7c50a73d39a1f2fbd670be8eee - Request c.d.l.a.v.a.UserManagementService/ListUserRights by /127.0.0.1:51652: failed with PERMISSION_DENIED/An error occurred. Please contact the operator and inquire about the request 6f7d7f7c50a73d39a1f2fbd670be8eee with tid 6f7d7f7c50a73d39a1f2fbd670be8eee
2025-08-24 16:42:01,507 [canton-env-ec-37] WARN  c.d.c.l.a.a.i.UserBasedAuthorizationInterceptor:participant=participant1 tid:b5b4e2ee907a4dec2a0a8df9f5d8a356 - PERMISSION_DENIED(7,b5b4e2ee): Could not resolve is_deactivated status for user 'bank_admin' and identity_provider_id 'Default' due to 'UserNotFound(bank_admin)' err-context:{location=UserBasedAuthorizationInterceptor.scala:133} 
2025-08-24 16:42:01,508 [canton-env-ec-167] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:b5b4e2ee907a4dec2a0a8df9f5d8a356 - Request c.d.l.a.v.a.UserManagementService/ListUserRights by /127.0.0.1:56888: failed with PERMISSION_DENIED/An error occurred. Please contact the operator and inquire about the request b5b4e2ee907a4dec2a0a8df9f5d8a356 with tid b5b4e2ee907a4dec2a0a8df9f5d8a356
2025-08-24 16:44:19,822 [canton-env-ec-163] WARN  c.d.c.l.a.a.i.UserBasedAuthorizationInterceptor:participant=participant1 tid:a82adcfe20dc0dcead7c27aafcbeba29 - PERMISSION_DENIED(7,a82adcfe): Could not resolve is_deactivated status for user 'bank_admin' and identity_provider_id 'Default' due to 'UserNotFound(bank_admin)' err-context:{location=UserBasedAuthorizationInterceptor.scala:133} 
2025-08-24 16:44:19,823 [canton-env-ec-163] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:a82adcfe20dc0dcead7c27aafcbeba29 - Request c.d.l.a.v.a.UserManagementService/ListUserRights by /127.0.0.1:38904: failed with PERMISSION_DENIED/An error occurred. Please contact the operator and inquire about the request a82adcfe20dc0dcead7c27aafcbeba29 with tid a82adcfe20dc0dcead7c27aafcbeba29
2025-08-24 16:47:05,834 [canton-env-ec-41] WARN  c.d.c.l.a.a.i.UserBasedAuthorizationInterceptor:participant=participant1 tid:76262f1e760317264080d35cd462126c - PERMISSION_DENIED(7,76262f1e): Could not resolve is_deactivated status for user 'bank_admin' and identity_provider_id 'Default' due to 'UserNotFound(bank_admin)' err-context:{location=UserBasedAuthorizationInterceptor.scala:133} 
2025-08-24 16:47:05,835 [canton-env-ec-41] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:76262f1e760317264080d35cd462126c - Request c.d.l.a.v.a.UserManagementService/ListUserRights by /127.0.0.1:38904: failed with PERMISSION_DENIED/An error occurred. Please contact the operator and inquire about the request 76262f1e760317264080d35cd462126c with tid 76262f1e760317264080d35cd462126c
2025-08-24 16:48:38,111 [canton-env-ec-37] WARN  c.d.c.l.a.a.i.UserBasedAuthorizationInterceptor:participant=participant1 tid:70cd5104ea4db8493110568e520b8545 - PERMISSION_DENIED(7,70cd5104): Could not resolve is_deactivated status for user 'bank_admin' and identity_provider_id 'Default' due to 'UserNotFound(bank_admin)' err-context:{location=UserBasedAuthorizationInterceptor.scala:133} 
2025-08-24 16:48:38,112 [canton-env-ec-163] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:70cd5104ea4db8493110568e520b8545 - Request c.d.l.a.v.a.UserManagementService/ListUserRights by /127.0.0.1:38904: failed with PERMISSION_DENIED/An error occurred. Please contact the operator and inquire about the request 70cd5104ea4db8493110568e520b8545 with tid 70cd5104ea4db8493110568e520b8545
2025-08-24 17:01:52,581 [daml.index.db.threadpool.connection.indexer-16] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T17:00:00Z)
2025-08-24 17:01:52,586 [daml.index.db.threadpool.connection.indexer-16] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T17:00:00Z)
2025-08-24 17:07:12,626 [canton-env-ec-65] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T18:07:12.623794Z
2025-08-24 17:07:12,762 [canton-env-ec-37] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T18:07:12.760005Z
2025-08-24 17:07:16,073 [canton-env-ec-167] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T18:07:16.071391Z
2025-08-24 17:07:32,583 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 17:07:32,583 [canton-env-ec-41] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 17:07:32,638 [canton-env-ec-163] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=24
2025-08-24 17:07:32,720 [canton-env-ec-41] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 17:07:32,721 [canton-env-ec-65] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 17:07:32,776 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=19
2025-08-24 17:07:36,031 [canton-env-ec-155] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - The sequencer subscription has been terminated by the server.
2025-08-24 17:07:36,032 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 17:07:36,086 [canton-env-ec-163] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - PAR::participant1::1220b845dcf0... subscribes from counter=29
2025-08-24 18:01:52,582 [daml.index.db.threadpool.connection.indexer-15] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T18:00:00Z)
2025-08-24 18:01:52,584 [daml.index.db.threadpool.connection.indexer-15] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T18:00:00Z)
2025-08-24 18:06:52,678 [canton-env-ec-163] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T19:06:52.675200Z
2025-08-24 18:06:52,801 [canton-env-ec-65] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T19:06:52.799574Z
2025-08-24 18:06:56,115 [canton-env-ec-41] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T19:06:56.113925Z
2025-08-24 18:07:12,628 [canton-env-ec-163] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 18:07:12,628 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 18:07:12,684 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=24
2025-08-24 18:07:12,762 [canton-env-ec-36] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 18:07:12,763 [canton-env-ec-37] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 18:07:12,818 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=19
2025-08-24 18:07:16,074 [canton-env-ec-155] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - The sequencer subscription has been terminated by the server.
2025-08-24 18:07:16,075 [canton-env-ec-116] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 18:07:16,130 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - PAR::participant1::1220b845dcf0... subscribes from counter=29
2025-08-24 19:01:52,581 [daml.index.db.threadpool.connection.indexer-9] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T19:00:00Z)
2025-08-24 19:01:52,583 [daml.index.db.threadpool.connection.indexer-9] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T19:00:00Z)
2025-08-24 19:06:32,715 [canton-env-ec-36] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T20:06:32.713655Z
2025-08-24 19:06:32,848 [canton-env-ec-161] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T20:06:32.846278Z
2025-08-24 19:06:36,154 [canton-env-ec-161] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T20:06:36.152512Z
2025-08-24 19:06:52,678 [canton-env-ec-161] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 19:06:52,679 [canton-env-ec-163] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 19:06:52,734 [canton-env-ec-65] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=24
2025-08-24 19:06:52,802 [canton-env-ec-116] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 19:06:52,803 [canton-env-ec-161] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 19:06:52,858 [canton-env-ec-161] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=19
2025-08-24 19:06:56,116 [canton-env-ec-116] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - The sequencer subscription has been terminated by the server.
2025-08-24 19:06:56,117 [canton-env-ec-163] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 19:06:56,171 [canton-env-ec-163] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - PAR::participant1::1220b845dcf0... subscribes from counter=29
2025-08-24 19:19:09,953 [canton-env-ec-155] WARN  c.d.c.a.AuthServiceJWT:participant=participant1 tid:672f0734199b340c8346c25147ce09f5 - Authorization error: Could not parse JWT token: Could not read value for sub
2025-08-24 19:19:09,966 [canton-env-ec-161] WARN  c.d.c.a.Authorizer:participant=participant1 tid:672f0734199b340c8346c25147ce09f5 - UNAUTHENTICATED(6,672f0734): The command is missing a (valid) JWT token err-context:{location=Authorizer.scala:334} 
2025-08-24 19:22:00,882 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 19:22:01,206 [main] ERROR c.d.canton.CantonCommunityApp$ - CANNOT_PARSE_CONFIG_FILES(8,0): Received an exception (full stack trace has been logged at DEBUG level) while attempting to parse 1 .conf-file(s). err-context:{exceptions=List(com.typesafe.config.ConfigException$Parse: /dev/stdin: 2: Key 'participant1.ledger_api.users.list()' may not be followed by token: end of file), location=CantonConfig.scala:1546} 
2025-08-24 19:22:01,216 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 19:22:01,218 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 19:22:01,218 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 19:37:41,856 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 19:37:42,181 [main] ERROR c.d.canton.CantonCommunityApp$ - CANNOT_PARSE_CONFIG_FILES(8,0): Received an exception (full stack trace has been logged at DEBUG level) while attempting to parse 1 .conf-file(s). err-context:{exceptions=List(com.typesafe.config.ConfigException$Parse: create_users.canton: 3: Key 'participant1.ledger_api.users.create(' may not be followed by token: 'user' (if you intended 'user' to be part of a key or string value, try enclosing the key or value in double quotes)), location=CantonConfig.scala:1546} 
2025-08-24 19:37:42,191 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 19:37:42,192 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 19:37:42,192 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 19:42:18,687 [canton-env-ec-155] WARN  c.d.c.l.a.a.i.UserBasedAuthorizationInterceptor:participant=participant1 tid:ace1fe432632f4a48e076e095b62d61c - PERMISSION_DENIED(7,ace1fe43): Could not resolve is_deactivated status for user 'bank_user' and identity_provider_id 'Default' due to 'UserNotFound(bank_user)' err-context:{location=UserBasedAuthorizationInterceptor.scala:133} 
2025-08-24 19:42:18,688 [canton-env-ec-155] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:ace1fe432632f4a48e076e095b62d61c - Request c.d.l.a.v.a.PartyManagementService/ListKnownParties by /127.0.0.1:52496: failed with PERMISSION_DENIED/An error occurred. Please contact the operator and inquire about the request ace1fe432632f4a48e076e095b62d61c with tid ace1fe432632f4a48e076e095b62d61c
2025-08-24 19:43:31,028 [canton-env-ec-163] WARN  c.d.c.a.Authorizer:participant=participant1 tid:0fa687e9643e9696aa7119768e05b0ca - PERMISSION_DENIED(7,0fa687e9): Claims are only valid for participantId 'participant1', actual participantId is 'participant1::1220b845dcf0d9cf52ce1e7457a744a6f3de7eff4a9ee95261b69405d1e0de8a768d' err-context:{location=Authorizer.scala:305} 
2025-08-24 19:43:31,029 [canton-env-ec-163] INFO  c.d.c.n.g.ApiRequestLogger:participant=participant1 tid:0fa687e9643e9696aa7119768e05b0ca - Request c.d.l.a.v.a.PartyManagementService/ListKnownParties by /127.0.0.1:52496: failed with PERMISSION_DENIED/An error occurred. Please contact the operator and inquire about the request 0fa687e9643e9696aa7119768e05b0ca with tid 0fa687e9643e9696aa7119768e05b0ca
2025-08-24 19:48:08,032 [canton-env-ec-161] WARN  c.d.c.a.AuthServiceJWT:participant=participant1 tid:4408463dbe7037c24dbc50cce4dced6c - Authorization error: Could not parse JWT token: Could not read value for sub
2025-08-24 19:48:08,035 [canton-env-ec-167] WARN  c.d.c.a.Authorizer:participant=participant1 tid:4408463dbe7037c24dbc50cce4dced6c - UNAUTHENTICATED(6,4408463d): The command is missing a (valid) JWT token err-context:{location=Authorizer.scala:334} 
2025-08-24 20:01:52,584 [daml.index.db.threadpool.connection.indexer-4] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T20:00:00Z)
2025-08-24 20:01:52,587 [daml.index.db.threadpool.connection.indexer-4] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T20:00:00Z)
2025-08-24 20:06:12,763 [canton-env-ec-161] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T21:06:12.759978Z
2025-08-24 20:06:12,886 [canton-env-ec-163] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T21:06:12.884239Z
2025-08-24 20:06:16,198 [canton-env-ec-41] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T21:06:16.196085Z
2025-08-24 20:06:32,716 [canton-env-ec-116] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 20:06:32,717 [canton-env-ec-155] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 20:06:32,773 [canton-env-ec-163] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=24
2025-08-24 20:06:32,848 [canton-env-ec-164] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 20:06:32,849 [canton-env-ec-163] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 20:06:32,904 [canton-env-ec-161] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=19
2025-08-24 20:06:36,155 [canton-env-ec-41] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - The sequencer subscription has been terminated by the server.
2025-08-24 20:06:36,156 [canton-env-ec-167] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 20:06:36,210 [canton-env-ec-41] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - PAR::participant1::1220b845dcf0... subscribes from counter=29
2025-08-24 20:16:40,930 [main] INFO  c.d.canton.CantonCommunityApp$ - Starting Canton version 2.10.2
2025-08-24 20:16:41,306 [main] ERROR c.d.canton.CantonCommunityApp$ - CANNOT_PARSE_CONFIG_FILES(8,0): Received an exception (full stack trace has been logged at DEBUG level) while attempting to parse 1 .conf-file(s). err-context:{exceptions=List(com.typesafe.config.ConfigException$Parse: /dev/stdin: 2: Key 'participant1.id' may not be followed by token: end of file), location=CantonConfig.scala:1546} 
2025-08-24 20:16:41,315 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down...
2025-08-24 20:16:41,316 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutdown complete.
2025-08-24 20:16:41,317 [Thread-0] INFO  c.d.canton.CantonCommunityApp$ - Shutting down logger. Bye bye.
2025-08-24 20:28:27,003 [canton-env-ec-41] WARN  c.d.c.a.AuthServiceJWT:participant=participant1 tid:825a23bb34fd88be50cbb1eb522089e0 - Authorization error: Could not parse JWT token: Could not read value for sub
2025-08-24 20:28:27,006 [canton-env-ec-37] WARN  c.d.c.a.Authorizer:participant=participant1 tid:825a23bb34fd88be50cbb1eb522089e0 - UNAUTHENTICATED(6,825a23bb): The command is missing a (valid) JWT token err-context:{location=Authorizer.scala:334} 
2025-08-24 20:44:35,800 [canton-env-ec-37] WARN  c.d.c.a.AuthServiceJWT:participant=participant1 tid:3500e8dc37f0aa713afc181c9f4b8e6f - Authorization error: Could not parse JWT token: Could not read value for sub
2025-08-24 20:44:35,804 [canton-env-ec-116] WARN  c.d.c.a.Authorizer:participant=participant1 tid:3500e8dc37f0aa713afc181c9f4b8e6f - UNAUTHENTICATED(6,3500e8dc): The command is missing a (valid) JWT token err-context:{location=Authorizer.scala:334} 
2025-08-24 21:01:52,584 [daml.index.db.threadpool.connection.indexer-13] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering for LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T21:00:00Z)
2025-08-24 21:01:52,587 [daml.index.db.threadpool.connection.indexer-13] INFO  c.d.c.p.i.MeteringAggregator:participant=participant1 - Aggregating transaction metering completed up to LedgerMeteringEnd(Offset(Bytes(00000000000000000b)),2025-08-24T21:00:00Z)
2025-08-24 21:05:52,799 [canton-env-ec-161] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - DOM::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T22:05:52.797536Z
2025-08-24 21:05:52,924 [canton-env-ec-41] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - MED::mydomain::122060f2dea2... authenticated new token with expiry 2025-08-24T22:05:52.922734Z
2025-08-24 21:05:56,230 [canton-env-ec-36] INFO  c.d.c.d.s.a.MemberAuthenticationServiceOld:domain=mydomain - PAR::participant1::1220b845dcf0... authenticated new token with expiry 2025-08-24T22:05:56.228667Z
2025-08-24 21:06:12,762 [canton-env-ec-116] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=topology-manager - The sequencer subscription has been terminated by the server.
2025-08-24 21:06:12,763 [canton-env-ec-41] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=topology-manager - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 21:06:12,818 [canton-env-ec-116] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - DOM::mydomain::122060f2dea2... subscribes from counter=24
2025-08-24 21:06:12,886 [canton-env-ec-163] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:domain=mydomain/client=mediator - The sequencer subscription has been terminated by the server.
2025-08-24 21:06:12,887 [canton-env-ec-65] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:domain=mydomain/client=mediator - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 21:06:12,942 [canton-env-ec-36] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain - MED::mydomain::122060f2dea2... subscribes from counter=19
2025-08-24 21:06:16,199 [canton-env-ec-155] INFO  c.d.c.s.c.t.GrpcSequencerSubscription:participant=participant1/domainId=mydomain::122060f2dea2 - The sequencer subscription has been terminated by the server.
2025-08-24 21:06:16,199 [canton-env-ec-41] INFO  c.d.c.s.c.t.GrpcSubscriptionErrorRetryPolicy:participant=participant1/domainId=mydomain::122060f2dea2 tid:75cff155b17fb2be7f6891189d3aab94 - Trying to reconnect to give the sequencer the opportunity to become available again (after Connection terminated by the server.)
2025-08-24 21:06:16,254 [canton-env-ec-116] INFO  c.d.c.d.s.s.GrpcSequencerService:domain=mydomain tid:75cff155b17fb2be7f6891189d3aab94 - PAR::participant1::1220b845dcf0... subscribes from counter=29
